# -*- coding: utf-8 -*-
"""
Receipt processing service - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º batch job –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
"""

import os
import json
import logging
import time
import re
import uuid
import requests
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any, Optional, Tuple

from google.cloud import bigquery, storage, aiplatform, bigquery_storage_v1
from flask import Request, jsonify
import functions_framework
import pandas as pd

# -----------------------------------------------------------------------------
# Configuration and constants
# -----------------------------------------------------------------------------
PROJECT_ID = os.getenv("PROJECT_ID", "datascience-417611")
LOCATION = os.getenv("LOCATION", "europe-central2")
DATASET = os.getenv("DATASET", "vlad")  # BigQuery dataset –æ—Å—Ç–∞–µ—Ç—Å—è vlad
BUCKET_NAME = f"{PROJECT_ID}-batch-processing"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyBJTQ1yNa9fZASgskN4IBXVgy-V8J931Mw")

# Initialize BigQuery client for country lookup
bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("receipt_processor")

# –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç—Ä–∞–Ω –Ω–∞ –≤–∞–ª—é—Ç—ã
COUNTRY_TO_CURRENCY = {
    'PL': 'PLN',  # –ü–æ–ª—å—à–∞ - Zloty
    'GB': 'GBP',  # –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è - Pounds Sterling
    'RO': 'RON',  # –†—É–º—ã–Ω–∏—è - Romanian Leu
    'HU': 'HUF',  # –í–µ–Ω–≥—Ä–∏—è - Hungarian Forint
    'RS': 'RSD',  # –°–µ—Ä–±–∏—è - Serbian Dinar
    'IT': 'EUR',  # –ò—Ç–∞–ª–∏—è - Euro
    'DE': 'EUR',  # –ì–µ—Ä–º–∞–Ω–∏—è - Euro
    'FR': 'EUR',  # –§—Ä–∞–Ω—Ü–∏—è - Euro
    'ES': 'EUR',  # –ò—Å–ø–∞–Ω–∏—è - Euro
    'PT': 'EUR',  # –ü–æ—Ä—Ç—É–≥–∞–ª–∏—è - Euro
    'NL': 'EUR',  # –ù–∏–¥–µ—Ä–ª–∞–Ω–¥—ã - Euro
    'BE': 'EUR',  # –ë–µ–ª—å–≥–∏—è - Euro
    'AT': 'EUR',  # –ê–≤—Å—Ç—Ä–∏—è - Euro
    'IE': 'EUR',  # –ò—Ä–ª–∞–Ω–¥–∏—è - Euro
    'FI': 'EUR',  # –§–∏–Ω–ª—è–Ω–¥–∏—è - Euro
    'GR': 'EUR',  # –ì—Ä–µ—Ü–∏—è - Euro
    'SK': 'EUR',  # –°–ª–æ–≤–∞–∫–∏—è - Euro
    'SI': 'EUR',  # –°–ª–æ–≤–µ–Ω–∏—è - Euro
    'EE': 'EUR',  # –≠—Å—Ç–æ–Ω–∏—è - Euro
    'LV': 'EUR',  # –õ–∞—Ç–≤–∏—è - Euro
    'LT': 'EUR',  # –õ–∏—Ç–≤–∞ - Euro
    'LU': 'EUR',  # –õ—é–∫—Å–µ–º–±—É—Ä–≥ - Euro
    'MT': 'EUR',  # –ú–∞–ª—å—Ç–∞ - Euro
    'CY': 'EUR',  # –ö–∏–ø—Ä - Euro
}

def get_currency_for_country(country: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–∞–ª—é—Ç—É –¥–ª—è —Å—Ç—Ä–∞–Ω—ã"""
    return COUNTRY_TO_CURRENCY.get(country, 'EUR')  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é EUR

def get_country_from_gamification(gamification_id: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç—Ä–∞–Ω—É –ø–æ gamification_id –∏–∑ BigQuery lookup —Ç–∞–±–ª–∏—Ü—ã"""
    if not gamification_id:
        return None
        
    try:
        query = f"""
        SELECT bill_country 
        FROM `{PROJECT_ID}.{DATASET}.gamification_lookup`
        WHERE gamification_id = @gamification_id
          AND is_active = TRUE
        LIMIT 1
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("gamification_id", "STRING", gamification_id)
            ]
        )
        
        query_job = bq_client.query(query, job_config=job_config, location=LOCATION)
        results = list(query_job.result())
        
        if results:
            return results[0].bill_country
        else:
            logger.warning(f"‚ö†Ô∏è No active gamification found for ID: {gamification_id}")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Error getting country for {gamification_id}: {e}")
        return None

def get_active_promotions() -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–π –∏–∑ gamification_lookup
    
    –ò–ó–ú–ï–ù–ï–ù–û: –£–±—Ä–∞–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–∞—Ç (start_date, finish_date)
    –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ is_active = TRUE (–∫–æ—Ç–æ—Ä—ã–π –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ NOT is_off)
    –≠—Ç–æ —Å–æ–≥–ª–∞—Å—É–µ—Ç—Å—è —Å –ª–æ–≥–∏–∫–æ–π gamification-sync –∏ daily-export-to-gcs
    """
    try:
        query = f"""
        SELECT 
            gamification_id,
            bill_country,
            bill_name,
            company_id,
            start_date,
            finish_date,
            is_active,
            created_at
        FROM `{PROJECT_ID}.{DATASET}.gamification_lookup`
        WHERE is_active = TRUE
        ORDER BY created_at DESC
        """
        
        query_job = bq_client.query(query, location=LOCATION)
        results = list(query_job.result())
        
        promotions = []
        for row in results:
            promotions.append({
                "gamification_id": row.gamification_id,
                "bill_country": row.bill_country,
                "bill_name": row.bill_name,
                "company_id": row.company_id,
                "start_date": row.start_date,
                "finish_date": row.finish_date,
                "is_active": row.is_active,
                "created_at": row.created_at
            })
        
        logger.info(f"Found {len(promotions)} active promotions")
        return promotions
        
    except Exception as e:
        logger.error(f"‚ùå Error getting active promotions: {e}")
        return []

def process_active_promotions_parallel(report_id: str, report_name: str, limit_per_promo: int = None) -> Dict[str, Any]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    
    Args:
        report_id: ID –æ—Ç—á–µ—Ç–∞
        report_name: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞  
        limit_per_promo: –õ–∏–º–∏—Ç —á–µ–∫–æ–≤ –Ω–∞ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏—é
        
    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–π
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–∏
        active_promotions = get_active_promotions()
        
        if not active_promotions:
            logger.warning("No active promotions found")
            return {
                "status": "warning",
                "message": "No active promotions found",
                "promotions_processed": 0
            }
        
        logger.info(f"Processing {len(active_promotions)} active promotions")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–æ–º–æ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        promotions_by_country = {}
        for promo in active_promotions:
            country = promo["bill_country"]
            if country not in promotions_by_country:
                promotions_by_country[country] = []
            promotions_by_country[country].append(promo)
        
        results = {
            "status": "success",
            "promotions_processed": len(active_promotions),
            "countries": list(promotions_by_country.keys()),
            "results_by_country": {}
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω—É –æ—Ç–¥–µ–ª—å–Ω–æ
        for country, country_promotions in promotions_by_country.items():
            logger.info(f"Processing {len(country_promotions)} promotions for country {country}")
            
            try:
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –±–∞—Ç—á–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                processor = BatchReceiptProcessor()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ–∫–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã
                country_result = processor.process_batch_receipts_complete(
                    report_id=report_id,
                    report_name=report_name,
                    countries=[country],
                    limit=limit_per_promo,  # –ë–µ–∑ –ª–∏–º–∏—Ç–∞ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —á–µ–∫–∏
                    test_mode=False
                )
                
                results["results_by_country"][country] = {
                    "promotions_count": len(country_promotions),
                    "promotions": [p["gamification_id"] for p in country_promotions],
                    "processing_result": country_result
                }
                
                logger.info(f"Completed processing for country {country}: {country_result.get('status', 'unknown')}")
                
            except Exception as e:
                logger.error(f"Error processing country {country}: {e}")
                results["results_by_country"][country] = {
                    "promotions_count": len(country_promotions),
                    "promotions": [p["gamification_id"] for p in country_promotions],
                    "error": str(e)
                }
        
        return results
        
    except Exception as e:
        logger.error(f"‚ùå Error processing active promotions: {e}")
        return {
            "status": "error",
            "message": f"Failed to process active promotions: {str(e)}",
            "promotions_processed": 0
        }

def get_countries_from_data(target_date: str = None, limit: int = None) -> List[str]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∞—Ç—ã
    
    is_success —Å—Ç–∞—Ç—É—Å—ã:
    -5: server-side error
    -4: client-side error reported  
    -3: cancelled by user or aborted
    -2: rejected by moderator
    -1: rejected by automation (no retry)
    0: rejected by automation
    1: accepted by automation (–ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)
    2: accepted by moderator ‚úÖ
    3: points proposition accepted (should change to -4 or 4)
    4: synchronized with CCA and points given ‚úÖ
    """
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º WHERE —É—Å–ª–æ–≤–∏–µ –¥–ª—è –¥–∞—Ç—ã
        date_filter = ""
        if target_date:
            date_filter = f"AND DATE(time_added_ts) = DATE('{target_date}')"
        
        query = f"""
        SELECT DISTINCT 
            gl.bill_country as country
        FROM `{PROJECT_ID}.{DATASET}.gamification_bills_flat` gbf
        LEFT JOIN `{PROJECT_ID}.{DATASET}.gamification_lookup` gl
            ON gbf.gamification_id = gl.gamification_id
            AND gl.is_active = TRUE
        WHERE gbf.is_success IN (2, 4)  -- –¢–û–õ–¨–ö–û: 2=moderator, 4=synchronized (–ë–ï–ó 1=automation!)
            AND gbf.is_finished = true
            AND gl.bill_country IS NOT NULL
            {date_filter}
        """
        
        query_job = bq_client.query(query, location=LOCATION)
        results = list(query_job.result())
        
        countries = [row.country for row in results if row.country]
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        unique_countries = list(set(countries))
        
        if not unique_countries:
            logger.warning(f"‚ö†Ô∏è No countries found in data for date: {target_date}")
            return []
            
        logger.info(f"üåç Found countries in data: {unique_countries}")
        return unique_countries
        
    except Exception as e:
        logger.error(f"‚ùå Error getting countries from data: {e}")
        return []  # No fallback - return empty list on error

# Retry mechanism
try:
    import backoff
    BACKOFF_AVAILABLE = True
except ImportError:
    BACKOFF_AVAILABLE = False
    logger.warning("backoff library not available, retry mechanism disabled")

# –ò–º–ø–æ—Ä—Ç JobState —Å fallback
try:
    from google.cloud.aiplatform_v1.types.job_state import JobState
except ImportError:
    try:
        from google.cloud.aiplatform.utils import JobState
    except ImportError:
        JobState = None
        logger.warning("JobState not available, using string comparison")

# Initialize clients
bq_client = bigquery.Client(project=PROJECT_ID, location=LOCATION)
storage_client = storage.Client(project=PROJECT_ID)
aiplatform.init(project=PROJECT_ID, location=LOCATION)

# Table names
PRODUCTS_TABLE = f"{PROJECT_ID}.{DATASET}.corrected_products"
VECTOR_READY_TABLE = f"{PROJECT_ID}.{DATASET}.products_vector_ready"
SHOP_TABLE = f"{PROJECT_ID}.{DATASET}.shop_directory"
ALL_DATA_TABLE = f"{PROJECT_ID}.{DATASET}.all_data"
GAMIFICATION_BILLS_FLAT = f"{PROJECT_ID}.{DATASET}.gamification_bills_flat"
FACT_SCAN_TABLE = f"{PROJECT_ID}.{DATASET}.fact_scan"
# Universal dictionaries for all countries
DICT_CITIES_ALL_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_all"
DICT_REGIONS_ALL_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_all"
# Legacy tables (kept for backward compatibility)
DICT_CITIES_PL_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_pl"
DICT_REGIONS_PL_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_pl"
DICT_CITIES_IT_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_it"
DICT_REGIONS_IT_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_it"
DICT_CITIES_GB_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_gb"
DICT_REGIONS_GB_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_gb"
DICT_CITIES_HU_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_hu"
DICT_REGIONS_HU_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_hu"
DICT_CITIES_PT_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_pt"
DICT_REGIONS_PT_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_pt"
DICT_CITIES_RO_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_ro"
DICT_REGIONS_RO_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_ro"
DICT_CITIES_FR_TABLE = f"{PROJECT_ID}.{DATASET}.dict_cities_fr"
DICT_REGIONS_FR_TABLE = f"{PROJECT_ID}.{DATASET}.dict_regions_fr"

SUPPORTED_COUNTRIES = ["IT", "PL", "DE", "FR", "GB", "HU", "PT", "RO", "RS", "ES", "AT", "EE", "GR", "IE", "NL", "BE", "FI", "SK", "SI", "LV", "LT", "LU", "MT", "CY"]

# -----------------------------------------------------------------------------
# Utility functions
# -----------------------------------------------------------------------------
def generate_report_id(report_name: Optional[str] = None) -> str:
    """Generate a unique report identifier."""
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    short = uuid.uuid4().hex[:8]
    if report_name:
        clean = re.sub(r"[^A-Za-z0-9_-]", "_", report_name)[:50]
        return f"RPT_{ts}_{clean}_{short}"
    return f"RPT_{ts}_{short}"

def _now() -> datetime:
    """Return the current UTC timestamp."""
    return datetime.utcnow()

def _delete_existing_promo_fact_scan_data(gamification_id: str) -> None:
    """
    –£–¥–∞–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ gamification_id –∏–∑ fact_scan —Ç–∞–±–ª–∏—Ü—ã.
    """
    try:
        delete_sql = f"""
        DELETE FROM `{FACT_SCAN_TABLE}`
        WHERE gamification_id = '{gamification_id}'
        """
        job = bq_client.query(delete_sql, location=LOCATION)
        result = job.result()
        
        deleted_rows = job.num_dml_affected_rows if hasattr(job, 'num_dml_affected_rows') else 0
        logger.info(f"üóëÔ∏è Deleted {deleted_rows} existing rows from fact_scan for gamification_id: {gamification_id}")
        
    except Exception as e:
        logger.error(f"‚ùå Failed to delete existing fact_scan data for gamification_id {gamification_id}: {e}")
        raise

def _insert_rows(table_id: str, rows: List[Dict[str, Any]]) -> None:
    """Insert multiple rows into a BigQuery table with error handling."""
    if not rows:
        return
    errors = bq_client.insert_rows_json(table_id, rows)
    if errors:
        raise RuntimeError(f"BigQuery insertion errors for {table_id}: {errors}")
    logger.info("Inserted %s rows into %s", len(rows), table_id)

def _storage_write_api_load(table_id: str, rows: List[Dict[str, Any]], report_id: str, report_name: str) -> None:
    """Load data to BigQuery using optimized batch load (5-10x faster than streaming)."""
    if not rows:
        return
    
    start_time = time.time()
    logger.info("üöÄ STARTING FAST BATCH LOAD: %s rows to %s", len(rows), table_id)
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –±–∞—Ç—á–∏–Ω–≥ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤
    BATCH_SIZE = 50000  # –ú–∞–∫—Å–∏–º—É–º 50K —Å—Ç—Ä–æ–∫ –∑–∞ —Ä–∞–∑ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    
    if len(rows) <= BATCH_SIZE:
        # –ú–∞–ª—ã–π –æ–±—ä–µ–º - –∑–∞–≥—Ä—É–∂–∞–µ–º —Å—Ä–∞–∑—É
        logger.info("üìä SMALL DATASET: Loading %s rows in single batch", len(rows))
        _load_single_batch(table_id, rows, report_id, report_name)
    else:
        # –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º - —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
        logger.info("üìä LARGE DATASET: %s rows, splitting into batches of %s", len(rows), BATCH_SIZE)
        
        for i in range(0, len(rows), BATCH_SIZE):
            batch = rows[i:i + BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            total_batches = (len(rows) + BATCH_SIZE - 1) // BATCH_SIZE
            
            logger.info("üîÑ PROCESSING BATCH %s/%s (%s rows)", batch_num, total_batches, len(batch))
            _load_single_batch(table_id, batch, report_id, f"{report_name}_batch_{batch_num}")
    
    elapsed_time = time.time() - start_time
    rows_per_sec = len(rows) / elapsed_time if elapsed_time > 0 else 0
    logger.info("‚úÖ FAST BATCH LOAD COMPLETED: %s rows in %.2f seconds (%.0f rows/sec)", 
                len(rows), elapsed_time, rows_per_sec)

def _load_single_batch(table_id: str, rows: List[Dict[str, Any]], report_id: str, report_name: str) -> None:
    """Load a single batch to BigQuery using optimized batch load."""
    try:
        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ Cloud Storage
        storage_client = storage.Client(project=PROJECT_ID)
        bucket = storage_client.bucket(BUCKET_NAME)
        
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S_%f")  # –ú–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥—ã –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        filename = f"fast_load/{report_id}_{timestamp}.json"
        blob = bucket.blob(filename)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON Lines —Ñ–æ—Ä–º–∞—Ç (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ)
        json_lines = [json.dumps(row, ensure_ascii=False, default=str) for row in rows]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Cloud Storage –æ–¥–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π
        blob.upload_from_string('\n'.join(json_lines), content_type='application/json')
        logger.info("Uploaded %s rows to Cloud Storage: %s", len(rows), filename)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º job configuration –¥–ª—è batch load
        job_config = bigquery.LoadJobConfig(
            source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
            autodetect=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ö–µ–º—É
            ignore_unknown_values=True,
            max_bad_records=10,
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            job_timeout_ms=300000,  # 5 –º–∏–Ω—É—Ç timeout
            use_avro_logical_types=True
        )
        
        # –°–æ–∑–¥–∞–µ–º URI –¥–ª—è Cloud Storage —Ñ–∞–π–ª–∞
        gcs_uri = f"gs://{BUCKET_NAME}/{filename}"
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º batch load job
        load_job = bq_client.load_table_from_uri(
            gcs_uri,
            table_id,
            job_config=job_config
        )
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è job
        load_job.result()
        if load_job.errors:
            logger.error("Fast batch load errors: %s", load_job.errors)
            raise RuntimeError(f"Fast batch load failed for {table_id}: {load_job.errors}")
        
        logger.info("Successfully fast loaded %s rows into %s", len(rows), table_id)
        
        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–∑ Cloud Storage
        try:
            blob.delete()
            logger.info("Deleted temporary file: %s", filename)
        except Exception as e:
            logger.warning("Failed to delete temporary file %s: %s", filename, e)
        
    except Exception as e:
        logger.error("‚ùå FAST BATCH LOAD FAILED: %s", e)
        logger.error("üìä FALLING BACK TO STANDARD BATCH LOAD for table: %s", table_id)
        # Fallback to standard batch load method
        try:
            _batch_load_to_bigquery(table_id, rows, report_id, report_name)
            logger.info("‚úÖ STANDARD BATCH LOAD SUCCESSFUL")
        except Exception as fallback_error:
            logger.error("‚ùå STANDARD BATCH LOAD ALSO FAILED: %s", fallback_error)
            logger.error("üìä FALLING BACK TO STREAMING INSERTS for table: %s", table_id)
            try:
                _insert_rows(table_id, rows)
                logger.info("‚úÖ STREAMING INSERTS SUCCESSFUL")
            except Exception as streaming_error:
                logger.error("‚ùå ALL LOAD METHODS FAILED for table %s: %s", table_id, streaming_error)
                raise RuntimeError(f"All BigQuery load methods failed for {table_id}: {streaming_error}")

def _batch_load_to_bigquery(table_id: str, rows: List[Dict[str, Any]], 
                           report_id: str, report_name: str) -> None:
    """Load data to BigQuery using batch load (Cloud Storage) instead of streaming insert."""
    if not rows:
        return
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ Cloud Storage
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(BUCKET_NAME)
    
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    filename = f"batch_load/{report_id}_{timestamp}.json"
    blob = bucket.blob(filename)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –≤ JSON Lines —Ñ–æ—Ä–º–∞—Ç
    json_lines = []
    for row in rows:
        json_lines.append(json.dumps(row, ensure_ascii=False, default=str))
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Cloud Storage
    blob.upload_from_string('\n'.join(json_lines), content_type='application/json')
    logger.info("Uploaded %s rows to Cloud Storage: %s", len(rows), filename)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º job configuration –¥–ª—è batch load
    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
        autodetect=False,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å—Ö–µ–º—É
        ignore_unknown_values=True,
        max_bad_records=10
    )
    
    # –°–æ–∑–¥–∞–µ–º URI –¥–ª—è Cloud Storage —Ñ–∞–π–ª–∞
    gcs_uri = f"gs://{BUCKET_NAME}/{filename}"
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º batch load job
    load_job = bq_client.load_table_from_uri(
        gcs_uri, 
        table_id, 
        job_config=job_config
    )
    
    # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è job
    load_job.result()
    
    if load_job.errors:
        logger.error("Batch load errors: %s", load_job.errors)
        raise RuntimeError(f"Batch load failed for {table_id}: {load_job.errors}")
    
    logger.info("Successfully batch loaded %s rows into %s", len(rows), table_id)
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–∑ Cloud Storage
    try:
        blob.delete()
        logger.info("Deleted temporary file: %s", filename)
    except Exception as e:
        logger.warning("Failed to delete temporary file %s: %s", filename, e)


def create_combined_correction_prompt(api_id: str,
                                     products_json: list,
                                     total_price: Optional[float] = None,
                                     country_code: Optional[str] = None,
                                     nip: Optional[str] = None,
                                     shopnetwork: Optional[str] = None,
                                     raw_address: Optional[str] = None) -> str:
    """Build a combined prompt for product correction AND city determination."""
    current_total = 0.0
    lines = []
    for i, p in enumerate(products_json, 1):
        name = p.get("name", "UNKNOWN")
        qty = float(p.get("number") or p.get("qty") or 1)
        ps = float(p.get("price_single") or p.get("price") or 0)
        pt = float(p.get("price_total") or (qty * ps))
        current_total += pt
        lines.append(f"{i}. {name} | Qty: {qty} √ó {ps} = {pt}")

    currency = "PLN" if (country_code or "IT") == "PL" else "EUR"
    total_block = ""
    if total_price and total_price > 0:
        difference = abs(total_price - current_total)
        total_block = (
            f"\nRECEIPT TOTAL: {currency}{total_price:.2f}\n"
            f"CURRENT SUM: {currency}{current_total:.2f}\n"
            f"DIFFERENCE: {currency}{difference:.2f}"
        )

    total_receipt_str = f"{total_price:.2f}" if total_price and total_price > 0 else "null"

    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≥–æ—Ä–æ–¥–∞
    unique_products = []
    seen = set()
    for p in products_json[:15]:
        name = (p.get('name') or '').upper()
        if name and len(name) > 2 and name not in seen:
            unique_products.append(name)
            seen.add(name)
    products_text = "\n".join(f"- {p}" for p in unique_products[:10])

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å–∞, ZIP –∫–æ–¥ –∏ –ø—Ä–æ–≤–∏–Ω—Ü–∏—é –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
    shop_address, headquarters_address = extract_addresses_from_receipt(raw_address) if raw_address else (None, None)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º zip_code –∏–∑ –∞–¥—Ä–µ—Å–∞ –º–∞–≥–∞–∑–∏–Ω–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) –∏–ª–∏ –∏–∑ –æ–±—â–µ–≥–æ –∞–¥—Ä–µ—Å–∞
    zip_code = None
    if shop_address:
        zip_code = extract_zip_code(shop_address, country_code or "PL")
    if not zip_code and raw_address:
        zip_code = extract_zip_code(raw_address, country_code or "PL")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –¥–ª—è –ò—Ç–∞–ª–∏–∏
    province_code = None
    if country_code == "IT":
        if shop_address:
            province_code = extract_province_code(shop_address, country_code)
        if not province_code and raw_address:
            province_code = extract_province_code(raw_address, country_code)
        logger.info(f"Extracted for IT in batch: ZIP={zip_code}, Province={province_code} from address={raw_address}")

    # –†–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
    if country_code == "PL":
        prompt = f"""
You are an expert in Polish retail geography and receipt data correction.

CRITICAL TASK: Fix product names AND prices, AND determine the city from receipt data.

RECEIPT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Sample products from receipt:
{products_text}

RECEIPT ITEMS:
{chr(10).join(lines)}{total_block}

YOUR TASKS:
1. FIX PRODUCT NAMES (IMPORTANT - KEEP POLISH LANGUAGE):
   - Expand abbreviations: 'SOK JAB≈ÅKOWY' ‚Üí 'Sok Jab≈Çkowy'
   - Fix typos: 'OGORKI' ‚Üí 'Og√≥rki', 'BULKI' ‚Üí 'Bu≈Çki'
   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'
   - Fix OCR errors: 'M0KA' ‚Üí 'MƒÖka', 'SZYNKA' ‚Üí 'Szynka'
   - Keep Polish names: 'MƒÖka Basia' stays 'MƒÖka Basia' (NOT 'Basia Flour')
   - Keep Polish names: 'Szynka' stays 'Szynka' (NOT 'Ham')
   - Use proper Polish capitalization: 'SOK JAB≈ÅKOWY' ‚Üí 'Sok Jab≈Çkowy'
   - Fix Polish diacritics: 'ƒÖ', 'ƒá', 'ƒô', '≈Ç', '≈Ñ', '√≥', '≈õ', '≈∫', '≈º'
   - Common Polish products: Chleb, Mleko, Mas≈Ço, Ser, Wƒôdlina, Owoce, Warzywa

2. FIX PRICES (CRITICAL):
   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)
   - Check if quantity √ó unit price = total price for each item
   - If not, adjust the unit price or quantity to make it consistent
   - All corrected prices must sum up to the receipt total (if provided)
   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4

3. DETERMINE CITY (CRITICAL - USE POLISH NAMES):
   - ALWAYS use Polish city names: Warszawa (not Warsaw), Krak√≥w (not Cracow), ≈Å√≥d≈∫ (not Lodz)
   - PRIORITY #1: Use ZIP code if available ({zip_code or 'none'}) ‚≠ê
   - PRIORITY #2: Use NIP first digits for regional hints:
     * 10-19: Mazowieckie (Warszawa area)
     * 20-29: Ma≈Çopolskie (Krak√≥w area)
     * 30-39: Lubelskie, Podkarpackie
     * 40-49: ≈ölƒÖskie (Katowice area)
     * 50-59: Dolno≈õlƒÖskie (Wroc≈Çaw area)
     * 60-69: Wielkopolskie (Pozna≈Ñ area)
     * 70-79: Zachodniopomorskie (Szczecin area)
     * 80-89: Pomorskie (Gda≈Ñsk area)
     * 90-99: Warmi≈Ñsko-mazurskie, Podlaskie
   - Use retail chain geographic presence knowledge
   - Analyze product types for regional preferences
   - Provide population estimates based on city size knowledge
   - Major Polish cities: Warszawa (~1.8M), Krak√≥w (~780K), ≈Å√≥d≈∫ (~680K), Wroc≈Çaw (~640K), Pozna≈Ñ (~540K), Gda≈Ñsk (~470K)

POLISH RETAIL CHAINS KNOWLEDGE:
- BIEDRONKA: Strong in all major cities, especially Warszawa, Krak√≥w, Wroc≈Çaw
- ≈ªABKA: Especially strong in Warszawa, Krak√≥w, Wroc≈Çaw, Pozna≈Ñ
- KAUFLAND: Major cities, especially Wroc≈Çaw, Pozna≈Ñ, Warszawa
- LIDL: All major cities, strong presence everywhere
- CARREFOUR: Warszawa, Krak√≥w, Pozna≈Ñ, Wroc≈Çaw
- TESCO: Major cities, especially Warszawa, Krak√≥w
- ALDI: Growing presence in major cities
- NETTO: Strong in northern regions

POLISH REGIONS AND MAJOR CITIES (USE STANDARD FORMS):
- MAZOWIECKIE: Warszawa, Radom, P≈Çock, Siedlce, Ostro≈Çƒôka
- MA≈ÅOPOLSKIE: Krak√≥w, Tarn√≥w, Nowy SƒÖcz, O≈õwiƒôcim
- ≈öLƒÑSKIE: Katowice, Czƒôstochowa, Sosnowiec, Gliwice, Zabrze, Bytom
- WIELKOPOLSKIE: Pozna≈Ñ, Kalisz, Konin, Pi≈Ça
- DOLNO≈öLƒÑSKIE: Wroc≈Çaw, Wa≈Çbrzych, Legnica, Jelenia G√≥ra
- ≈Å√ìDZKIE: ≈Å√≥d≈∫, Piotrk√≥w Trybunalski, Pabianice, Tomasz√≥w Mazowiecki
- POMORSKIE: Gda≈Ñsk, Gdynia, Sopot, S≈Çupsk, Tczew
- ZACHODNIOPOMORSKIE: Szczecin, Koszalin, Stargard, Ko≈Çobrzeg
- LUBELSKIE: Lublin, Che≈Çm, Zamo≈õƒá, Bia≈Ça Podlaska
- PODKARPACKIE: Rzesz√≥w, Przemy≈õl, Stalowa Wola, Mielec
- PODLASKIE: Bia≈Çystok, Suwa≈Çki, ≈Åom≈ºa, August√≥w
- WARMI≈ÉSKO-MAZURSKIE: Olsztyn, ElblƒÖg, E≈Çk, Ostr√≥da
- KUJAWSKO-POMORSKIE: Bydgoszcz, Toru≈Ñ, W≈Çoc≈Çawek, GrudziƒÖdz
- ≈öWIƒòTOKRZYSKIE: Kielce, Ostrowiec ≈öwiƒôtokrzyski, Starachowice
- LUBUSKIE: Zielona G√≥ra, Gorz√≥w Wielkopolski, ≈ªary, Nowa S√≥l
- OPOLSKIE: Opole, Kƒôdzierzyn-Ko≈∫le, Nysa, Brzeg

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "total_receipt": {total_receipt_str},
  "total_calculated": <sum of all corrected price_total>,
  "corrections_made": true/false,
  "products": [
    {{
      "name_original": "original name from receipt",
      "name_corrected": "fixed name",
      "quantity": integer,
      "price_single_original": number,
      "price_single_corrected": number,
      "price_total": number,
      "price_correction_reason": "OCR error: 3‚Üí8" or null
    }}
  ],
  "city_analysis": {{
    "city": "city name or UNKNOWN (use standard forms: Warszawa, Krak√≥w, Gda≈Ñsk)",
    "region": "region name or UNKNOWN",
    "zip_code": "extracted or matched zip code or null",
    "city_population": integer,
    "match_method": "zip_code_match" | "address_match" | "name_match" | "nip_hint",
    "confidence": "HIGH/MEDIUM/LOW",
    "evidence": "brief explanation of how you identified the location, including which matching method was used"
  }}
}}

IMPORTANT: Ensure all arithmetic is correct, total matches, product names stay in Polish, and provide best city estimate!

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON.
"""
    elif country_code == "IT":
        prompt = f"""
You are an expert in Italian retail geography and receipt data correction.

CRITICAL TASK: Fix product names AND prices, AND determine the city from receipt data.

CRITICAL MATCHING PRIORITY for city determination (use in this exact order):
1. ZIP CODE MATCH (HIGHEST PRIORITY) ‚≠ê‚≠ê‚≠ê
   - If zip_code is provided and not 'none', use it FIRST
   - Example: zip_code "80126" ‚Üí NAPOLI (Campania)
   - Example: zip_code "00100" ‚Üí ROMA (Lazio)
   
2. PROVINCE/REGION CODE MATCH ‚≠ê‚≠ê
   - If address contains (RM), (NA), (MI), etc. ‚Üí filter by province
   - Example: "(RM)" ‚Üí Roma province ‚Üí filter cities in Lazio
   
3. STREET ADDRESS CONTEXT ‚≠ê
   - Use street name to disambiguate between similar cities
   
4. NORMALIZED CITY NAME SIMILARITY
   - Use only if ZIP/province not available
   
5. NIP REGION HINTS
   - Use only as last resort

RECEIPT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Shop Address (if separated): {shop_address or 'none'}
- Headquarters Address (if separated): {headquarters_address or 'none'}
- Extracted ZIP Code: {zip_code or 'none'} {'‚≠ê USE THIS FIRST!' if zip_code and zip_code != 'none' else ''}
- Extracted Province Code: {province_code or 'none'} {'‚≠ê USE THIS SECOND!' if province_code and province_code != 'none' else ''}
- Sample products from receipt:
{products_text}

RECEIPT ITEMS:
{chr(10).join(lines)}{total_block}

YOUR TASKS:
1. FIX PRODUCT NAMES:
   - Expand abbreviations: "COCA" ‚Üí "COCA COLA"
   - Fix typos: "PIANATA" ‚Üí "PIADINA"
   - Standardize units: "1,5LT" ‚Üí "1.5L"
   - Fix OCR errors: "C0CA C0LA" ‚Üí "COCA COLA"

2. FIX PRICES (CRITICAL):
   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)
   - Check if quantity √ó unit price = total price for each item
   - If not, adjust the unit price or quantity to make it consistent
   - All corrected prices must sum up to the receipt total (if provided)
   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4

3. DETERMINE CITY (IMPORTANT):
   - PRIORITY #1: Use ZIP code if available ({zip_code or 'none'}) ‚≠ê
   - PRIORITY #2: Use Province code if available ({province_code or 'none'}) ‚≠ê
   - PRIORITY #3: Use NIP first digits for regional hints (Northern Italy: 01-19, Central: 20-59, Southern: 60-99)
   - Consider all Italian cities and towns (population 10,000+)
   - Use retail chain geographic presence knowledge
   - Analyze product types for regional preferences
   - Provide population estimates based on city size knowledge

ITALIAN RETAIL CHAINS KNOWLEDGE:
- CONAD: Cooperative stores, strong in Emilia-Romagna, Tuscany
- CARREFOUR: Major cities, especially Milano, Roma, Napoli
- ESSELUNGA: Lombardia, Tuscany, strong in Milano, Firenze
- COOP: Pan-Italian, especially strong in northern regions
- LIDL: All major cities, strong presence everywhere
- EUROSPIN: Very cheap, strong in southern regions
- PAM: Northern Italy, especially Lombardia
- DESPAR: Veneto, Lombardia regions
- PENNY MARKET: Growing presence, especially in central Italy
- MD DISCOUNT: Southern Italy, especially Campania, Puglia
- SELEX: Central Italy
- FAMILA: Veneto, Friuli-Venezia Giulia
- BENNET: Northern Italy, especially Lombardia
- CRAI: Southern Italy, especially Sicily, Calabria

ITALIAN REGIONS AND MAJOR CITIES (USE STANDARD FORMS):
- LAZIO: Roma, Latina, Frosinone, Viterbo, Rieti
- LOMBARDIA: Milano, Bergamo, Brescia, Monza, Como, Varese, Pavia, Cremona
- CAMPANIA: Napoli, Salerno, Caserta, Avellino, Benevento
- PIEMONTE: Torino, Alessandria, Novara, Cuneo, Asti
- SICILIA: Palermo, Catania, Messina, Siracusa, Agrigento
- VENETO: Venezia, Verona, Padova, Vicenza, Treviso
- EMILIA-ROMAGNA: Bologna, Modena, Parma, Reggio Emilia, Ravenna, Ferrara
- TOSCANA: Firenze, Pisa, Livorno, Prato, Siena, Arezzo
- PUGLIA: Bari, Taranto, Foggia, Lecce, Brindisi
- LIGURIA: Genova, La Spezia, Savona, Imperia
- CALABRIA: Reggio Calabria, Catanzaro, Cosenza
- MARCHE: Ancona, Pesaro, Macerata, Ascoli Piceno
- ABRUZZO: L'Aquila, Pescara, Chieti, Teramo
- UMBRIA: Perugia, Terni

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "total_receipt": {total_receipt_str},
  "total_calculated": <sum of all corrected price_total>,
  "corrections_made": true/false,
  "products": [
    {{
      "name_original": "original name from receipt",
      "name_corrected": "fixed name",
      "quantity": integer,
      "price_single_original": number,
      "price_single_corrected": number,
      "price_total": number,
      "price_correction_reason": "OCR error: 3‚Üí8" or null
    }}
  ],
  "city_analysis": {{
    "city": "city name or UNKNOWN (use standard forms: Roma, Milano, Napoli)",
    "region": "region name or UNKNOWN",
    "zip_code": "extracted or matched zip code or null",
    "province_code": "extracted province code (RM, NA, MI, etc.) or null",
    "city_population": integer,
    "match_method": "zip_code_match" | "province_match" | "address_match" | "name_match" | "nip_hint",
    "confidence": "HIGH/MEDIUM/LOW",
    "evidence": "brief explanation of how you identified the location, including which matching method was used"
  }}
}}

IMPORTANT: Ensure all arithmetic is correct, total matches, and provide best city estimate!

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON.
"""
    elif country_code == "DE":
        prompt = f"""
You are an expert in German retail geography and receipt data correction.

CRITICAL TASK: Fix product names AND prices, AND determine the city from receipt data.

RECEIPT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Sample products from receipt:
{products_text}

RECEIPT ITEMS:
{chr(10).join(lines)}{total_block}

YOUR TASKS:
1. FIX PRODUCT NAMES (IMPORTANT - KEEP GERMAN LANGUAGE):
   - Expand abbreviations: 'BROT' ‚Üí 'Brot', 'MILCH' ‚Üí 'Milch'
   - Fix typos: 'WURST' ‚Üí 'Wurst', 'KASE' ‚Üí 'K√§se'
   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'
   - Fix OCR errors: 'BROT' ‚Üí 'Brot', 'BUTTER' ‚Üí 'Butter'
   - Keep German names: 'Brot' stays 'Brot' (NOT 'Bread')
   - Keep German names: 'Wurst' stays 'Wurst' (NOT 'Sausage')
   - Use proper German capitalization: 'BROT' ‚Üí 'Brot'
   - Fix German diacritics: '√§', '√∂', '√º', '√ü'
   - Common German products: Brot, Milch, Butter, K√§se, Wurst, Obst, Gem√ºse

2. FIX PRICES (CRITICAL):
   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)
   - Check if quantity √ó unit price = total price for each item
   - If not, adjust the unit price or quantity to make it consistent
   - All corrected prices must sum up to the receipt total (if provided)
   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4

3. DETERMINE CITY (IMPORTANT):
   - Use retail chain geographic presence knowledge
   - Consider all German cities and towns (population 50,000+)
   - Analyze product types for regional preferences
   - Provide population estimates based on city size knowledge

GERMAN RETAIL CHAINS KNOWLEDGE:
- ALDI: Strong nationwide, especially in rural areas
- LIDL: Pan-German presence, strong in all regions
- REWE: Strong in western and southern Germany
- EDEKA: Strong regional presence, especially in northern Germany
- KAUFLAND: Major cities, especially in eastern Germany
- REAL: Large cities, especially in urban centers
- NETTO: Strong in northern and eastern Germany
- PENNY: Growing presence, especially in urban areas
- NORMA: Southern Germany, especially Bavaria
- TEGUT: Hesse, Thuringia regions
- SPAR: Various regions, especially rural areas

GERMAN MAJOR CITIES (USE STANDARD FORMS):
- NORTH RHINE-WESTPHALIA: K√∂ln, D√ºsseldorf, Dortmund, Essen, Duisburg, Bochum, Wuppertal, Bielefeld, Bonn, M√ºnster
- BAVARIA: M√ºnchen, N√ºrnberg, Augsburg, Regensburg, Ingolstadt, W√ºrzburg, F√ºrth, Erlangen, Bayreuth
- BADEN-W√úRTTEMBERG: Stuttgart, Mannheim, Karlsruhe, Freiburg, Heidelberg, Heilbronn, Ulm, Pforzheim, Reutlingen
- LOWER SAXONY: Hannover, Braunschweig, Osnabr√ºck, Oldenburg, G√∂ttingen, Wolfsburg, Hildesheim, Salzgitter
- HESSE: Frankfurt, Wiesbaden, Kassel, Darmstadt, Offenbach, Hanau, Marburg, Gie√üen
- SAXONY: Dresden, Leipzig, Chemnitz, Zwickau, Plauen, G√∂rlitz, Freiberg, Bautzen
- RHINELAND-PALATINATE: Mainz, Ludwigshafen, Koblenz, Trier, Kaiserslautern, Worms, Neuwied
- BERLIN: Berlin (city-state)
- HAMBURG: Hamburg (city-state)
- BREMEN: Bremen, Bremerhaven (city-state)
- SCHLESWIG-HOLSTEIN: Kiel, L√ºbeck, Flensburg, Neum√ºnster, Norderstedt
- MECKLENBURG-WESTERN POMERANIA: Schwerin, Rostock, Neubrandenburg, Stralsund, Greifswald
- BRANDENBURG: Potsdam, Cottbus, Brandenburg, Frankfurt (Oder), Oranienburg
- SAXONY-ANHALT: Magdeburg, Halle, Dessau-Ro√ülau, Wittenberg, Stendal
- THURINGIA: Erfurt, Jena, Gera, Weimar, Gotha, Eisenach, Nordhausen

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "total_receipt": {total_receipt_str},
  "total_calculated": <sum of all corrected price_total>,
  "corrections_made": true/false,
  "products": [
    {{
      "name_original": "original name from receipt",
      "name_corrected": "fixed name",
      "quantity": integer,
      "price_single_original": number,
      "price_single_corrected": number,
      "price_total": number,
      "price_correction_reason": "OCR error: 3‚Üí8" or null
    }}
  ],
  "city_analysis": {{
    "city": "city name or UNKNOWN (use standard forms: Berlin, M√ºnchen, Hamburg)",
    "region": "region/province name or UNKNOWN",
    "city_population": integer,
    "confidence": "HIGH/MEDIUM/LOW",
    "evidence": "brief explanation of how you identified the location"
  }}
}}

IMPORTANT: Ensure all arithmetic is correct, total matches, product names stay in German, and provide best city estimate!

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON.
"""
    elif country_code == "FR":
        prompt = f"""
You are an expert in French retail geography and receipt data correction.

CRITICAL TASK: Fix product names AND prices, AND determine the city from receipt data.

RECEIPT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Sample products from receipt:
{products_text}

RECEIPT ITEMS:
{chr(10).join(lines)}{total_block}

YOUR TASKS:
1. FIX PRODUCT NAMES (IMPORTANT - KEEP FRENCH LANGUAGE):
   - Expand abbreviations: 'PAIN' ‚Üí 'Pain', 'LAIT' ‚Üí 'Lait'
   - Fix typos: 'FROMAGE' ‚Üí 'Fromage', 'JAMBON' ‚Üí 'Jambon'
   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'
   - Fix OCR errors: 'PAIN' ‚Üí 'Pain', 'BEURRE' ‚Üí 'Beurre'
   - Keep French names: 'Pain' stays 'Pain' (NOT 'Bread')
   - Keep French names: 'Jambon' stays 'Jambon' (NOT 'Ham')
   - Use proper French capitalization: 'PAIN' ‚Üí 'Pain'
   - Fix French diacritics: '√†', '√¢', '√§', '√ß', '√©', '√®', '√™', '√´', '√Æ', '√Ø', '√¥', '√∂', '√π', '√ª', '√º', '√ø'
   - Common French products: Pain, Lait, Beurre, Fromage, Jambon, Fruits, L√©gumes

2. FIX PRICES (CRITICAL):
   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)
   - Check if quantity √ó unit price = total price for each item
   - If not, adjust the unit price or quantity to make it consistent
   - All corrected prices must sum up to the receipt total (if provided)
   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4

3. DETERMINE CITY (IMPORTANT):
   - Use retail chain geographic presence knowledge
   - Consider all French cities and towns (population 50,000+)
   - Analyze product types for regional preferences
   - Provide population estimates based on city size knowledge

FRENCH RETAIL CHAINS KNOWLEDGE:
- CARREFOUR: Pan-French presence, strong in major cities
- LECLERC: Strong nationwide, especially in rural and suburban areas
- AUCHAN: Major cities, especially in northern and eastern France
- CASINO: Southern France, especially in Provence and Languedoc
- INTERMARCHE: Strong in rural areas and small towns
- SUPER U / SYSTEME U: Cooperative stores, strong in western France
- LIDL: Growing presence, especially in urban areas
- ALDI: Growing presence, especially in eastern France
- MONOPRIX: Urban centers, especially Paris and major cities
- FRANPRIX: Paris and urban centers
- SPAR: Various regions, especially rural areas

FRENCH MAJOR CITIES (USE STANDARD FORMS):
- ILE-DE-FRANCE: Paris, Boulogne-Billancourt, Saint-Denis, Argenteuil, Montreuil, Cr√©teil, Nanterre, Vitry-sur-Seine, Courbevoie, Versailles
- PROVENCE-ALPES-COTE D'AZUR: Marseille, Nice, Toulon, N√Æmes, Aix-en-Provence, Montpellier, Avignon, Cannes, Antibes, La Seyne-sur-Mer
- AUVERGNE-RHONE-ALPES: Lyon, Saint-√âtienne, Grenoble, Villeurbanne, Clermont-Ferrand, Annecy, Valence, Chamb√©ry, Bourg-en-Bresse, Saint-Priest
- HAUTS-DE-FRANCE: Lille, Amiens, Roubaix, Tourcoing, Dunkerque, Calais, Villeneuve-d'Ascq, Saint-Quentin, Beauvais, Abbeville
- GRAND EST: Strasbourg, Mulhouse, Reims, Metz, Colmar, Troyes, Charleville-M√©zi√®res, Ch√¢lons-en-Champagne, √âpinal, Haguenau
- OCCITANIE: Toulouse, Montpellier, N√Æmes, Perpignan, B√©ziers, Montauban, Narbonne, Albi, Carcassonne, S√®te
- NOUVELLE-AQUITAINE: Bordeaux, Limoges, Poitiers, La Rochelle, Angoul√™me, Agen, P√©rigueux, Bayonne, Pau, Mont-de-Marsan
- PAYS DE LA LOIRE: Nantes, Le Mans, Angers, Saint-Nazaire, Cholet, La Roche-sur-Yon, Laval, Saumur, Saint-Herblain, Orvault
- BRETAGNE: Rennes, Brest, Quimper, Lorient, Vannes, Saint-Malo, Foug√®res, Saint-Brieuc, Lannion, Concarneau
- NORMANDIE: Rouen, Le Havre, Caen, Cherbourg, √âvreux, Dieppe, Saint-√âtienne-du-Rouvray, Sotteville-l√®s-Rouen, Le Grand-Quevilly, Vernon
- CENTRE-VAL DE LOIRE: Tours, Orl√©ans, Blois, Ch√¢teauroux, Bourges, Chartres, Dreux, Vierzon, Olivet, Saint-Jean-de-Braye
- BOURGOGNE-FRANCHE-COMTE: Dijon, Besan√ßon, Belfort, Chalon-sur-Sa√¥ne, Auxerre, Nevers, M√¢con, Sens, Montb√©liard, Beaune
- CORSE: Ajaccio, Bastia, Porto-Vecchio, Corte, Sart√®ne, Calvi, L'√éle-Rousse, Propriano, Bonifacio, Al√©ria

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "total_receipt": {total_receipt_str},
  "total_calculated": <sum of all corrected price_total>,
  "corrections_made": true/false,
  "products": [
    {{
      "name_original": "original name from receipt",
      "name_corrected": "fixed name",
      "quantity": integer,
      "price_single_original": number,
      "price_single_corrected": number,
      "price_total": number,
      "price_correction_reason": "OCR error: 3‚Üí8" or null
    }}
  ],
  "city_analysis": {{
    "city": "city name or UNKNOWN (use standard forms: Paris, Lyon, Marseille)",
    "region": "region name or UNKNOWN",
    "city_population": integer,
    "confidence": "HIGH/MEDIUM/LOW",
    "evidence": "brief explanation of how you identified the location"
  }}
}}

IMPORTANT: Ensure all arithmetic is correct, total matches, product names stay in French, and provide best city estimate!

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON.
"""
    elif country_code == "GB":
        # Use universal prompt for GB
        pass
    elif country_code == "HU":
        # Use universal prompt for HU
        pass
    elif country_code == "PT":
        # Use universal prompt for PT
        pass
    elif country_code == "RO":
        # Use universal prompt for RO
        pass
    # Old prompts removed - using universal prompt for all countries
    if False:
        prompt = (
            "You are an expert in fixing Hungarian receipt data with OCR errors.\n\n"
            "CRITICAL TASK: Fix product names AND correct prices that have OCR errors.\n"
            "The receipt total is usually correct, but individual prices often have OCR mistakes.\n\n"
            "RECEIPT DATA:\n"
            f"API_ID: {api_id}\n"
            + "\n".join(lines)
            + total_block
            + "\n\nYOUR TASKS:\n"
              "1. FIX PRODUCT NAMES (IMPORTANT - KEEP HUNGARIAN LANGUAGE):\n"
              "   - Expand abbreviations: 'KENY√âR' ‚Üí 'Keny√©r', 'TEJ' ‚Üí 'Tej'\n"
              "   - Fix typos: 'VAGON' ‚Üí 'Vajon', 'SAJT' ‚Üí 'Sajt'\n"
              "   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'\n"
              "   - Fix OCR errors: 'KENY√âR' ‚Üí 'Keny√©r', 'VAGON' ‚Üí 'Vajon'\n"
              "   - Keep Hungarian names: 'Keny√©r' stays 'Keny√©r' (NOT 'Bread')\n"
              "   - Use proper Hungarian capitalization: 'KENY√âR' ‚Üí 'Keny√©r'\n"
              "   - Fix Hungarian diacritics: '√°', '√©', '√≠', '√≥', '√∂', '≈ë', '√∫', '√º', '≈±'\n"
              "   - Common Hungarian products: Keny√©r, Tej, Vaj, Sajt, Sonka, Gy√ºm√∂lcs, Z√∂lds√©g\n\n"
              "2. FIX PRICES (CRITICAL):\n"
              "   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)\n"
              "   - Check if quantity √ó unit price = total price for each item\n"
              "   - If not, adjust the unit price or quantity to make it consistent\n"
              "   - All corrected prices must sum up to the receipt total (if provided)\n"
              "   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4\n\n"
              "3. VALIDATION RULES:\n"
              "   - Each line: quantity √ó price_single MUST equal price_total\n"
              "   - Sum of all price_total MUST equal receipt total (if provided)\n"
              "   - Prices must be realistic for Hungarian supermarkets (100 - 50000 HUF per item)\n"
              "   - KEEP PRODUCT NAMES IN HUNGARIAN - DO NOT TRANSLATE TO ENGLISH\n\n"
              "HUNGARIAN PRODUCT EXAMPLES:\n"
              "   - 'Keny√©r 500g' ‚Üí 'Keny√©r 500 g' (NOT 'Bread 500 g')\n"
              "   - 'Sonka 200g' ‚Üí 'Sonka 200 g' (NOT 'Ham 200 g')\n"
              "   - 'Tej 1L' ‚Üí 'Tej 1 L'\n"
              "   - 'Sajt 300g' ‚Üí 'Sajt 300 g'\n\n"
              "OUTPUT FORMAT (strict JSON):\n"
            + "JSON schema not needed - using universal prompt"
            + "\n\nIMPORTANT: Ensure all arithmetic is correct, total matches, and product names stay in Hungarian!"
        )
    elif country_code == "PT":
        # Portuguese prompt
        prompt = (
            "You are an expert in fixing Portuguese receipt data with OCR errors.\n\n"
            "CRITICAL TASK: Fix product names AND correct prices that have OCR errors.\n"
            "The receipt total is usually correct, but individual prices often have OCR mistakes.\n\n"
            "RECEIPT DATA:\n"
            f"API_ID: {api_id}\n"
            + "\n".join(lines)
            + total_block
            + "\n\nYOUR TASKS:\n"
              "1. FIX PRODUCT NAMES (IMPORTANT - KEEP PORTUGUESE LANGUAGE):\n"
              "   - Expand abbreviations: 'P√ÉO' ‚Üí 'P√£o', 'LEITE' ‚Üí 'Leite'\n"
              "   - Fix typos: 'MANTEIGA' ‚Üí 'Manteiga', 'QUEIJO' ‚Üí 'Queijo'\n"
              "   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'\n"
              "   - Fix OCR errors: 'P√ÉO' ‚Üí 'P√£o', 'MANTEIGA' ‚Üí 'Manteiga'\n"
              "   - Keep Portuguese names: 'P√£o' stays 'P√£o' (NOT 'Bread')\n"
              "   - Use proper Portuguese capitalization: 'P√ÉO' ‚Üí 'P√£o'\n"
              "   - Fix Portuguese diacritics: '√°', '√†', '√¢', '√£', '√©', '√™', '√≠', '√≥', '√¥', '√µ', '√∫', '√ß'\n"
              "   - Common Portuguese products: P√£o, Leite, Manteiga, Queijo, Fiambre, Fruta, Legumes\n\n"
              "2. FIX PRICES (CRITICAL):\n"
              "   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)\n"
              "   - Check if quantity √ó unit price = total price for each item\n"
              "   - If not, adjust the unit price or quantity to make it consistent\n"
              "   - All corrected prices must sum up to the receipt total (if provided)\n"
              "   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4\n\n"
              "3. VALIDATION RULES:\n"
              "   - Each line: quantity √ó price_single MUST equal price_total\n"
              "   - Sum of all price_total MUST equal receipt total (if provided)\n"
              "   - Prices must be realistic for Portuguese supermarkets (‚Ç¨0.10 - ‚Ç¨50 per item)\n"
              "   - KEEP PRODUCT NAMES IN PORTUGUESE - DO NOT TRANSLATE TO ENGLISH\n\n"
              "PORTUGUESE PRODUCT EXAMPLES:\n"
              "   - 'P√£o 500g' ‚Üí 'P√£o 500 g' (NOT 'Bread 500 g')\n"
              "   - 'Fiambre 200g' ‚Üí 'Fiambre 200 g' (NOT 'Ham 200 g')\n"
              "   - 'Leite 1L' ‚Üí 'Leite 1 L'\n"
              "   - 'Queijo 300g' ‚Üí 'Queijo 300 g'\n\n"
              "OUTPUT FORMAT (strict JSON):\n"
            + "JSON schema not needed - using universal prompt"
            + "\n\nIMPORTANT: Ensure all arithmetic is correct, total matches, and product names stay in Portuguese!"
        )
    elif country_code == "RO":
        # Romanian prompt
        prompt = (
            "You are an expert in fixing Romanian receipt data with OCR errors.\n\n"
            "CRITICAL TASK: Fix product names AND correct prices that have OCR errors.\n"
            "The receipt total is usually correct, but individual prices often have OCR mistakes.\n\n"
            "RECEIPT DATA:\n"
            f"API_ID: {api_id}\n"
            + "\n".join(lines)
            + total_block
            + "\n\nYOUR TASKS:\n"
              "1. FIX PRODUCT NAMES (IMPORTANT - KEEP ROMANIAN LANGUAGE):\n"
              "   - Expand abbreviations: 'P√ÇINE' ‚Üí 'P√¢ine', 'LAPT' ‚Üí 'Lapte'\n"
              "   - Fix typos: 'UNTE' ‚Üí 'Unt', 'BR√ÇNZƒÇ' ‚Üí 'Br√¢nzƒÉ'\n"
              "   - Standardize units: '1L' ‚Üí '1 L', '500G' ‚Üí '500 g'\n"
              "   - Fix OCR errors: 'P√ÇINE' ‚Üí 'P√¢ine', 'UNTE' ‚Üí 'Unt'\n"
              "   - Keep Romanian names: 'P√¢ine' stays 'P√¢ine' (NOT 'Bread')\n"
              "   - Use proper Romanian capitalization: 'P√ÇINE' ‚Üí 'P√¢ine'\n"
              "   - Fix Romanian diacritics: 'ƒÉ', '√¢', '√Æ', '»ô', '»õ'\n"
              "   - Common Romanian products: P√¢ine, Lapte, Unt, Br√¢nzƒÉ, »òuncƒÉ, Fructe, Legume\n\n"
              "2. FIX PRICES (CRITICAL):\n"
              "   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)\n"
              "   - Check if quantity √ó unit price = total price for each item\n"
              "   - If not, adjust the unit price or quantity to make it consistent\n"
              "   - All corrected prices must sum up to the receipt total (if provided)\n"
              "   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4\n\n"
              "3. VALIDATION RULES:\n"
              "   - Each line: quantity √ó price_single MUST equal price_total\n"
              "   - Sum of all price_total MUST equal receipt total (if provided)\n"
              "   - Prices must be realistic for Romanian supermarkets (0.50 - 250 RON per item)\n"
              "   - KEEP PRODUCT NAMES IN ROMANIAN - DO NOT TRANSLATE TO ENGLISH\n\n"
              "ROMANIAN PRODUCT EXAMPLES:\n"
              "   - 'P√¢ine 500g' ‚Üí 'P√¢ine 500 g' (NOT 'Bread 500 g')\n"
              "   - '»òuncƒÉ 200g' ‚Üí '»òuncƒÉ 200 g' (NOT 'Ham 200 g')\n"
              "   - 'Lapte 1L' ‚Üí 'Lapte 1 L'\n"
              "   - 'Br√¢nzƒÉ 300g' ‚Üí 'Br√¢nzƒÉ 300 g'\n\n"
              "OUTPUT FORMAT (strict JSON):\n"
            + "JSON schema not needed - using universal prompt"
            + "\n\nIMPORTANT: Ensure all arithmetic is correct, total matches, and product names stay in Romanian!"
        )
    else:
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω
        prompt = f"""
You are an expert in {country_code} retail geography and receipt data correction.

CRITICAL TASK: Fix product names AND prices, AND determine the city from receipt data.

RECEIPT DATA:
- API_ID: {api_id}
- Country: {country_code}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Sample products from receipt:
{products_text}

RECEIPT ITEMS:
{chr(10).join(lines)}{total_block}

YOUR TASKS:
1. FIX PRODUCT NAMES:
   - Fix product names (brand first, title-case, ASCII where possible)
   - Standardize units, expand common abbreviations
   - Fix OCR errors and typos

2. FIX PRICES (CRITICAL):
   - OCR often misreads prices (e.g., 1.89 might be 1.39, 3.09 might be 8.09)
   - Check if quantity √ó unit price = total price for each item
   - If not, adjust the unit price or quantity to make it consistent
   - All corrected prices must sum up to the receipt total (if provided)
   - Common OCR errors: 3‚Üí8, 5‚Üí6, 1‚Üí7, 0‚Üí8, 9‚Üí4

3. DETERMINE CITY (CRITICAL - USE NATIONAL NAMES):
   - Use VAT/NIP first digits for regional hints if available
   - Consider retail chain geographic presence knowledge
   - Analyze product types for regional preferences
   - Use general European geographic knowledge
   - Consider population centers and economic hubs
   - ALWAYS use national city names: Warszawa (not Warsaw), Roma (not Rome), M√ºnchen (not Munich)
   - Check the country-specific city list below for correct national names

EUROPEAN RETAIL CHAINS KNOWLEDGE:
- ALDI: Strong in Germany, Netherlands, Belgium, France, Austria, Switzerland
- LIDL: Pan-European presence, strong in Germany, France, Italy, Spain, Poland
- CARREFOUR: Major cities in France, Spain, Italy, Belgium, Romania
- TESCO: UK, Ireland, Czech Republic, Hungary, Slovakia, Poland
- REWE: Germany, Austria, Czech Republic, Hungary
- SPAR: Pan-European, especially Austria, Netherlands, Germany, Italy
- EDEKA: Germany, strong regional presence
- AUCHAN: France, Poland, Romania, Russia, Ukraine
- INTERMARCHE: France, Belgium, Luxembourg, Portugal
- PENNY: Germany, Austria, Italy, Czech Republic

MAJOR EUROPEAN CITIES BY COUNTRY (USE NATIONAL NAMES):
- POLAND: Warszawa, Krak√≥w, ≈Å√≥d≈∫, Wroc≈Çaw, Pozna≈Ñ, Gda≈Ñsk, Szczecin, Bydgoszcz, Lublin, Katowice
- GERMANY: Berlin, Hamburg, M√ºnchen, K√∂ln, Frankfurt, Stuttgart, D√ºsseldorf, Dortmund, Essen, Leipzig
- FRANCE: Paris, Marseille, Lyon, Toulouse, Nice, Nantes, Montpellier, Strasbourg, Bordeaux, Lille
- ITALY: Roma, Milano, Napoli, Torino, Palermo, Genova, Bologna, Firenze, Bari, Catania
- SPAIN: Madrid, Barcelona, Valencia, Sevilla, Zaragoza, M√°laga, Murcia, Palma, Las Palmas, Bilbao
- NETHERLANDS: Amsterdam, Rotterdam, Den Haag, Utrecht, Eindhoven, Tilburg, Groningen, Almere, Breda, Nijmegen
- BELGIUM: Brussels, Antwerp, Ghent, Charleroi, Li√®ge, Bruges, Namur, Leuven, Mons, Aalst
- AUSTRIA: Vienna, Graz, Linz, Salzburg, Innsbruck, Klagenfurt, Villach, Wels, Sankt P√∂lten, Dornbirn
- SWITZERLAND: Z√ºrich, Geneva, Basel, Bern, Lausanne, Winterthur, Lucerne, St. Gallen, Lugano, Biel
- ROMANIA: Bucure»ôti, Cluj-Napoca, Timi»ôoara, Ia»ôi, Constan»õa, Craiova, Gala»õi, Ploie»ôti, Bra»ôov, BrƒÉila
- PORTUGAL: Lisboa, Porto, Braga, Set√∫bal, Coimbra, Queluz, Funchal, Cac√©m, Vila Nova de Gaia, Loures
- HUNGARY: Budapest, Debrecen, Szeged, Miskolc, P√©cs, Gy≈ër, Ny√≠regyh√°za, Kecskem√©t, Sz√©kesfeh√©rv√°r, Szombathely
- UNITED KINGDOM: London, Birmingham, Manchester, Glasgow, Liverpool, Leeds, Sheffield, Edinburgh, Bristol, Leicester

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "total_receipt": {total_receipt_str},
  "total_calculated": <sum of all corrected price_total>,
  "corrections_made": true/false,
  "products": [
    {{
      "name_original": "original name from receipt",
      "name_corrected": "fixed name",
      "quantity": integer,
      "price_single_original": number,
      "price_single_corrected": number,
      "price_total": number,
      "price_correction_reason": "OCR error: 3‚Üí8" or null
    }}
  ],
  "city_analysis": {{
    "city": "city name or UNKNOWN",
    "region": "region/province name or UNKNOWN",
    "city_population": "population number if known",
    "confidence": "HIGH/MEDIUM/LOW",
    "evidence": "brief explanation of how you identified the location"
  }}
}}

IMPORTANT: Ensure all arithmetic is correct, total matches, and provide best city estimate!

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON.
"""
    return prompt

def call_gemini_api(prompt: str) -> str:
    """Call the Gemini 2.0 Flash API with the given prompt."""
    if not GEMINI_API_KEY:
        raise RuntimeError("GEMINI_API_KEY is not set; cannot call Gemini API")
    url = (
        "https://generativelanguage.googleapis.com/v1beta/models/"
        "gemini-2.0-flash:generateContent?key=" + GEMINI_API_KEY
    )
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "temperature": 0,
            "maxOutputTokens": 8192,
            "candidateCount": 1,
            "responseMimeType": "application/json"
        },
    }
    response = requests.post(url, headers=headers, json=data, timeout=60)
    response.raise_for_status()
    j = response.json()
    return j.get("candidates", [{}])[0].get("content", {}).get("parts", [{}])[0].get("text", "")

def call_gemini_api_with_retry(prompt: str) -> str:
    """Call the Gemini API with retry mechanism for failed requests."""
    if BACKOFF_AVAILABLE:
        @backoff.on_exception(
            backoff.expo,
            (requests.RequestException, requests.HTTPError),
            max_tries=3,
            base=2,
            max_value=10,
            logger=logger
        )
        def _call_with_retry():
            return call_gemini_api(prompt)
        
        try:
            return _call_with_retry()
        except Exception as e:
            logger.error(f"Gemini API call failed after retries: {e}")
            raise
    else:
        # Fallback without retry
        return call_gemini_api(prompt)

def clean_and_parse_json(response_text: str) -> Dict[str, Any]:
    """Attempt to parse JSON from the Gemini response."""
    try:
        txt = response_text.strip()
        if "```json" in txt:
            txt = txt.split("```json", 1)[1].split("```", 1)[0].strip()
        elif "```" in txt:
            txt = txt.split("```", 1)[1].split("```", 1)[0].strip()
        try:
            parsed = json.loads(txt)
            # Ensure we return a dict, not a list
            if isinstance(parsed, list):
                logger.warning("Gemini returned a list instead of dict, wrapping in products key")
                return {"products": parsed}
            return parsed
        except json.JSONDecodeError:
            txt2 = re.sub(r"(\w+):", r'"\1":', txt)
            txt2 = re.sub(r",(\s*[}\]])", r"\1", txt2)
            txt2 = re.sub(r"\\(?![\"\\/bfnrt])", r"\\\\", txt2)
            parsed = json.loads(txt2)
            # Ensure we return a dict, not a list
            if isinstance(parsed, list):
                logger.warning("Gemini returned a list instead of dict (after fix), wrapping in products key")
                return {"products": parsed}
            return parsed
    except Exception as e:
        logger.warning("JSON parsing error: %s", e)
    return {"products": []}

def normalise_text(s: str) -> str:
    """Normalise a string for matching."""
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—å—Å–∫–∏–µ –¥–∏–∞–∫—Ä–∏—Ç–∏–∫–∏
    replacements = str.maketrans(
        "√†√°√¢√§√£√•√ß√®√©√™√´√¨√≠√Æ√Ø√±√≤√≥√¥√∂√µ√π√∫√ª√º√Ω≈æ√Ä√Å√Ç√Ñ√É√Ö√á√à√â√ä√ã√å√ç√é√è√ë√í√ì√î√ñ√ï√ô√ö√õ√ú√ù≈ΩƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª",
        "aaaaaaceeeeiiiinooooouuuuyzAAAAAACEEEEIIIINOOOOOUUUUYZacelnoszzACELNOSZZ",
    )
    s2 = s.translate(replacements)
    return re.sub(r"[^a-z0-9]+", "", s2.lower())

def extract_zip_code(address: str, country: str = "PL") -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—á—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∏–∑ –∞–¥—Ä–µ—Å–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω.
    """
    if not address:
        return None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
    zip_patterns = {
        "PL": r"\b\d{2}-\d{3}\b",  # –ü–æ–ª—å—à–∞: 00-000
        "IT": r"\b\d{5}\b",  # –ò—Ç–∞–ª–∏—è: 00000
        "GB": r"\b[A-Z]{1,2}\d{1,2}[A-Z]?\s?\d[A-Z]{2}\b",  # –í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏—è: SW1A 1AA
        "FR": r"\b\d{5}\b",  # –§—Ä–∞–Ω—Ü–∏—è: 00000
        "DE": r"\b\d{5}\b",  # –ì–µ—Ä–º–∞–Ω–∏—è: 00000
        "ES": r"\b\d{5}\b",  # –ò—Å–ø–∞–Ω–∏—è: 00000
        "PT": r"\b\d{4}-\d{3}\b",  # –ü–æ—Ä—Ç—É–≥–∞–ª–∏—è: 0000-000
        "RO": r"\b\d{6}\b",  # –†—É–º—ã–Ω–∏—è: 000000
        "HU": r"\b\d{4}\b",  # –í–µ–Ω–≥—Ä–∏—è: 0000
        "AT": r"\b\d{4}\b",  # –ê–≤—Å—Ç—Ä–∏—è: 0000
        "GR": r"\b\d{3}\s?\d{2}\b",  # –ì—Ä–µ—Ü–∏—è: 000 00
        "IE": r"\b[A-Z]\d{2}\s?[A-Z0-9]{4}\b",  # –ò—Ä–ª–∞–Ω–¥–∏—è: D02 AF30
        "NL": r"\b\d{4}\s?[A-Z]{2}\b",  # –ù–∏–¥–µ—Ä–ª–∞–Ω–¥—ã: 0000 AA
        "BE": r"\b\d{4}\b",  # –ë–µ–ª—å–≥–∏—è: 0000
        "FI": r"\b\d{5}\b",  # –§–∏–Ω–ª—è–Ω–¥–∏—è: 00000
        "SK": r"\b\d{3}\s?\d{2}\b",  # –°–ª–æ–≤–∞–∫–∏—è: 000 00
        "SI": r"\b\d{4}\b",  # –°–ª–æ–≤–µ–Ω–∏—è: 0000
        "EE": r"\b\d{5}\b",  # –≠—Å—Ç–æ–Ω–∏—è: 00000
        "LV": r"\b[LV]-?\d{4}\b",  # –õ–∞—Ç–≤–∏—è: LV-0000
        "LT": r"\b[LT]-?\d{5}\b",  # –õ–∏—Ç–≤–∞: LT-00000
        "LU": r"\b\d{4}\b",  # –õ—é–∫—Å–µ–º–±—É—Ä–≥: 0000
        "MT": r"\b[A-Z]{3}\s?\d{4}\b",  # –ú–∞–ª—å—Ç–∞: AAA 0000
        "CY": r"\b\d{4}\b",  # –ö–∏–ø—Ä: 0000
        "RS": r"\b\d{5,6}\b",  # –°–µ—Ä–±–∏—è: 00000 –∏–ª–∏ 000000
    }
    
    pattern = zip_patterns.get(country.upper(), r"\b\d{4,6}\b")  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º 4-6 —Ü–∏—Ñ—Ä
    
    matches = re.findall(pattern, address.upper())
    if matches:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å, –æ—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤
        zip_code = matches[0].replace(" ", "").replace("-", "")
        # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å—Ç—Ä–∞–Ω –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç
        if country.upper() in ["PL", "PT"]:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–µ—Ñ–∏—Å –¥–ª—è –ø–æ–ª—å—Å–∫–∏—Ö –∏ –ø–æ—Ä—Ç—É–≥–∞–ª—å—Å–∫–∏—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
            if len(zip_code) == 5:
                zip_code = f"{zip_code[:2]}-{zip_code[2:]}"
            elif len(zip_code) == 7:
                zip_code = f"{zip_code[:4]}-{zip_code[4:]}"
        return zip_code
    
    return None

def extract_province_code(address: str, country: str = "IT") -> Optional[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–¥ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –∏–∑ –∞–¥—Ä–µ—Å–∞ (–¥–ª—è –ò—Ç–∞–ª–∏–∏: RM, NA, MI –∏ —Ç.–¥.)
    
    Args:
        address: –ê–¥—Ä–µ—Å –∏–∑ —á–µ–∫–∞
        country: –ö–æ–¥ —Å—Ç—Ä–∞–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é IT)
    
    Returns:
        –ö–æ–¥ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "RM", "NA") –∏–ª–∏ None
    """
    if country != "IT" or not address:
        return None
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ò—Ç–∞–ª–∏–∏: (RM), (NA), (MI) –∏ —Ç.–¥.
    pattern = r'\(([A-Z]{2})\)'
    match = re.search(pattern, address.upper())
    if match:
        province_code = match.group(1)
        logger.info(f"Extracted province code: {province_code} from address: {address}")
        return province_code
    return None

def extract_addresses_from_receipt(raw_address: str) -> Tuple[Optional[str], Optional[str]]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–≤–∞ –∞–¥—Ä–µ—Å–∞ –∏–∑ —á–µ–∫–∞: –∞–¥—Ä–µ—Å –º–∞–≥–∞–∑–∏–Ω–∞ –∏ –∞–¥—Ä–µ—Å –≥–ª–∞–≤–Ω–æ–≥–æ –æ—Ñ–∏—Å–∞.
    –û–±—ã—á–Ω–æ –æ–Ω–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –∑–∞–ø—è—Ç—ã–º–∏, —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π –∏–ª–∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫.
    
    Returns:
        Tuple[shop_address, headquarters_address]
    """
    if not raw_address:
        return None, None
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –∞–¥—Ä–µ—Å–∞ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º
    # –û–±—ã—á–Ω–æ –ø–µ—Ä–≤—ã–π –∞–¥—Ä–µ—Å - —ç—Ç–æ –∞–¥—Ä–µ—Å –º–∞–≥–∞–∑–∏–Ω–∞, –≤—Ç–æ—Ä–æ–π - –≥–ª–∞–≤–Ω—ã–π –æ—Ñ–∏—Å
    separators = ["\n", ";", "|", "//"]
    
    addresses = [raw_address]
    for sep in separators:
        if sep in raw_address:
            addresses = [a.strip() for a in raw_address.split(sep, 1)]
            break
    
    shop_address = addresses[0] if len(addresses) > 0 else None
    headquarters_address = addresses[1] if len(addresses) > 1 else None
    
    return shop_address, headquarters_address

def parse_city_population(population_str: str) -> Optional[int]:
    """Parse city population from various formats like '1.79 million', '1790000', '1,790,000'."""
    if not population_str or population_str == "UNKNOWN":
        return None
    
    try:
        # Remove common suffixes and convert to lowercase
        text = str(population_str).lower().strip()
        
        # Handle "million" format
        if 'million' in text:
            # Extract number before "million"
            num_part = text.split('million')[0].strip()
            # Remove commas and convert to float
            num_part = num_part.replace(',', '').replace('.', '')
            if num_part.isdigit():
                return int(num_part) * 1000000
        
        # Handle "thousand" format  
        elif 'thousand' in text or 'k' in text:
            num_part = text.replace('thousand', '').replace('k', '').strip()
            num_part = num_part.replace(',', '').replace('.', '')
            if num_part.isdigit():
                return int(num_part) * 1000
        
        # Handle direct number format
        else:
            # Remove commas and try to convert
            clean_num = text.replace(',', '').replace('.', '')
            if clean_num.isdigit():
                return int(clean_num)
        
        return None
    except Exception as e:
        logger.warning(f"Failed to parse city population '{population_str}': {e}")
        return None

def normalize_network_name_by_country(network: str, country_code: str = "PL") -> str:
    """Normalize retail network names by country."""
    if not network or network == 'UNKNOWN':
        return 'UNKNOWN'
    
    if country_code == "PL":
        network_mapping = {
            # Major Polish chains
            'BIEDRONKA': 'BIEDRONKA', 'biedronka': 'BIEDRONKA', 'Biedronka': 'BIEDRONKA',
            '≈ªABKA': '≈ªABKA', 'zabka': '≈ªABKA', 'Zabka': '≈ªABKA', 'ZABKA': '≈ªABKA',
            'KAUFLAND': 'KAUFLAND', 'kaufland': 'KAUFLAND', 'Kaufland': 'KAUFLAND',
            'LIDL': 'LIDL', 'lidl': 'LIDL', 'Lidl': 'LIDL',
            'CARREFOUR': 'CARREFOUR', 'carrefour': 'CARREFOUR', 'Carrefour': 'CARREFOUR',
            'TESCO': 'TESCO', 'tesco': 'TESCO', 'Tesco': 'TESCO',
            'ALDI': 'ALDI', 'aldi': 'ALDI', 'Aldi': 'ALDI',
            'NETTO': 'NETTO', 'netto': 'NETTO', 'Netto': 'NETTO',
            'PENNY': 'PENNY MARKET', 'penny': 'PENNY MARKET', 'Penny': 'PENNY MARKET',
            'REAL': 'REAL', 'real': 'REAL', 'Real': 'REAL',
            'INTERMARCHE': 'INTERMARCHE', 'intermarche': 'INTERMARCHE', 'Intermarche': 'INTERMARCHE',
            'SPAR': 'SPAR', 'spar': 'SPAR', 'Spar': 'SPAR',
            'POLOMARKET': 'POLOMARKET', 'polomarket': 'POLOMARKET', 'Polomarket': 'POLOMARKET',
            'STOKROTKA': 'STOKROTKA', 'stokrotka': 'STOKROTKA', 'Stokrotka': 'STOKROTKA',
            'LEWIATAN': 'LEWIATAN', 'lewiatan': 'LEWIATAN', 'Lewiatan': 'LEWIATAN',
            'ABC': 'ABC', 'abc': 'ABC', 'Abc': 'ABC',
            'DELIKATESY': 'DELIKATESY', 'delikatesy': 'DELIKATESY', 'Delikatesy': 'DELIKATESY',
            'FRAC': 'FRAC', 'frac': 'FRAC', 'Frac': 'FRAC',
            'GROSZEK': 'GROSZEK', 'groszek': 'GROSZEK', 'Groszek': 'GROSZEK',
            'MILA': 'MILA', 'mila': 'MILA', 'Mila': 'MILA',
            'POLSKI': 'POLSKI', 'polski': 'POLSKI', 'Polski': 'POLSKI',
            'SKLEP': 'SKLEP', 'sklep': 'SKLEP', 'Sklep': 'SKLEP'
        }
    elif country_code == "IT":
        network_mapping = {
            # Major Italian chains (from old code)
            'CONAD': 'CONAD', 'conad': 'CONAD', 'Conad': 'CONAD',
            'COOP': 'COOP', 'coop': 'COOP', 'Coop': 'COOP',
            'ESSELUNGA': 'ESSELUNGA', 'esselunga': 'ESSELUNGA', 'Esselunga': 'ESSELUNGA',
            'CARREFOUR': 'CARREFOUR', 'carrefour': 'CARREFOUR', 'Carrefour': 'CARREFOUR',
            'LIDL': 'LIDL', 'lidl': 'LIDL', 'Lidl': 'LIDL',
            'EUROSPIN': 'EUROSPIN', 'eurospin': 'EUROSPIN', 'Eurospin': 'EUROSPIN',
            'PAM': 'PAM', 'pam': 'PAM', 'Pam': 'PAM',
            'DESPAR': 'DESPAR', 'despar': 'DESPAR', 'Despar': 'DESPAR',
            'SELEX': 'SELEX', 'selex': 'SELEX', 'Selex': 'SELEX',
            'FAMILA': 'FAMILA', 'famila': 'FAMILA', 'Famila': 'FAMILA',
            'BENNET': 'BENNET', 'bennet': 'BENNET', 'Bennet': 'BENNET',
            'CRAI': 'CRAI', 'crai': 'CRAI', 'Crai': 'CRAI',
            'MD DISCOUNT': 'MD DISCOUNT', 'md discount': 'MD DISCOUNT', 'MD': 'MD DISCOUNT',
            'PENNY MARKET': 'PENNY MARKET', 'penny market': 'PENNY MARKET', 'PENNY': 'PENNY MARKET'
        }
    elif country_code == "DE":
        network_mapping = {
            # Major German chains
            'ALDI': 'ALDI', 'aldi': 'ALDI', 'Aldi': 'ALDI',
            'LIDL': 'LIDL', 'lidl': 'LIDL', 'Lidl': 'LIDL',
            'REWE': 'REWE', 'rewe': 'REWE', 'Rewe': 'REWE',
            'EDEKA': 'EDEKA', 'edeka': 'EDEKA', 'Edeka': 'EDEKA',
            'KAUFLAND': 'KAUFLAND', 'kaufland': 'KAUFLAND', 'Kaufland': 'KAUFLAND',
            'REAL': 'REAL', 'real': 'REAL', 'Real': 'REAL',
            'NETTO': 'NETTO', 'netto': 'NETTO', 'Netto': 'NETTO',
            'PENNY': 'PENNY', 'penny': 'PENNY', 'Penny': 'PENNY',
            'NORMA': 'NORMA', 'norma': 'NORMA', 'Norma': 'NORMA',
            'TEGUT': 'TEGUT', 'tegut': 'TEGUT', 'Tegut': 'TEGUT',
            'SPAR': 'SPAR', 'spar': 'SPAR', 'Spar': 'SPAR',
            'DM': 'DM', 'dm': 'DM', 'Dm': 'DM',
            'ROSSMANN': 'ROSSMANN', 'rossmann': 'ROSSMANN', 'Rossmann': 'ROSSMANN'
        }
    elif country_code == "FR":
        network_mapping = {
            # Major French chains
            'CARREFOUR': 'CARREFOUR', 'carrefour': 'CARREFOUR', 'Carrefour': 'CARREFOUR',
            'LECLERC': 'LECLERC', 'leclerc': 'LECLERC', 'Leclerc': 'LECLERC',
            'AUCHAN': 'AUCHAN', 'auchan': 'AUCHAN', 'Auchan': 'AUCHAN',
            'CASINO': 'CASINO', 'casino': 'CASINO', 'Casino': 'CASINO',
            'INTERMARCHE': 'INTERMARCHE', 'intermarche': 'INTERMARCHE', 'Intermarch√©': 'INTERMARCHE',
            'SUPER U': 'SUPER U', 'super u': 'SUPER U', 'Super U': 'SUPER U',
            'SYSTEME U': 'SYSTEME U', 'systeme u': 'SYSTEME U', 'Syst√®me U': 'SYSTEME U',
            'LIDL': 'LIDL', 'lidl': 'LIDL', 'Lidl': 'LIDL',
            'ALDI': 'ALDI', 'aldi': 'ALDI', 'Aldi': 'ALDI',
            'MONOPRIX': 'MONOPRIX', 'monoprix': 'MONOPRIX', 'Monoprix': 'MONOPRIX',
            'FRANPRIX': 'FRANPRIX', 'franprix': 'FRANPRIX', 'Franprix': 'FRANPRIX',
            'SPAR': 'SPAR', 'spar': 'SPAR', 'Spar': 'SPAR'
        }
    else:
        # Universal mapping for other countries
        network_mapping = {
            'LIDL': 'LIDL', 'lidl': 'LIDL', 'Lidl': 'LIDL',
            'ALDI': 'ALDI', 'aldi': 'ALDI', 'Aldi': 'ALDI',
            'CARREFOUR': 'CARREFOUR', 'carrefour': 'CARREFOUR', 'Carrefour': 'CARREFOUR',
            'SPAR': 'SPAR', 'spar': 'SPAR', 'Spar': 'SPAR'
        }
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Å—Ç—Ä–æ–∫—É
    try:
        network_str = str(network) if network else 'UNKNOWN'
        network_upper = network_str.upper()
        if network_upper in [k.upper() for k in network_mapping]:
            for k, v in network_mapping.items():
                if k.upper() == network_upper:
                    return v
        
        return network_str.upper()
    except Exception as e:
        logger.warning(f"Error normalizing network name '{network}': {e}")
        return 'UNKNOWN'

# Backward compatibility
def normalize_polish_network_name(network: str) -> str:
    """Backward compatibility function for Polish networks."""
    return normalize_network_name_by_country(network, "PL")

def analyze_receipt_for_city(api_id: str, products_json: List[dict], nip: str = None, shopnetwork: str = None, country: str = "PL", raw_address: str = None) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int], Optional[str], Optional[str], Optional[str]]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á–µ–∫ —á–µ—Ä–µ–∑ Gemini API –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ NIP, –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –∏ —Å–µ—Ç–∏ –º–∞–≥–∞–∑–∏–Ω–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ zip_code –∏ —Ä–∞–∑–ª–∏—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –º–∞–≥–∞–∑–∏–Ω–∞ –∏ –≥–ª–∞–≤–Ω–æ–≥–æ –æ—Ñ–∏—Å–∞.
    """
    # DEBUG: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ country
    logger.info(f"=== AI ANALYSIS DEBUG ===")
    logger.info(f"API_ID: {api_id}, COUNTRY: {country}")
    logger.info(f"NIP: {nip}, SHOPNETWORK: {shopnetwork}")
    logger.info(f"Raw Address: {raw_address}")
    
    if not products_json:
        return None, None, None, None, None, None, None
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å–∞ (–º–∞–≥–∞–∑–∏–Ω –∏ –≥–ª–∞–≤–Ω—ã–π –æ—Ñ–∏—Å)
    shop_address, headquarters_address = extract_addresses_from_receipt(raw_address) if raw_address else (None, None)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º zip_code –∏–∑ –∞–¥—Ä–µ—Å–∞ –º–∞–≥–∞–∑–∏–Ω–∞ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) –∏–ª–∏ –∏–∑ –æ–±—â–µ–≥–æ –∞–¥—Ä–µ—Å–∞
    zip_code = None
    if shop_address:
        zip_code = extract_zip_code(shop_address, country)
    if not zip_code and raw_address:
        zip_code = extract_zip_code(raw_address, country)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ –ø—Ä–æ–≤–∏–Ω—Ü–∏–∏ –¥–ª—è –ò—Ç–∞–ª–∏–∏
    province_code = None
    if country == "IT":
        if shop_address:
            province_code = extract_province_code(shop_address, country)
        if not province_code and raw_address:
            province_code = extract_province_code(raw_address, country)
    
    logger.info(f"Extracted addresses - Shop: {shop_address}, HQ: {headquarters_address}, ZIP: {zip_code}, Province: {province_code}")
    
    # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã (–∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ)
    unique = []
    seen = set()
    for p in products_json[:15]:
        name = (p.get('name') or '').upper()
        if name and len(name) > 2 and name not in seen:
            unique.append(name)
            seen.add(name)
    products_text = "\n".join(f"- {p}" for p in unique[:10])
    
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
    if country == "PL":
        prompt = f"""
You are an expert in Polish retail geography and store networks. Analyze this Polish receipt to determine the most likely city.

CRITICAL MATCHING PRIORITY (use in this exact order):
1. ZIP CODE MATCH (HIGHEST PRIORITY) ‚≠ê‚≠ê‚≠ê
   - If zip_code is provided and not 'none', use it FIRST
   - Match city by zip_code from your knowledge
   - Example: zip_code "20-315" ‚Üí Lublin (Poland)
   - Example: zip_code "00-001" ‚Üí Warszawa (Poland)
   
2. STREET ADDRESS CONTEXT ‚≠ê
   - Use street name to disambiguate between similar cities
   
3. NORMALIZED CITY NAME SIMILARITY
   - Use only if ZIP not available
   
4. NIP REGION HINTS
   - Use only as last resort

RECEIPT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Shop Address (if separated): {shop_address or 'none'}
- Headquarters Address (if separated): {headquarters_address or 'none'}
- Extracted ZIP Code: {zip_code or 'none'} {'‚≠ê USE THIS FIRST!' if zip_code and zip_code != 'none' else ''}
- Sample products from receipt:
{products_text}

IMPORTANT: Receipts often contain TWO addresses:
1. Shop/Store Address - This is the location where the purchase was made (USE THIS for city determination)
2. Headquarters Address - This is the company's main office (IGNORE THIS for city determination)

If addresses are separated, prioritize the SHOP ADDRESS for determining the city and zip code.

EXAMPLES OF CORRECT MATCHING:

Example 1:
- Address: "20-315 Lublin, Al.Witosa 8"
- ZIP Code: "20-315"
- ‚úÖ CORRECT: Match by ZIP "20-315" ‚Üí Lublin (Lubelskie)
- ‚ùå WRONG: Ignore ZIP and use only "Lublin" text

Example 2:
- Address: "00-001 Warszawa, ul. Marsza≈Çkowska 1"
- ZIP Code: "00-001"
- ‚úÖ CORRECT: Match by ZIP "00-001" ‚Üí Warszawa (Mazowieckie)
- ‚ùå WRONG: Ignore ZIP and guess randomly

POLISH RETAIL CHAINS KNOWLEDGE:
- BIEDRONKA: Strong in all major cities, especially Warszawa, Krak√≥w, Wroc≈Çaw
- ≈ªABKA: Especially strong in Warszawa, Krak√≥w, Wroc≈Çaw, Pozna≈Ñ
- KAUFLAND: Major cities, especially Wroc≈Çaw, Pozna≈Ñ, Warszawa
- LIDL: All major cities, strong presence everywhere
- CARREFOUR: Warszawa, Krak√≥w, Pozna≈Ñ, Wroc≈Çaw
- TESCO: Major cities, especially Warszawa, Krak√≥w
- ALDI: Growing presence in major cities
- NETTO: Strong in northern regions

GEOGRAPHIC STRATEGY:
1. Use NIP first digits for regional hints:
   - 10-19: Mazowieckie (Warszawa area)
   - 20-29: Ma≈Çopolskie (Krak√≥w area) 
   - 30-39: Lubelskie, Podkarpackie
   - 40-49: ≈ölƒÖskie (Katowice area)
   - 50-59: Dolno≈õlƒÖskie (Wroc≈Çaw area)
   - 60-69: Wielkopolskie (Pozna≈Ñ area)
   - 70-79: Zachodniopomorskie (Szczecin area)
   - 80-89: Pomorskie (Gda≈Ñsk area)
   - 90-99: Warmi≈Ñsko-mazurskie, Podlaskie

2. Consider all Polish cities and towns (population 50,000+)
3. Use retail chain geographic presence knowledge
4. Analyze product types for regional preferences
5. Provide population estimates based on city size knowledge

POLISH REGIONS AND MAJOR CITIES (USE STANDARD FORMS):
- MAZOWIECKIE: Warszawa, Radom, P≈Çock, Siedlce, Ostro≈Çƒôka
- MA≈ÅOPOLSKIE: Krak√≥w, Tarn√≥w, Nowy SƒÖcz, O≈õwiƒôcim
- ≈öLƒÑSKIE: Katowice, Czƒôstochowa, Sosnowiec, Gliwice, Zabrze, Bytom
- WIELKOPOLSKIE: Pozna≈Ñ, Kalisz, Konin, Pi≈Ça
- DOLNO≈öLƒÑSKIE: Wroc≈Çaw, Wa≈Çbrzych, Legnica, Jelenia G√≥ra
- ≈Å√ìDZKIE: ≈Å√≥d≈∫, Piotrk√≥w Trybunalski, Pabianice, Tomasz√≥w Mazowiecki
- POMORSKIE: Gda≈Ñsk, Gdynia, Sopot, S≈Çupsk, Tczew
- ZACHODNIOPOMORSKIE: Szczecin, Koszalin, Stargard, Ko≈Çobrzeg
- LUBELSKIE: Lublin, Che≈Çm, Zamo≈õƒá, Bia≈Ça Podlaska
- PODKARPACKIE: Rzesz√≥w, Przemy≈õl, Stalowa Wola, Mielec
- PODLASKIE: Bia≈Çystok, Suwa≈Çki, ≈Åom≈ºa, August√≥w
- WARMI≈ÉSKO-MAZURSKIE: Olsztyn, ElblƒÖg, E≈Çk, Ostr√≥da
- KUJAWSKO-POMORSKIE: Bydgoszcz, Toru≈Ñ, W≈Çoc≈Çawek, GrudziƒÖdz
- ≈öWIƒòTOKRZYSKIE: Kielce, Ostrowiec ≈öwiƒôtokrzyski, Starachowice
- LUBUSKIE: Zielona G√≥ra, Gorz√≥w Wielkopolski, ≈ªary, Nowa S√≥l
- OPOLSKIE: Opole, Kƒôdzierzyn-Ko≈∫le, Nysa, Brzeg

IMPORTANT: Always use standard city names like "Warszawa" (not "WARSAW"), "Krak√≥w" (not "CRACOW"), "Gda≈Ñsk" (not "DANZIG").

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "nip": "{nip or 'none'}",
  "city": "city name or UNKNOWN (use standard forms: Warszawa, Krak√≥w, Gda≈Ñsk)",
  "region": "region name or UNKNOWN",
  "zip_code": "postal code (e.g., 00-000 for Poland, 00000 for Italy) or null if not found",
  "city_population": "population number (e.g., 1791000 for Warszawa, 779000 for Krak√≥w)",
  "match_method": "zip_code_match" | "address_match" | "name_match" | "nip_hint",
  "confidence": "HIGH/MEDIUM/LOW",
  "evidence": "brief explanation of how you identified the location, including which address was used (shop vs headquarters) and which matching method was used"
}}

CONFIDENCE LEVELS:
- HIGH: Multiple clear indicators (product brands + geographic logic + NIP region match)
- MEDIUM: Some indicators with reasonable geographic assumptions
- LOW: Best educated guess based on limited evidence

IMPORTANT: 
1. Always attempt to provide city and region even if uncertain. 
2. ALWAYS provide city_population - this is mandatory! Use your knowledge of Polish cities:
   - Warszawa: ~1,790,000
   - Krak√≥w: ~779,000  
   - Wroc≈Çaw: ~643,000
   - Gda≈Ñsk: ~470,000
   - Pozna≈Ñ: ~534,000
   - ≈Å√≥d≈∫: ~677,000
   - Katowice: ~294,000
   - Lublin: ~339,000
   - Bia≈Çystok: ~297,000
   - Szczecin: ~400,000
   - Gdynia: ~246,000
   - Bydgoszcz: ~346,000
   - Lublin: ~339,000
   - Czƒôstochowa: ~214,000
   - Radom: ~211,000
   - Sosnowiec: ~199,000
   - Toru≈Ñ: ~201,000
   - Kielce: ~194,000
   - Gliwice: ~178,000
   - Zabrze: ~171,000
   - Bytom: ~164,000
   - Olsztyn: ~171,000
   - Rzesz√≥w: ~196,000
   - Ruda ≈ölƒÖska: ~136,000
   - Rybnik: ~138,000
   - Tychy: ~127,000
   - DƒÖbrowa G√≥rnicza: ~119,000
   - ElblƒÖg: ~119,000
   - P≈Çock: ~119,000
   - Wa≈Çbrzych: ~112,000
   - W≈Çoc≈Çawek: ~108,000
   - Tarn√≥w: ~108,000
   - Chorz√≥w: ~107,000
   - Kalisz: ~100,000
   - Koszalin: ~107,000
   - Legnica: ~99,000
   - GrudziƒÖdz: ~95,000
   - S≈Çupsk: ~90,000
   - Jaworzno: ~89,000
   - Jastrzƒôbie-Zdr√≥j: ~87,000
   - Nowy SƒÖcz: ~83,000
   - Jelenia G√≥ra: ~79,000
   - Konin: ~73,000
   - Piotrk√≥w Trybunalski: ~72,000
   - Lubin: ~71,000
   - Inowroc≈Çaw: ~70,000
   - Ostr√≥w Wielkopolski: ~69,000
   - Stargard: ~67,000
   - Mys≈Çenice: ~65,000
   - Pabianice: ~63,000
   - Gniezno: ~68,000
   - Ostr√≥w Mazowiecka: ~22,000
   - S≈Çupca: ~13,000
   - ≈ªywiec: ~31,000
   - Stalowa Wola: ~60,000
   - Mielec: ~59,000
   - ≈Åƒôczyca: ~14,000
   - Tarnobrzeg: ~46,000
   - Pu≈Çawy: ~47,000
   - Ole≈õnica: ~36,000
   - Gorz√≥w Wielkopolski: ~123,000
   - W≈Çoc≈Çawek: ~108,000
   - Zielona G√≥ra: ~140,000
   - Krosno: ~45,000
   - Legionowo: ~64,000
   - Skar≈ºysko-Kamienna: ~44,000
   - Radomsko: ~46,000
   - O≈õwiƒôcim: ~37,000
   - Starachowice: ~48,000
   - Zawiercie: ~49,000
   - Miƒôdzyrzecz: ~18,000
   - P≈Ço≈Ñsk: ~22,000
   - O≈Çawa: ~32,000
   - G≈Çog√≥w: ~67,000
   - Jaros≈Çaw: ~37,000
   - Nowy Targ: ~33,000
   - Jas≈Ço: ~35,000
   - Kƒôtrzyn: ~27,000
   - Racib√≥rz: ~54,000
   - ≈öwiƒôtoch≈Çowice: ~49,000
3. Use geographic knowledge about Polish retail distribution patterns. 
4. Prefer real Polish city names over 'UNKNOWN'. 
5. Use standard city forms (Warszawa, Krak√≥w, Gda≈Ñsk).

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON."""
    elif country == "IT":
        prompt = f"""
You are an expert in Italian retail geography and store chains.

CRITICAL MATCHING PRIORITY (use in this exact order):
1. ZIP CODE MATCH (HIGHEST PRIORITY) ‚≠ê‚≠ê‚≠ê
   - If zip_code is provided and not 'none', use it FIRST
   - Match city by zip_code from your knowledge
   - Example: zip_code "80126" ‚Üí NAPOLI (Campania)
   - Example: zip_code "00100" ‚Üí ROMA (Lazio)
   
2. PROVINCE/REGION CODE MATCH ‚≠ê‚≠ê
   - If address contains (RM), (NA), (MI), etc. ‚Üí filter by province
   - Example: "(RM)" ‚Üí Roma province ‚Üí filter cities in Lazio
   - Example: "(NA)" ‚Üí Napoli province ‚Üí filter cities in Campania
   
3. STREET ADDRESS CONTEXT ‚≠ê
   - Use street name to disambiguate between similar cities
   - Example: "VIA GIUSTINIANO" in Napoli vs other cities
   
4. NORMALIZED CITY NAME SIMILARITY
   - Use only if ZIP/province not available
   
5. NIP REGION HINTS
   - Use only as last resort

REQUEST: We need the name of the chain of stores/restaurants (shop_chain), the city, shop street name, zip code, 
building number and integer population size of that city (city_population) of Italy. If possible, derive it from the 
receipt data, NIP number, or from clues and general knowledge about Italian geography and retail distribution.

If totally not possible please return 'UNKNOWN' as value. Try to provide best-guess based on available information. 
Try to fix mistakes in names and use standard Italian geographic naming.

INPUT DATA:
- API_ID: {api_id}
- NIP (VAT Number): {nip or 'none'}
- Raw Address: {raw_address or 'none'}
- Shop Address (if separated): {shop_address or 'none'}
- Headquarters Address (if separated): {headquarters_address or 'none'}
- Extracted ZIP Code: {zip_code or 'none'} {'‚≠ê USE THIS FIRST!' if zip_code and zip_code != 'none' else ''}
- Extracted Province Code: {province_code or 'none'} {'‚≠ê USE THIS SECOND!' if province_code and province_code != 'none' else ''}
- Sample products from receipt:
{products_text}

IMPORTANT: Receipts often contain TWO addresses:
1. Shop/Store Address - This is the location where the purchase was made (USE THIS for city determination)
2. Headquarters Address - This is the company's main office (IGNORE THIS for city determination)

If addresses are separated, prioritize the SHOP ADDRESS for determining the city and zip code.

EXAMPLES OF CORRECT MATCHING:

Example 1:
- Address: "VIA GIUSTINIANO, 150-80126 NAPOLI"
- ZIP Code: "80126"
- ‚úÖ CORRECT: Match by ZIP "80126" ‚Üí NAPOLI (Campania)
- ‚ùå WRONG: Ignore ZIP and use only "NAPOLI" text

Example 2:
- Address: "VIA ULE PLATANI, 10 MANTEANA (RM)"
- Province: "RM"
- ZIP Code: None
- ‚úÖ CORRECT: Filter by province "RM" ‚Üí Roma area ‚Üí MANTEANA (Lazio)
- ‚ùå WRONG: Ignore province and guess randomly

Example 3:
- Address: "VIA ROMA, 1-00100 ROMA"
- ZIP Code: "00100"
- Province: None
- ‚úÖ CORRECT: Match by ZIP "00100" ‚Üí ROMA (Lazio)

ITALIAN RETAIL CHAINS KNOWLEDGE:
- CONAD: Cooperative stores‚Ä¶ "CONAD", "C-", "FIOR FIORE"
- CARREFOUR: ‚Ä¶ "TERRE D'ITALIA"
- ESSELUNGA: ‚Ä¶ Lombardia, "NATURAMA"
- COOP: ‚Ä¶ "SAPORI&DINTORNI"
- LIDL: ‚Ä¶ discount
- EUROSPIN: ‚Ä¶ very cheap, "TRE MULINI"
- PAM / PANORAMA, DESPAR, PENNY MARKET, MD DISCOUNT, SELEX, FAMILA, BENNET, CRAI

GEOGRAPHIC STRATEGY:
1. Use NIP first digits for regional hints (Northern Italy: 01-19, Central: 20-59, Southern: 60-99)
2. Consider all Italian cities and towns (population 10,000+)
3. Use retail chain geographic presence knowledge
4. Estimate reasonable street addresses and zip codes
5. Provide population estimates based on city size knowledge

ITALIAN REGIONS AND MAJOR CITIES (USE STANDARD FORMS):
- LAZIO: Roma, Latina, Frosinone, Viterbo, Rieti
- LOMBARDIA: Milano, Bergamo, Brescia, Monza, Como, Varese, Pavia, Cremona
- CAMPANIA: Napoli, Salerno, Caserta, Avellino, Benevento
- PIEMONTE: Torino, Alessandria, Novara, Cuneo, Asti
- SICILIA: Palermo, Catania, Messina, Siracusa, Agrigento
- VENETO: Venezia, Verona, Padova, Vicenza, Treviso
- EMILIA-ROMAGNA: Bologna, Modena, Parma, Reggio Emilia, Ravenna, Ferrara
- TOSCANA: Firenze, Pisa, Livorno, Prato, Siena, Arezzo
- PUGLIA: Bari, Taranto, Foggia, Lecce, Brindisi
- LIGURIA: Genova, La Spezia, Savona, Imperia
- CALABRIA: Reggio Calabria, Catanzaro, Cosenza
- MARCHE: Ancona, Pesaro, Macerata, Ascoli Piceno
- ABRUZZO: L'Aquila, Pescara, Chieti, Teramo
- UMBRIA: Perugia, Terni

IMPORTANT: Always use standard city names like "Roma" (not "ROMA" or "Rome"), "Milano" (not "MILAN"), "Napoli" (not "Naples").

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "nip": "{nip or 'none'}",
  "shop_chain": "identified chain name or UNKNOWN",
  "city": "city name or UNKNOWN (use standard forms: Roma, Milano, Napoli)",
  "region": "region name or UNKNOWN",
  "shop_address": "street name and building number or VIA ROMA 1",
  "zip_code": "5-digit postal code or 00000",
  "province_code": "extracted province code (RM, NA, MI, etc.) or null",
  "city_population": integer,
  "match_method": "zip_code_match" | "province_match" | "address_match" | "name_match" | "nip_hint",
  "confidence": "HIGH/MEDIUM/LOW",
  "evidence": "brief explanation of how you identified the location and chain, including which matching method was used"
}}

CONFIDENCE LEVELS:
- HIGH: Multiple clear indicators (product brands + geographic logic + NIP region match)
- MEDIUM: Some indicators with reasonable geographic assumptions
- LOW: Best educated guess based on limited evidence

IMPORTANT: 
1. Always attempt to provide city and region even if uncertain. 
2. ALWAYS provide city_population - this is mandatory! Use your knowledge of Italian cities:
   - Roma: ~2,800,000
   - Milano: ~1,400,000
   - Napoli: ~910,000
   - Torino: ~848,000
   - Palermo: ~630,000
   - Genova: ~570,000
   - Bologna: ~390,000
   - Firenze: ~366,000
   - Bari: ~315,000
   - Catania: ~298,000
   - Venezia: ~258,000
   - Verona: ~258,000
   - Messina: ~232,000
   - Padova: ~210,000
   - Trieste: ~200,000
   - Brescia: ~196,000
   - Parma: ~195,000
   - Taranto: ~188,000
   - Prato: ~185,000
   - Modena: ~184,000
   - Reggio Calabria: ~180,000
   - Reggio Emilia: ~170,000
   - Perugia: ~165,000
   - Livorno: ~157,000
   - Ravenna: ~155,000
   - Cagliari: ~154,000
   - Foggia: ~150,000
   - Rimini: ~148,000
   - Salerno: ~133,000
   - Ferrara: ~132,000
   - Sassari: ~127,000
   - Monza: ~123,000
   - Bergamo: ~120,000
   - Forl√¨: ~118,000
   - Trento: ~117,000
   - Vicenza: ~111,000
   - Terni: ~110,000
   - Bolzano: ~106,000
   - Novara: ~104,000
   - Piacenza: ~103,000
   - Ancona: ~101,000
   - Andria: ~100,000
   - Arezzo: ~99,000
   - Udine: ~99,000
   - Cesena: ~97,000
   - L'Aquila: ~70,000
   - La Spezia: ~93,000
   - Pescara: ~120,000
   - Como: ~84,000
   - Pisa: ~89,000
   - Treviso: ~85,000
   - Varese: ~81,000
   - Busto Arsizio: ~83,000
   - Vigevano: ~63,000
   - Gallarate: ~54,000
   - Saronno: ~39,000
   - Legnano: ~60,000
   - Rho: ~51,000
   - Cinisello Balsamo: ~76,000
   - Sesto San Giovanni: ~81,000
   - Cologno Monzese: ~47,000
   - Paderno Dugnano: ~47,000
   - Rozzano: ~42,000
   - Pioltello: ~37,000
   - Segrate: ~39,000
   - San Giuliano Milanese: ~37,000
   - Corsico: ~34,000
   - Cesano Maderno: ~38,000
   - Limbiate: ~35,000
   - Bollate: ~36,000
   - Arese: ~19,000
   - Garbagnate Milanese: ~27,000
   - Lainate: ~26,000
   - Pero: ~11,000
   - Baranzate: ~11,000
   - Solaro: ~14,000
   - Bresso: ~26,000
   - Cormano: ~20,000
   - Cusano Milanino: ~20,000
   - Novate Milanese: ~20,000
   - Settimo Milanese: ~19,000
   - Vimodrone: ~16,000
   - Peschiera Borromeo: ~23,000
   - San Donato Milanese: ~32,000
   - Mediglia: ~12,000
   - Pieve Emanuele: ~15,000
   - Opera: ~13,000
   - Locate di Triulzi: ~10,000
   - San Zenone al Lambro: ~4,000
   - Zibido San Giacomo: ~6,000
   - Vermezzo con Zelo: ~8,000
   - Bubbiano: ~2,000
   - Calvignasco: ~1,000
   - Casirate d'Adda: ~4,000
   - Cassina de' Pecchi: ~13,000
   - Gessate: ~9,000
   - Gorgonzola: ~20,000
   - Grezzago: ~3,000
   - Inzago: ~11,000
   - Masate: ~3,000
   - Melzo: ~18,000
   - Pozzo d'Adda: ~6,000
   - Trezzano Rosa: ~5,000
   - Trezzo sull'Adda: ~12,000
   - Truccazzano: ~6,000
   - Vaprio d'Adda: ~8,000
   - Vignate: ~9,000
   - Basiano: ~3,000
   - Cambiago: ~6,000
   - Carugate: ~15,000
   - Cernusco sul Naviglio: ~31,000
   - Cologno Monzese: ~47,000
   - Gorgonzola: ~20,000
   - Inzago: ~11,000
   - Liscate: ~4,000
   - Melzo: ~18,000
   - Pioltello: ~37,000
   - Pozzo d'Adda: ~6,000
   - Rodano: ~5,000
   - Settala: ~7,000
   - Truccazzano: ~6,000
   - Vignate: ~9,000
   - Vimodrone: ~16,000
3. Use geographic knowledge about Italian retail distribution patterns. 
4. Prefer real Italian city names over 'UNKNOWN'. 
5. Use standard city forms (Roma, Milano, Napoli).

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON."""
    else:
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω (–ì–µ—Ä–º–∞–Ω–∏—è, –§—Ä–∞–Ω—Ü–∏—è, –ò—Å–ø–∞–Ω–∏—è –∏ –¥—Ä.)
        prompt = f"""
You are an expert in European retail geography and store networks. Analyze this receipt to determine the most likely city.

CRITICAL MATCHING PRIORITY (use in this exact order):
1. ZIP CODE MATCH (HIGHEST PRIORITY) ‚≠ê‚≠ê‚≠ê
   - If zip_code is provided and not 'none', use it FIRST
   - Match city by zip_code from your knowledge
   - Example: zip_code "18690" ‚Üí ALMUNECAR (Spain)
   - Example: zip_code "75001" ‚Üí PARIS (France)
   
2. STREET ADDRESS CONTEXT ‚≠ê
   - Use street name to disambiguate between similar cities
   
3. NORMALIZED CITY NAME SIMILARITY
   - Use only if ZIP not available
   
4. VAT/NIP REGION HINTS
   - Use only as last resort

RECEIPT DATA:
- API_ID: {api_id}
- NIP/VAT Number: {nip or 'none'}
- Store Network: {shopnetwork or 'none'}
- Raw Address: {raw_address or 'none'}
- Shop Address (if separated): {shop_address or 'none'}
- Headquarters Address (if separated): {headquarters_address or 'none'}
- Extracted ZIP Code: {zip_code or 'none'} {'‚≠ê USE THIS FIRST!' if zip_code and zip_code != 'none' else ''}
- Sample products from receipt:
{products_text}

IMPORTANT: Receipts often contain TWO addresses:
1. Shop/Store Address - This is the location where the purchase was made (USE THIS for city determination)
2. Headquarters Address - This is the company's main office (IGNORE THIS for city determination)

If addresses are separated, prioritize the SHOP ADDRESS for determining the city and zip code.

EXAMPLES OF CORRECT MATCHING:

Example 1 (Spain):
- Address: "C/ MARIANA PINEDA S/N, 18690 ALMUNECAR"
- ZIP Code: "18690"
- ‚úÖ CORRECT: Match by ZIP "18690" ‚Üí ALMUNECAR (Andalusia)
- ‚ùå WRONG: Ignore ZIP and use only "ALMUNECAR" text

Example 2 (France):
- Address: "RUE DE LA PAIX, 75001 PARIS"
- ZIP Code: "75001"
- ‚úÖ CORRECT: Match by ZIP "75001" ‚Üí PARIS (√éle-de-France)
- ‚ùå WRONG: Ignore ZIP and guess randomly

EUROPEAN RETAIL CHAINS KNOWLEDGE:
- ALDI: Strong in Germany, Netherlands, Belgium, France, Austria, Switzerland
- LIDL: Pan-European presence, strong in Germany, France, Italy, Spain, Poland
- CARREFOUR: Major cities in France, Spain, Italy, Belgium, Romania
- TESCO: UK, Ireland, Czech Republic, Hungary, Slovakia, Poland
- REWE: Germany, Austria, Czech Republic, Hungary
- SPAR: Pan-European, especially Austria, Netherlands, Germany, Italy
- EDEKA: Germany, strong regional presence
- AUCHAN: France, Poland, Romania, Russia, Ukraine
- INTERMARCHE: France, Belgium, Luxembourg, Portugal
- PENNY: Germany, Austria, Italy, Czech Republic

GEOGRAPHIC STRATEGY:
1. Use VAT/NIP first digits for regional hints if available
2. Consider retail chain geographic presence knowledge
3. Analyze product types for regional preferences (local brands, languages)
4. Use general European geographic knowledge
5. Consider population centers and economic hubs

MAJOR EUROPEAN CITIES BY COUNTRY (USE NATIONAL NAMES):
- POLAND (PL): Warszawa, Krak√≥w, ≈Å√≥d≈∫, Wroc≈Çaw, Pozna≈Ñ, Gda≈Ñsk, Szczecin, Bydgoszcz, Lublin, Katowice
- GERMANY (DE): Berlin, Hamburg, M√ºnchen, K√∂ln, Frankfurt, Stuttgart, D√ºsseldorf, Dortmund, Essen, Leipzig
- FRANCE (FR): Paris, Marseille, Lyon, Toulouse, Nice, Nantes, Montpellier, Strasbourg, Bordeaux, Lille
- ITALY (IT): Roma, Milano, Napoli, Torino, Palermo, Genova, Bologna, Firenze, Bari, Catania
- SPAIN (ES): Madrid, Barcelona, Valencia, Sevilla, Zaragoza, M√°laga, Murcia, Palma, Las Palmas, Bilbao
- NETHERLANDS (NL): Amsterdam, Rotterdam, Den Haag, Utrecht, Eindhoven, Tilburg, Groningen, Almere, Breda, Nijmegen
- BELGIUM (BE): Brussels, Antwerp, Ghent, Charleroi, Li√®ge, Bruges, Namur, Leuven, Mons, Aalst
- AUSTRIA (AT): Vienna, Graz, Linz, Salzburg, Innsbruck, Klagenfurt, Villach, Wels, Sankt P√∂lten, Dornbirn
- ROMANIA (RO): Bucure»ôti, Cluj-Napoca, Timi»ôoara, Ia»ôi, Constan»õa, Craiova, Gala»õi, Ploie»ôti, Bra»ôov, BrƒÉila
- PORTUGAL (PT): Lisboa, Porto, Braga, Set√∫bal, Coimbra, Queluz, Funchal, Cac√©m, Vila Nova de Gaia, Loures
- HUNGARY (HU): Budapest, Debrecen, Szeged, Miskolc, P√©cs, Gy≈ër, Ny√≠regyh√°za, Kecskem√©t, Sz√©kesfeh√©rv√°r, Szombathely
- UNITED KINGDOM (GB): London, Birmingham, Manchester, Glasgow, Liverpool, Leeds, Sheffield, Edinburgh, Bristol, Leicester
- SERBIA (RS): Belgrade, Novi Sad, Ni≈°, Kragujevac, Subotica, Zrenjanin, Panƒçevo, ƒåaƒçak, Novi Pazar, Kraljevo
- ESTONIA (EE): Tallinn, Tartu, Narva, P√§rnu, Kohtla-J√§rve, Viljandi, Rakvere, Maardu, Kuressaare, V√µru
- GREECE (GR): Athens, Thessaloniki, Patras, Heraklion, Larissa, Volos, Ioannina, Kavala, Kalamata, Rhodes
- IRELAND (IE): Dublin, Cork, Limerick, Galway, Waterford, Drogheda, Dundalk, Swords, Bray, Navan
- FINLAND (FI): Helsinki, Espoo, Tampere, Vantaa, Oulu, Turku, Jyv√§skyl√§, Lahti, Kuopio, Pori
- SLOVAKIA (SK): Bratislava, Ko≈°ice, Pre≈°ov, ≈Ωilina, Nitra, Bansk√° Bystrica, Trnava, Martin, Trenƒç√≠n, Poprad
- SLOVENIA (SI): Ljubljana, Maribor, Celje, Kranj, Velenje, Koper, Novo Mesto, Ptuj, Trbovlje, Kamnik
- LATVIA (LV): Riga, Daugavpils, LiepƒÅja, Jelgava, J≈´rmala, Ventspils, Rƒìzekne, Valmiera, Ogre, Tukums
- LITHUANIA (LT): Vilnius, Kaunas, Klaipƒóda, ≈†iauliai, Panevƒó≈æys, Alytus, Marijampolƒó, Ma≈æeikiai, Jonava, Utena
- LUXEMBOURG (LU): Luxembourg City, Esch-sur-Alzette, Differdange, Dudelange, P√©tange, Sanem, Hesperange, Bettembourg, Schifflange, Kayl
- MALTA (MT): Valletta, Birkirkara, Mosta, Qormi, ≈ªabbar, San Pawl il-Baƒßar, Sliema, ≈ªejtun, Fgura, ≈ªebbuƒ°
- CYPRUS (CY): Nicosia, Limassol, Larnaca, Paphos, Famagusta, Kyrenia, Morphou, Aradippou, Paralimni, Geroskipou

OUTPUT FORMAT (strict JSON):
{{
  "api_id": "{api_id}",
  "nip": "{nip or 'none'}",
  "city": "city name or UNKNOWN",
  "region": "region/province name or UNKNOWN",
  "zip_code": "postal code (country-specific format) or null if not found",
  "city_population": "population number if known",
  "match_method": "zip_code_match" | "address_match" | "name_match" | "vat_hint",
  "confidence": "HIGH/MEDIUM/LOW",
  "evidence": "brief explanation of how you identified the location, including which address was used (shop vs headquarters) and which matching method was used"
}}

CONFIDENCE LEVELS:
- HIGH: Multiple clear indicators (product brands + geographic logic + VAT region match)
- MEDIUM: Some indicators with reasonable geographic assumptions
- LOW: Best educated guess based on limited evidence

IMPORTANT: Always attempt to provide city and region even if uncertain. 
Use geographic knowledge about European retail distribution patterns. Prefer real city names over 'UNKNOWN'.

CRITICAL: Return ONLY valid JSON, no markdown formatting, no code blocks, no explanations. Just pure JSON."""

    try:
        response = call_gemini_api_with_retry(prompt)
        
        # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
        try:
            result = clean_and_parse_json(response)
            city_name = result.get('city', '').strip()
            region = result.get('region', '').strip()
            city_population = result.get('city_population')
            confidence = result.get('confidence', 'LOW')
            evidence = result.get('evidence', '')
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º match_method, zip_code, province_code –∏–∑ –æ—Ç–≤–µ—Ç–∞ AI
            match_method = result.get('match_method')
            ai_zip_code = result.get('zip_code', '').strip() if result.get('zip_code') else None
            ai_province_code = result.get('province_code', '').strip() if result.get('province_code') else None
            
            # –ü–æ–ª—É—á–∞–µ–º zip_code –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω (–Ω–µ —Ç–æ–ª—å–∫–æ IT)
            shop_chain = None
            shop_address = None
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º zip_code –∏–∑ –ò–ò, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å, –∏–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –∏–∑ –∞–¥—Ä–µ—Å–∞
            final_zip_code = ai_zip_code if ai_zip_code else zip_code
            
            # –î–ª—è –∏—Ç–∞–ª—å—è–Ω—Å–∫–∏—Ö —á–µ–∫–æ–≤ –ø–æ–ª—É—á–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            if country == "IT":
                shop_chain = result.get('shop_chain', '').strip() or None
                shop_address = result.get('shop_address', '').strip() or None
            
            # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞–Ω —Ç–æ–∂–µ –ø–æ–ª—É—á–∞–µ–º city_population –µ—Å–ª–∏ –µ—Å—Ç—å
            if country != "IT" and not city_population:
                city_population = result.get('city_population')
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º city_population –≤ int –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
            if city_population:
                try:
                    city_population = int(city_population)
                except (ValueError, TypeError):
                    city_population = None
            
            logger.info(f"AI analysis for {api_id}: city={city_name}, region={region}, population={city_population}, confidence={confidence}")
            
        except Exception as e:
            logger.warning(f"Failed to parse JSON response for {api_id}: {e}")
            # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –º–µ—Ç–æ–¥—É
            city_name = response.strip()
            city_name = re.sub(r'[^\wƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈ºƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª\s-]', '', city_name).strip()
            city_name = city_name.split()[0] if city_name.split() else ""
            region = None
            city_population = None
        
        if city_name.upper() == 'UNKNOWN':
            return None, None, None, None, None, None, None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–æ—Ä–æ–¥ —á–µ—Ä–µ–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ BigQuery —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º zip_code –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã dict_cities_all –∏ dict_regions_all
        if country in SUPPORTED_COUNTRIES:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–π –≥–æ—Ä–æ–¥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–µ —Å zip_code
            city_result, region_result, region_code_result = lookup_pl_location(city_name, country, final_zip_code)
            logger.info(f"Lookup result for {country}: city={city_result}, region={region_result}, region_code={region_code_result}, zip_code={final_zip_code}")
            return city_result, region_result, region_code_result, city_population, shop_chain, shop_address, final_zip_code
        else:
            # –î–ª—è –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç AI
            logger.info(f"Using AI result for {country}: city={city_name}, region={region}")
            return city_name, region, None, city_population, shop_chain, shop_address, final_zip_code
        
    except Exception as e:
        logger.warning(f"Failed to analyze receipt for city: {e}")
        return None, None, None, None, None, None, None


def lookup_pl_location(address: str, country: str = "PL", zip_code: Optional[str] = None) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    """
    –ü–æ–ª—É—á–∞–µ–º –≥–æ—Ä–æ–¥ –∏ region –∏—Å–ø–æ–ª—å–∑—É—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã dict_cities_all –∏ dict_regions_all.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ zip_code –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
    """
    # DEBUG: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ country –≤ lookup
    logger.info(f"LOOKUP DEBUG: address={address}, country={country}, zip_code={zip_code}")
    
    if not address:
        return None, None, None

    original_token = re.split(r"[, ]+", address.strip(), 1)[0] or None
    if not original_token:
        return None, None, None

    norm_candidate = normalise_text(original_token)
    country_code = country.upper() if country else "PL"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω
    cities_table = DICT_CITIES_ALL_TABLE
    regions_table = DICT_REGIONS_ALL_TABLE

    # –°—Ç—Ä–æ–∏–º SQL –∑–∞–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º zip_code –¥–ª—è –±–æ–ª—å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    if zip_code:
        # –ï—Å–ª–∏ –µ—Å—Ç—å zip_code, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        sql = f"""
        WITH ranked AS (
          SELECT
            city_name,
            region_code,
            city_norm,
            zip_code,
            CASE 
              WHEN zip_code = @zip_code THEN 1
              WHEN city_norm = @city_norm THEN 2
              ELSE 3
            END AS priority,
            COUNT(*) OVER (
              PARTITION BY country, city_norm, region_code, zip_code
            ) AS rcnt
          FROM `{cities_table}`
          WHERE country = @country_code 
            AND (city_norm = @city_norm OR zip_code = @zip_code)
        )
        SELECT city_name, region_code, zip_code
        FROM ranked
        ORDER BY priority ASC, rcnt DESC, region_code DESC
        LIMIT 1
        """
        
        job_cfg = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("country_code", "STRING", country_code),
                bigquery.ScalarQueryParameter("city_norm", "STRING", norm_candidate),
                bigquery.ScalarQueryParameter("zip_code", "STRING", zip_code),
            ]
        )
    else:
        # –ï—Å–ª–∏ zip_code –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ city_norm
        sql = f"""
        WITH ranked AS (
          SELECT
            city_name,
            region_code,
            city_norm,
            COUNT(*) OVER (
              PARTITION BY country, city_norm, region_code
            ) AS rcnt
          FROM `{cities_table}`
          WHERE country = @country_code AND city_norm = @city_norm
        )
        SELECT city_name, region_code
        FROM ranked
        ORDER BY rcnt DESC, region_code DESC
        LIMIT 1
        """
        
        job_cfg = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("country_code", "STRING", country_code),
                bigquery.ScalarQueryParameter("city_norm", "STRING", norm_candidate),
            ]
        )

    try:
        rows = list(bq_client.query(sql, job_config=job_cfg))
        if not rows:
            logger.info(f"No city found for norm_candidate: {norm_candidate}, zip_code: {zip_code}")
            return original_token, None, None

        city_name = rows[0].city_name or original_token
        region_code = rows[0].region_code
        
        logger.info(f"City lookup: {original_token} -> city_name={city_name}, region_code={region_code}")
    except Exception as e:
        logger.error(f"Error in city lookup query: {e}")
        return original_token, None, None

    region_name = None
    if region_code:
        sql_reg = (
            f"SELECT region_name "
            f"FROM `{regions_table}` "
            f"WHERE country = @country_code AND region_code = @region_code LIMIT 1"
        )
        job_cfg_reg = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("country_code", "STRING", country_code),
                bigquery.ScalarQueryParameter("region_code", "STRING", region_code),
            ]
        )
        try:
            logger.info(f"Executing region query for region_code: {region_code}")
            reg = list(bq_client.query(sql_reg, job_config=job_cfg_reg))
            logger.info(f"Region query returned {len(reg)} rows")
            
            if reg and reg[0].region_name:
                region_name = reg[0].region_name
                logger.info(f"Region found: {region_code} -> {region_name}")
            else:
                logger.warning(f"Region not found for region_code: {region_code}, reg={reg}")
        except Exception as e:
            logger.error(f"Error fetching region for {region_code}: {e}")

    logger.info(f"Final result: city={city_name}, region={region_name}, region_code={region_code}")
    city_display = city_name if (city_name and len(city_name) >= 3) else original_token
    return city_display, region_name, region_code

def save_corrected_products_to_bq(report_id: str,
                                  report_name: str,
                                  corrected_data: Dict[str, Any]) -> None:
    """Save corrected products to BigQuery tables."""
    rows = []
    now = _now().isoformat()
    for p in corrected_data.get("products", []):
        try:
            row = {
                "report_id": report_id,
                "report_name": report_name,
                "api_id": corrected_data.get("api_id", ""),
                "product_name_original": str(p.get("name_original", ""))[:500],
                "product_name_corrected": str(p.get("name_corrected", ""))[:500],
                "quantity": int(p.get("quantity") or 1),
                "price_single_original": float(p.get("price_single_original") or 0.0),
                "price_single_corrected": float(p.get("price_single_corrected") or 0.0),
                "price_total": float(p.get("price_total") or 0.0),
                "name_correction_made": p.get("name_original") != p.get("name_corrected"),
                "price_correction_made": bool(p.get("price_correction_reason")),
                "price_correction_reason": p.get("price_correction_reason"),
                "created_at": now,
            }
            rows.append(row)
        except Exception as e:
            logger.warning("Skipping product due to error: %s", e)
            continue
    if not rows:
        return

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º Storage Write API –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    _storage_write_api_load(PRODUCTS_TABLE, rows, report_id, report_name)
    
    vec_rows = [{
        "report_id": r["report_id"],
        "report_name": r["report_name"],
        "api_id": r["api_id"],
        "clean_product_name": r["product_name_corrected"],
        "quantity": r["quantity"],
        "price_single": r["price_single_corrected"],
        "price_total": r["price_total"],
        "created_at": r["created_at"],
    } for r in rows]
    _storage_write_api_load(VECTOR_READY_TABLE, vec_rows, report_id, report_name)

def save_shop_to_bq(report_id: str,
                    report_name: str,
                    api_id: str,
                    country: str,
                    shopnetwork: Optional[str],
                    shop_name: Optional[str],
                    raw_address: Optional[str],
                    nip: Optional[str],
                    products_json: Optional[List[dict]] = None,
                    city: Optional[str] = None,
                    region: Optional[str] = None,
                    region_code: Optional[str] = None,
                    city_population: Optional[int] = None,
                    confidence: Optional[str] = None,
                    evidence: Optional[str] = None,
                    gamification_id: Optional[str] = None) -> None:
    """Save shop metadata to BigQuery."""
    now = _now().isoformat()
    shop_chain = shopnetwork or shop_name or "UNKNOWN"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–æ—Ä–æ–¥–∞ –∏–ª–∏ fallback –∫ —Å—Ç–∞—Ä–æ–π –ª–æ–≥–∏–∫–µ
    if city is None:
        if country in ["PL", "IT", "GB", "HU", "PT", "RO", "FR"]:  # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω
            # –í—Å–µ–≥–¥–∞ –≤—ã–∑—ã–≤–∞–µ–º AI –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è
            if products_json:
                logger.info(f"Using AI analysis for city determination for {api_id}")
                city, region, region_code, city_population, ai_shop_chain, ai_shop_address, ai_zip_code = analyze_receipt_for_city(
                api_id=api_id,
                products_json=products_json,
                nip=nip,
                shopnetwork=shopnetwork,
                    country=country,
                    raw_address=raw_address  # –ü–µ—Ä–µ–¥–∞–µ–º –∞–¥—Ä–µ—Å –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            )
            if city:
                logger.info(f"AI determined city for {api_id}: {city}, {region}, population={city_population}")
                # –ï—Å–ª–∏ AI –Ω–µ –Ω–∞—à–µ–ª –≥–æ—Ä–æ–¥ - –æ—Å—Ç–∞–≤–ª—è–µ–º UNKNOWN (–Ω–∏–∫–∞–∫–æ–≥–æ fallback)
                if not city or city == "UNKNOWN":
                    logger.warning(f"AI could not determine city for {api_id}, leaving as UNKNOWN")
                    city = "UNKNOWN"
                    region = "UNKNOWN"
                    region_code = None
            
                raw_address = f"{city}, {raw_address or ''}".strip(', ')
    
    shop_city = city if city else "UNKNOWN"
    shop_region = region if region else "UNKNOWN"
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º gamification_id –∏–∑ all_data –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if not gamification_id:
        try:
            gamification_query = f"""
            SELECT gamification_id
            FROM `{PROJECT_ID}.{DATASET}.all_data`
            WHERE api_id = @api_id
            LIMIT 1
            """
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("api_id", "STRING", api_id)
                ]
            )
            gamification_result = list(bq_client.query(gamification_query, job_config=job_config, location=LOCATION))
            if gamification_result:
                gamification_id = gamification_result[0].gamification_id
        except Exception as e:
            logger.warning(f"Could not get gamification_id from all_data for {api_id}: {e}")
    
    row = {
        "report_id": report_id,
        "report_name": report_name,
        "api_id": api_id,
        "nip": nip or "",
        "shop_chain": normalize_network_name_by_country(shop_chain, country),
        "city": shop_city,
        "region": shop_region,
        "region_code": region_code,
        "shop_address": raw_address[:255] if raw_address else None,
        "city_population": city_population,
        "confidence": confidence or "AUTO",
        "evidence": evidence,
        "country": country,
        "gamification_id": gamification_id,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º gamification_id
        "created_at": now,
    }
    _storage_write_api_load(SHOP_TABLE, [row], report_id, report_name)

def save_shop_to_bq_with_city(report_id: str,
                              report_name: str,
                              api_id: str,
                              country: str,
                              shopnetwork: Optional[str],
                              shop_name: Optional[str],
                              raw_address: Optional[str],
                              nip: Optional[str],
                              products_json: Optional[List[dict]] = None,
                              city: Optional[str] = None,
                              region: Optional[str] = None,
                              region_code: Optional[str] = None,
                              city_population: Optional[int] = None,
                              confidence: Optional[str] = None,
                              evidence: Optional[str] = None,
                              gamification_id: Optional[str] = None) -> None:
    """Save shop metadata to BigQuery with city data."""
    now = _now().isoformat()
    shop_chain = shopnetwork or shop_name or "UNKNOWN"
    
    shop_city = city if city else "UNKNOWN"
    shop_region = region if region else "UNKNOWN"
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–ª—É—á–∞–µ–º gamification_id –∏–∑ all_data –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
    if not gamification_id:
        try:
            gamification_query = f"""
            SELECT gamification_id
            FROM `{PROJECT_ID}.{DATASET}.all_data`
            WHERE api_id = @api_id
            LIMIT 1
            """
            job_config = bigquery.QueryJobConfig(
                query_parameters=[
                    bigquery.ScalarQueryParameter("api_id", "STRING", api_id)
                ]
            )
            gamification_result = list(bq_client.query(gamification_query, job_config=job_config, location=LOCATION))
            if gamification_result:
                gamification_id = gamification_result[0].gamification_id
        except Exception as e:
            logger.warning(f"Could not get gamification_id from all_data for {api_id}: {e}")
    
    row = {
        "report_id": report_id,
        "report_name": report_name,
        "api_id": api_id,
        "nip": nip or "",
        "shop_chain": normalize_network_name_by_country(shop_chain, country),
        "city": shop_city,
        "region": shop_region,
        "region_code": region_code,
        "shop_address": raw_address[:255] if raw_address else None,
        "city_population": city_population,
        "confidence": confidence or "AUTO",
        "evidence": evidence,
        "country": country,
        "gamification_id": gamification_id,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º gamification_id
        "created_at": now,
    }
    _storage_write_api_load(SHOP_TABLE, [row], report_id, report_name)

def process_single_receipt(api_id: str,
                           products_json: List[dict],
                           total_price: Optional[float],
                           country: str,
                           report_id: str,
                           report_name: str,
                           shopnetwork: Optional[str] = None,
                           shop_name: Optional[str] = None,
                           raw_address: Optional[str] = None,
                           nip: Optional[str] = None,
                           gamification_id: Optional[str] = None) -> Dict[str, Any]:
    """Process a single receipt."""
    logger.info("Processing single receipt: api_id=%s", api_id)
    if country not in SUPPORTED_COUNTRIES:
        raise ValueError(f"Unsupported country code: {country}")
    
    prompt = create_combined_correction_prompt(api_id, products_json, total_price, country, nip, shopnetwork, raw_address)
    gemini_response = call_gemini_api_with_retry(prompt)
    corrected = clean_and_parse_json(gemini_response)
    
    save_corrected_products_to_bq(report_id, report_name, corrected)
    
    save_shop_to_bq(
        report_id=report_id,
        report_name=report_name,
        api_id=api_id,
        country=country,
        shopnetwork=shopnetwork,
        shop_name=shop_name,
        raw_address=raw_address,
        nip=nip,
        products_json=products_json,
        gamification_id=gamification_id,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º gamification_id
    )
    return {
        "status": "success",
        "api_id": api_id,
        "corrected_products": len(corrected.get("products", [])),
        "shop_saved": True,
    }

def process_single_receipt_by_id(api_id: str, country: str, report_id: str, report_name: str) -> Dict[str, Any]:
    """Process a single receipt by ID - –ø–æ–ª—É—á–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ BigQuery –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö."""
    logger.info(f"üîÑ Processing single receipt by ID: {api_id}")
    
    try:
        # 1. –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ–∫–∞ –∏–∑ BigQuery
        receipt_query = f"""
        SELECT 
            api_id,
            JSON_VALUE(data, '$.products_string') as products,
            JSON_VALUE(data, '$.sum') as total_price,
            JSON_VALUE(data, '$.shop_name') as shopnetwork,
            JSON_VALUE(data, '$.shop_name') as shop_name,
            JSON_VALUE(data, '$.address') as address,
            JSON_VALUE(data, '$.nip') as nip,
            gamification_id
        FROM `{PROJECT_ID}.{DATASET}.gamification_bills_flat`
        WHERE api_id = @api_id
        AND is_success = 1 
        AND is_finished = true
        LIMIT 1
        """
        
        job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("api_id", "STRING", api_id)
            ]
        )
        
        query_job = bq_client.query(receipt_query, job_config=job_config, location=LOCATION)
        results = list(query_job.result())
        
        if not results:
            return {
                "status": "error", 
                "message": f"Receipt with api_id {api_id} not found or not successful"
            }
        
        receipt_data = results[0]
        
        # 2. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω—É –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if not country:
            country = get_country_from_gamification(receipt_data.gamification_id) or "PL"
            logger.info(f"üåç Auto-detected country: {country}")
        
        # 3. –ü–∞—Ä—Å–∏–º JSON –¥–∞–Ω–Ω—ã–µ
        products_json = json.loads(receipt_data.products) if isinstance(receipt_data.products, str) else receipt_data.products
        total_price = float(receipt_data.total_price) if receipt_data.total_price else None
        
        # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ–∫
        result = process_single_receipt(
            api_id=api_id,
            products_json=products_json,
            total_price=total_price,
            country=country,
            report_id=report_id,
            report_name=report_name,
            shopnetwork=receipt_data.shopnetwork,
            shop_name=receipt_data.shop_name,
            raw_address=receipt_data.address,
            nip=receipt_data.nip,
            gamification_id=receipt_data.gamification_id,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º gamification_id –∏–∑ gamification_bills_flat
        )
        
        result["message"] = f"Successfully processed receipt {api_id} from BigQuery"
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error processing receipt by ID {api_id}: {e}", exc_info=True)
        return {
            "status": "error",
            "message": f"Failed to process receipt {api_id}: {str(e)}"
    }

def _extract_text_from_batch_line(obj: dict) -> str:
    """Extract text from Vertex AI batch result."""
    try:
        return obj["predictions"][0]["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        pass
    try:
        return obj["response"]["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        pass
    return ""

def load_receipts_to_fact_scan(country: str, 
                              target_date: Optional[str],
                              report_id: str,
                              report_name: str,
                              since_timestamp: Optional[str] = None,
                              gamification_id: Optional[str] = None,
                              overwrite_mode: bool = False) -> Dict[str, Any]:
    """
    Load ALL receipts from gamification_bills_flat to fact_scan
    
    This includes ALL receipts regardless of is_success status:
    - Successful receipts (is_success = 1, 2, 4)
    
    Args:
        country: Country code (PL, IT, DE, FR)
        target_date: Target date for processing (YYYY-MM-DD)
        report_id: Report ID for tracking
        report_name: Report name for tracking
        since_timestamp: Optional timestamp filter
        gamification_id: Optional specific gamification ID to process
        overwrite_mode: If True, overwrites existing data for the gamification_id
    - Failed receipts (is_success = -5, -4, -3, -2, -1, 0)
    - Temporary receipts (is_success = 3)
    
    Used for complete analytics and error tracking.
    """
    staging_table = None  # Initialize for cleanup in finally block
    try:
        # Date/time filter
        if since_timestamp:
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
            date_filter = f"AND time_added_ts >= TIMESTAMP '{since_timestamp}'"
        elif target_date:
            # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ (–¥–ª—è –¥–Ω–µ–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
            date_filter = f"AND time_added_date = DATE '{target_date}'"
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ
            date_filter = ""
        
        # Gamification ID filter
        gamification_filter = ""
        if gamification_id:
            gamification_filter = f"AND gamification_id = '{gamification_id}'"
            logger.info(f"üéØ Processing specific gamification_id: {gamification_id}")
            
            # –ï—Å–ª–∏ overwrite_mode = True, —É–¥–∞–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ–º–æ
            if overwrite_mode:
                logger.info(f"üóëÔ∏è Overwrite mode enabled - will delete existing data for gamification_id: {gamification_id}")
                _delete_existing_promo_fact_scan_data(gamification_id)
        
        # –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: staging ‚Üí MERGE –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        staging_table = f"{FACT_SCAN_TABLE}_staging_{report_id.replace('-', '_')}"
        logger.info(f"Creating staging table {staging_table} for country {country}")
        
        # 1. –°–æ–∑–¥–∞–µ–º staging —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏
        staging_sql = f"""
        CREATE OR REPLACE TABLE `{staging_table}` (
            scan_id STRING,
            user_id STRING,
            place_id STRING,
            date_id STRING,
            currency_id STRING,
            amount_in_eur FLOAT64,
            amount_in_pln FLOAT64,
            amount_in_usd FLOAT64,
            original_currency_amount FLOAT64,
            promo_currency_amount FLOAT64,
            purchase_price FLOAT64,
            items_count INT64,
            start_scan TIMESTAMP,
            finish_scan TIMESTAMP,
            enter_scan_mode TIMESTAMP,
            scan_duration_seconds INT64,
            shopnetwork STRING,
            error_code STRING,
            error_substatus STRING,
            is_promoted BOOLEAN,
            gain_currency BOOLEAN,
            is_finished BOOL,
            report_id STRING,
            is_success INT64,
            country STRING,
            gamification_id STRING,
            processed_priority INT64,
            ingested_at TIMESTAMP,
            sum FLOAT64,
            products_string STRING,
            total_items_count INT64
        ) AS
        SELECT
            scan_id,
            user_id,
            place_id,
            date_id,
            currency_id,
            amount_in_eur,
            amount_in_pln,
            amount_in_usd,
            original_currency_amount,
            promo_currency_amount,
            purchase_price,
            items_count,
            start_scan,
            finish_scan,
            enter_scan_mode,
            scan_duration_seconds,
            shopnetwork,
            error_code,
            error_substatus,
            is_promoted,
            gain_currency,
            is_finished,
            report_id,
            is_success,
            country,
            gamification_id,
            processed_priority,
            ingested_at,
            sum,
            products_string,
            total_items_count
        FROM (
        SELECT
            b.api_id AS scan_id,
            b.user_id,
            CAST(NULL AS STRING) AS place_id,
            FORMAT_DATE('%Y%m%d', b.time_added_date) AS date_id,
            CASE 
                WHEN b.country = 'PL' THEN 'PLN'
                WHEN b.country = 'GB' THEN 'GBP'
                WHEN b.country = 'RO' THEN 'RON'
                WHEN b.country = 'HU' THEN 'HUF'
                WHEN b.country = 'RS' THEN 'RSD'
                ELSE 'EUR'
            END AS currency_id,
            
            -- Currency amounts
            -- –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ EUR –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫—É—Ä—Å–∞–º –≤–∞–ª—é—Ç
            CASE
                WHEN b.country = 'PL' THEN 
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 4.5  -- PLN to EUR: ~4.5 PLN = 1 EUR
                WHEN b.country = 'GB' THEN
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 0.85  -- GBP to EUR: ~0.85 GBP = 1 EUR
                WHEN b.country = 'RO' THEN
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 5.0  -- RON to EUR: ~5.0 RON = 1 EUR
                WHEN b.country = 'HU' THEN
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 400.0  -- HUF to EUR: ~400 HUF = 1 EUR
                WHEN b.country = 'RS' THEN
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 117.0  -- RSD to EUR: ~117 RSD = 1 EUR
                ELSE SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64)  -- –£–∂–µ –≤ EUR
            END AS amount_in_eur,
            CASE WHEN b.country = 'PL' THEN SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) ELSE NULL END AS amount_in_pln,
            NULL AS amount_in_usd,
            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS original_currency_amount,
            SAFE_CAST(JSON_VALUE(b.data, '$.points') AS FLOAT64) AS promo_currency_amount,
            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS purchase_price,
            
            -- Items count from products_string array
            -- products_string —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–∞–∫ JSON —Å—Ç—Ä–æ–∫–∞, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ—á—å –∫–∞–∫ VALUE, –ø–æ—Ç–æ–º –ø–∞—Ä—Å–∏—Ç—å
            CASE
                WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                THEN (
                    SELECT COUNT(*) 
                    FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                )
                ELSE NULL
            END AS items_count,
            
            -- Timestamps
            b.time_added_ts AS start_scan,
            CASE
                WHEN b.is_finished = TRUE
                THEN SAFE.PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S%Ez', JSON_VALUE(b.data, '$.time_processed'))
                ELSE NULL
            END AS finish_scan,
            b.time_added_ts AS enter_scan_mode,
            
            -- Duration calculation
            CASE
                WHEN b.is_finished = TRUE AND JSON_VALUE(b.data, '$.time_processed') IS NOT NULL
                THEN DATETIME_DIFF(
                    SAFE.PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S%Ez', JSON_VALUE(b.data, '$.time_processed')),
                    b.time_added_ts,
                    SECOND
                )
                ELSE NULL
            END AS scan_duration_seconds,
            
            -- Shop and status info
            JSON_VALUE(b.data, '$.shop_name') AS shopnetwork,
            b.status AS error_code,
            b.substatus AS error_substatus,
            
            -- Derived fields
            CASE WHEN b.is_success = 1 THEN TRUE ELSE FALSE END AS is_promoted,
            CASE WHEN SAFE_CAST(JSON_VALUE(b.data, '$.points') AS INT64) > 0 THEN TRUE ELSE FALSE END AS gain_currency,
            b.is_finished,
            
                    -- Report info
                    @report_id AS report_id,
                CAST(COALESCE(b.is_success, 0) AS INT64) AS is_success,
                b.country AS country,  -- –ë–µ—Ä–µ–º –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –Ω–µ –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞
                COALESCE(b.gamification_id, '') AS gamification_id,
                
                -- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (1,2,4) –≤–∞–∂–Ω–µ–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö (0,3)
                CAST(1 AS INT64) AS processed_priority,
                CURRENT_TIMESTAMP() AS ingested_at,
                
                -- –ù–æ–≤—ã–µ –ø–æ–ª—è: sum –∏ products_string (STRING –¥–ª—è Looker Studio)
                SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS sum,
                -- products_string: —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫)
                CASE
                    WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                    THEN (
                        SELECT TO_JSON_STRING(ARRAY_AGG(JSON_VALUE(product, '$.name')))
                        FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                        WHERE JSON_VALUE(product, '$.name') IS NOT NULL
                    )
                    ELSE NULL
                END AS products_string,
                
                -- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ (total items count)
                -- –£–ú–ù–ê–Ø –õ–û–ì–ò–ö–ê: number –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –µ–¥–∏–Ω–∏—Ü –ò–õ–ò –≤–µ—Å–æ–º –≤ –≥—Ä–∞–º–º–∞—Ö
                -- –ï—Å–ª–∏ number <= 20 –ò price_total / number >= 0.5 ‚Üí number = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü ‚Üí —Å—É–º–º–∏—Ä—É–µ–º
                -- –ò–Ω–∞—á–µ ‚Üí number = –≤–µ—Å –≤ –≥—Ä–∞–º–º–∞—Ö ‚Üí —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ 1 –µ–¥–∏–Ω–∏—Ü–∞
                CASE
                    WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                    THEN (
                        SELECT SUM(
                            CASE
                                WHEN SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64) <= 20
                                 AND SAFE_CAST(JSON_VALUE(product, '$.price_total') AS FLOAT64) / 
                                     NULLIF(SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64), 0) >= 0.5
                                THEN SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64)
                                ELSE 1
                            END
                        )
                        FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                        WHERE JSON_VALUE(product, '$.number') IS NOT NULL
                    )
                    ELSE (
                        -- Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                        CASE
                            WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                            THEN (
                                SELECT COUNT(*) 
                                FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                            )
                            ELSE NULL
                        END
                    )
                END AS total_items_count,
                
                -- –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ü–û API_ID: –±–µ—Ä–µ–º —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ api_id
                ROW_NUMBER() OVER (
                    PARTITION BY b.api_id, b.country, b.gamification_id
                    ORDER BY b.time_added_ts DESC, b.read_ts DESC
                ) as rn
            
        FROM `{GAMIFICATION_BILLS_FLAT}` b
        -- –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç (fallback, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—É–±–ª–∏—á–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ)
        WHERE b.time_added_date IS NOT NULL
          {date_filter}
              {gamification_filter}
              -- –ù–ï–¢ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ country - –∑–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Å—Ç—Ä–∞–Ω—ã!
        )
        WHERE rn = 1  -- –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è (—Å–∞–º–∞—è –ø–æ—Å–ª–µ–¥–Ω—è—è) –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ api_id
        """
        
        staging_job_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("report_id", "STRING", report_id)
                # –ù–µ –ø–µ—Ä–µ–¥–∞–µ–º country - –∑–∞–≥—Ä—É–∂–∞–µ–º –í–°–ï —Å—Ç—Ä–∞–Ω—ã
            ]
        )
        
        staging_job = bq_client.query(staging_sql, job_config=staging_job_config, location=LOCATION)
        staging_result = staging_job.result()
        
        logger.info(f"Created staging table {staging_table} with {staging_job.num_dml_affected_rows or 0} rows")
        
        # 2. MERGE –∏–∑ staging –≤ —Ü–µ–ª–µ–≤—É—é —Ç–∞–±–ª–∏—Ü—É (—Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏)
        merge_sql = f"""
        MERGE `{FACT_SCAN_TABLE}` t
        USING `{staging_table}` s
        ON t.scan_id = s.scan_id AND t.country = s.country AND t.gamification_id = s.gamification_id
        
        WHEN MATCHED AND CAST(s.processed_priority AS INT64) > t.processed_priority THEN
            UPDATE SET 
                user_id = s.user_id,
                place_id = s.place_id,
                date_id = s.date_id,
                currency_id = s.currency_id,
                amount_in_eur = s.amount_in_eur,
                amount_in_pln = s.amount_in_pln,
                amount_in_usd = s.amount_in_usd,
                original_currency_amount = s.original_currency_amount,
                promo_currency_amount = s.promo_currency_amount,
                purchase_price = s.purchase_price,
                items_count = s.items_count,
                start_scan = s.start_scan,
                finish_scan = s.finish_scan,
                enter_scan_mode = s.enter_scan_mode,
                scan_duration_seconds = s.scan_duration_seconds,
                shopnetwork = s.shopnetwork,
                error_code = s.error_code,
                error_substatus = s.error_substatus,
                is_promoted = s.is_promoted,
                gain_currency = s.gain_currency,
                is_finished = s.is_finished,
                report_id = s.report_id,
                is_success = CAST(s.is_success AS INT64),
                country = s.country,
                gamification_id = s.gamification_id,
                processed_priority = s.processed_priority,
                ingested_at = s.ingested_at,
                sum = s.sum,
                products_string = s.products_string,
                total_items_count = s.total_items_count
                
        WHEN MATCHED AND CAST(s.processed_priority AS INT64) = t.processed_priority AND s.ingested_at > t.ingested_at THEN
            UPDATE SET 
                user_id = s.user_id,
                place_id = s.place_id,
                date_id = s.date_id,
                currency_id = s.currency_id,
                amount_in_eur = s.amount_in_eur,
                amount_in_pln = s.amount_in_pln,
                amount_in_usd = s.amount_in_usd,
                original_currency_amount = s.original_currency_amount,
                promo_currency_amount = s.promo_currency_amount,
                purchase_price = s.purchase_price,
                items_count = s.items_count,
                start_scan = s.start_scan,
                finish_scan = s.finish_scan,
                enter_scan_mode = s.enter_scan_mode,
                scan_duration_seconds = s.scan_duration_seconds,
                shopnetwork = s.shopnetwork,
                error_code = s.error_code,
                error_substatus = s.error_substatus,
                is_promoted = s.is_promoted,
                gain_currency = s.gain_currency,
                is_finished = s.is_finished,
                report_id = s.report_id,
                is_success = CAST(s.is_success AS INT64),
                country = s.country,
                gamification_id = s.gamification_id,
                processed_priority = s.processed_priority,
                ingested_at = s.ingested_at,
                sum = s.sum,
                products_string = s.products_string,
                total_items_count = s.total_items_count
                
        WHEN NOT MATCHED BY TARGET THEN
            INSERT (
                scan_id, user_id, place_id, date_id, currency_id, amount_in_eur,
                amount_in_pln, amount_in_usd, original_currency_amount,
                promo_currency_amount, purchase_price, items_count, start_scan,
                finish_scan, enter_scan_mode, scan_duration_seconds, shopnetwork,
                error_code, error_substatus, is_promoted, gain_currency, is_finished, report_id,
                is_success, country, gamification_id, processed_priority, ingested_at,
                sum, products_string, total_items_count
            )
            VALUES (
                s.scan_id, s.user_id, s.place_id, s.date_id, s.currency_id, s.amount_in_eur,
                s.amount_in_pln, s.amount_in_usd, s.original_currency_amount,
                s.promo_currency_amount, s.purchase_price, s.items_count, s.start_scan,
                s.finish_scan, s.enter_scan_mode, s.scan_duration_seconds, s.shopnetwork,
                s.error_code, s.error_substatus, s.is_promoted, s.gain_currency, s.is_finished, s.report_id,
                s.is_success, s.country, s.gamification_id, s.processed_priority, s.ingested_at,
                s.sum, s.products_string, s.total_items_count
            )
        """
        
        merge_job = bq_client.query(merge_sql, location=LOCATION)
        merge_result = merge_job.result()
        
        rows_merged = merge_job.num_dml_affected_rows or 0
        
        logger.info(f"Merged {rows_merged} rows into fact_scan for {country} (deduplicated by api_id)")
        
        return {
            "status": "success",
            "table": "fact_scan",
            "rows_inserted": rows_merged,
            "country": country,
            "target_date": target_date
        }
        
    except Exception as e:
        logger.exception(f"Error loading receipts to fact_scan for {country}")
        return {
            "status": "error",
            "table": "fact_scan",
            "message": str(e),
            "country": country,
            "rows_inserted": 0
        }

    finally:
        # –í—Å–µ–≥–¥–∞ —É–¥–∞–ª—è–µ–º staging —Ç–∞–±–ª–∏—Ü—É, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
        if staging_table:
            try:
                cleanup_sql = f"DROP TABLE IF EXISTS `{staging_table}`"
                bq_client.query(cleanup_sql, location=LOCATION).result()
                logger.info(f"Cleaned up staging table {staging_table}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup staging table {staging_table}: {cleanup_error}")

def load_all_promos_to_fact_scan(start_date: str,
                                end_date: str,
                                       report_id: str,
                                report_name: str,
                                overwrite_mode: bool = False) -> Dict[str, Any]:
    """
    Load ALL promotions data from gamification_bills_flat to fact_scan for date range
    
    This processes ALL countries and ALL gamification_ids in the specified date range.
    
    Args:
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        report_id: Report ID for tracking
        report_name: Report name for tracking
        overwrite_mode: If True, deletes existing data before loading
    """
    try:
        # Date filter
        date_filter = f"AND time_added_date >= DATE '{start_date}' AND time_added_date <= DATE '{end_date}'"
        
        logger.info(f"üåç Loading ALL promotions data from {start_date} to {end_date}")
        
        if overwrite_mode:
            logger.info(f"üóëÔ∏è Overwrite mode - deleting existing data for date range")
            delete_sql = f"""
            DELETE FROM `{FACT_SCAN_TABLE}`
            WHERE time_added_date >= DATE '{start_date}' 
              AND time_added_date <= DATE '{end_date}'
            """
            delete_job = bq_client.query(delete_sql, location=LOCATION)
            delete_job.result()
            deleted_rows = delete_job.num_dml_affected_rows or 0
            logger.info(f"üóëÔ∏è Deleted {deleted_rows} existing rows for date range")
        
        # Get unique countries from the data
        countries_query = f"""
        SELECT DISTINCT country
        FROM `{GAMIFICATION_BILLS_FLAT}`
        WHERE time_added_date >= DATE '{start_date}' 
          AND time_added_date <= DATE '{end_date}'
          AND country IS NOT NULL
        ORDER BY country
        """
        
        countries_result = bq_client.query(countries_query, location=LOCATION)
        countries = [row.country for row in countries_result]
        
        logger.info(f"üìä Found {len(countries)} countries: {countries}")
        
        total_rows = 0
        results_by_country = {}
        
        # Process each country separately
        for country in countries:
            logger.info(f"üîÑ Processing country: {country}")
            
            # –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: staging ‚Üí MERGE –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω—ã
            staging_table = f"{FACT_SCAN_TABLE}_staging_{report_id.replace('-', '_')}_{country}"
            
            # 1. –°–æ–∑–¥–∞–µ–º staging —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω—ã
            staging_sql = f"""
            CREATE OR REPLACE TABLE `{staging_table}` (
                scan_id STRING,
                user_id STRING,
                place_id STRING,
                date_id STRING,
                currency_id STRING,
                amount_in_eur FLOAT64,
                amount_in_pln FLOAT64,
                amount_in_usd FLOAT64,
                original_currency_amount FLOAT64,
                promo_currency_amount FLOAT64,
                purchase_price FLOAT64,
                items_count INT64,
                start_scan TIMESTAMP,
                finish_scan TIMESTAMP,
                enter_scan_mode TIMESTAMP,
                scan_duration_seconds INT64,
                shopnetwork STRING,
                error_code STRING,
                error_substatus STRING,
                is_promoted BOOLEAN,
                gain_currency BOOLEAN,
                is_finished BOOL,
                report_id STRING,
                is_success INT64,
                country STRING,
                gamification_id STRING,
                processed_priority INT64,
                ingested_at TIMESTAMP,
                sum FLOAT64,
                products_string STRING,
                total_items_count INT64
            ) AS
            SELECT
                scan_id,
                user_id,
                place_id,
                date_id,
                currency_id,
                amount_in_eur,
                amount_in_pln,
                amount_in_usd,
                original_currency_amount,
                promo_currency_amount,
                purchase_price,
                items_count,
                start_scan,
                finish_scan,
                enter_scan_mode,
                scan_duration_seconds,
                shopnetwork,
                error_code,
                error_substatus,
                is_promoted,
                gain_currency,
                is_finished,
                report_id,
                is_success,
                country,
                gamification_id,
                processed_priority,
                ingested_at,
                sum,
                products_string,
                total_items_count
            FROM (
                SELECT
                    b.api_id AS scan_id,
                    b.user_id,
                    CAST(NULL AS STRING) AS place_id,
                    FORMAT_DATE('%Y%m%d', b.time_added_date) AS date_id,
                    CASE 
                        WHEN b.country = 'PL' THEN 'PLN'
                        WHEN b.country = 'GB' THEN 'GBP'
                        WHEN b.country = 'RO' THEN 'RON'
                        WHEN b.country = 'HU' THEN 'HUF'
                        WHEN b.country = 'RS' THEN 'RSD'
                        ELSE 'EUR'
                    END AS currency_id,
                    
                    -- Currency amounts
                    -- –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ EUR –ø–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∫—É—Ä—Å–∞–º –≤–∞–ª—é—Ç
                    CASE
                        WHEN b.country = 'PL' THEN 
                            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 4.5  -- PLN to EUR: ~4.5 PLN = 1 EUR
                        WHEN b.country = 'GB' THEN
                            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 0.85  -- GBP to EUR: ~0.85 GBP = 1 EUR
                        WHEN b.country = 'RO' THEN
                            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 5.0  -- RON to EUR: ~5.0 RON = 1 EUR
                        WHEN b.country = 'HU' THEN
                            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 400.0  -- HUF to EUR: ~400 HUF = 1 EUR
                        WHEN b.country = 'RS' THEN
                            SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) / 117.0  -- RSD to EUR: ~117 RSD = 1 EUR
                        ELSE SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64)  -- –£–∂–µ –≤ EUR
                    END AS amount_in_eur,
                    CASE WHEN b.country = 'PL' THEN SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) ELSE NULL END AS amount_in_pln,
                    NULL AS amount_in_usd,
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS original_currency_amount,
                    SAFE_CAST(JSON_VALUE(b.data, '$.points') AS FLOAT64) AS promo_currency_amount,
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS purchase_price,
                    
                    -- Items count from products_string array
                    -- products_string —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–∞–∫ JSON —Å—Ç—Ä–æ–∫–∞, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ—á—å –∫–∞–∫ VALUE, –ø–æ—Ç–æ–º –ø–∞—Ä—Å–∏—Ç—å
                    CASE
                        WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                        THEN (
                            SELECT COUNT(*) 
                            FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                        )
                        ELSE NULL
                    END AS items_count,
                    
                    -- Timestamps
                    b.time_added_ts AS start_scan,
                    CASE
                        WHEN b.is_finished = TRUE
                        THEN SAFE.PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S%Ez', JSON_VALUE(b.data, '$.time_processed'))
                        ELSE NULL
                    END AS finish_scan,
                    b.time_added_ts AS enter_scan_mode,
                    
                    -- Duration calculation
                    CASE
                        WHEN b.is_finished = TRUE AND JSON_VALUE(b.data, '$.time_processed') IS NOT NULL
                        THEN DATETIME_DIFF(
                            SAFE.PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S%Ez', JSON_VALUE(b.data, '$.time_processed')),
                            b.time_added_ts,
                            SECOND
                        )
                        ELSE NULL
                    END AS scan_duration_seconds,
                    
                    -- Shop and status info
                    JSON_VALUE(b.data, '$.shop_name') AS shopnetwork,
                    b.status AS error_code,
                    b.substatus AS error_substatus,
                    
                    -- Derived fields
                    CASE WHEN b.is_success = 1 THEN TRUE ELSE FALSE END AS is_promoted,
                    CASE WHEN SAFE_CAST(JSON_VALUE(b.data, '$.points') AS INT64) > 0 THEN TRUE ELSE FALSE END AS gain_currency,
                    b.is_finished,
                    
                    -- Report info
                    '{report_id}' AS report_id,
                    CAST(COALESCE(b.is_success, 0) AS INT64) AS is_success,
                    b.country,
                    COALESCE(b.gamification_id, '') AS gamification_id,
                    
                    -- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (1,2,4) –≤–∞–∂–Ω–µ–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö (0,3)
                    CAST(1 AS INT64) AS processed_priority,
                    CURRENT_TIMESTAMP() AS ingested_at,
                    
                    -- –ù–æ–≤—ã–µ –ø–æ–ª—è: sum –∏ products_string (STRING –¥–ª—è Looker Studio)
                    SAFE_CAST(JSON_VALUE(b.data, '$.sum') AS FLOAT64) AS sum,
                    -- products_string: —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫)
                    CASE
                        WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                        THEN (
                            SELECT TO_JSON_STRING(ARRAY_AGG(JSON_VALUE(product, '$.name')))
                            FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                            WHERE JSON_VALUE(product, '$.name') IS NOT NULL
                        )
                        ELSE NULL
                    END AS products_string,
                    
                    -- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ (total items count)
                    -- –£–ú–ù–ê–Ø –õ–û–ì–ò–ö–ê: number –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –µ–¥–∏–Ω–∏—Ü –ò–õ–ò –≤–µ—Å–æ–º –≤ –≥—Ä–∞–º–º–∞—Ö
                    -- –ï—Å–ª–∏ number <= 20 –ò price_total / number >= 0.5 ‚Üí number = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ–¥–∏–Ω–∏—Ü ‚Üí —Å—É–º–º–∏—Ä—É–µ–º
                    -- –ò–Ω–∞—á–µ ‚Üí number = –≤–µ—Å –≤ –≥—Ä–∞–º–º–∞—Ö ‚Üí —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ 1 –µ–¥–∏–Ω–∏—Ü–∞
                    CASE
                        WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                        THEN (
                            SELECT SUM(
                                CASE
                                    WHEN SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64) <= 20
                                     AND SAFE_CAST(JSON_VALUE(product, '$.price_total') AS FLOAT64) / 
                                         NULLIF(SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64), 0) >= 0.5
                                    THEN SAFE_CAST(JSON_VALUE(product, '$.number') AS INT64)
                                    ELSE 1
                                END
                            )
                            FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                            WHERE JSON_VALUE(product, '$.number') IS NOT NULL
                        )
                        ELSE (
                            -- Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
                            CASE
                                WHEN JSON_VALUE(b.data, '$.products_string') IS NOT NULL
                                THEN (
                                    SELECT COUNT(*) 
                                    FROM UNNEST(JSON_QUERY_ARRAY(JSON_VALUE(b.data, '$.products_string'))) AS product
                                )
                                ELSE NULL
                            END
                        )
                    END AS total_items_count,
                    
                    -- –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–Ø –ü–û API_ID: –±–µ—Ä–µ–º —Å–∞–º—É—é –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ api_id
                    ROW_NUMBER() OVER (
                        PARTITION BY b.api_id, b.country, b.gamification_id
                        ORDER BY b.time_added_ts DESC, b.read_ts DESC
                    ) as rn
                    
                FROM `{GAMIFICATION_BILLS_FLAT}` b
                -- –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫—É—Ä—Å—ã –≤–∞–ª—é—Ç (fallback, —Ç–∞–∫ –∫–∞–∫ –Ω–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø—É–±–ª–∏—á–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ)
                WHERE b.time_added_date IS NOT NULL
                  AND b.country = '{country}'
                  {date_filter}
            )
            WHERE rn = 1  -- –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤–∞—è (—Å–∞–º–∞—è –ø–æ—Å–ª–µ–¥–Ω—è—è) –∑–∞–ø–∏—Å—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ api_id
            """
            
            staging_job = bq_client.query(staging_sql, location=LOCATION)
            staging_result = staging_job.result()
            
            country_rows = staging_job.num_dml_affected_rows or 0
            logger.info(f"Created staging table {staging_table} with {country_rows} rows for {country}")
            
            # 2. MERGE –∏–∑ staging –≤ —Ü–µ–ª–µ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
            merge_sql = f"""
            MERGE `{FACT_SCAN_TABLE}` t
            USING `{staging_table}` s
            ON t.scan_id = s.scan_id AND t.country = s.country AND t.gamification_id = s.gamification_id
            
            WHEN MATCHED AND CAST(s.processed_priority AS INT64) > t.processed_priority THEN
                UPDATE SET 
                    user_id = s.user_id,
                    place_id = s.place_id,
                    date_id = s.date_id,
                    currency_id = s.currency_id,
                    amount_in_eur = s.amount_in_eur,
                    amount_in_pln = s.amount_in_pln,
                    amount_in_usd = s.amount_in_usd,
                    original_currency_amount = s.original_currency_amount,
                    promo_currency_amount = s.promo_currency_amount,
                    purchase_price = s.purchase_price,
                    items_count = s.items_count,
                    start_scan = s.start_scan,
                    finish_scan = s.finish_scan,
                    enter_scan_mode = s.enter_scan_mode,
                    scan_duration_seconds = s.scan_duration_seconds,
                    shopnetwork = s.shopnetwork,
                    error_code = s.error_code,
                    error_substatus = s.error_substatus,
                    is_promoted = s.is_promoted,
                    gain_currency = s.gain_currency,
                    report_id = s.report_id,
                    is_success = CAST(s.is_success AS INT64),
                    country = s.country,
                    gamification_id = s.gamification_id,
                    processed_priority = s.processed_priority,
                    ingested_at = s.ingested_at,
                    sum = s.sum,
                    products_string = s.products_string,
                    total_items_count = s.total_items_count
                    
            WHEN MATCHED AND CAST(s.processed_priority AS INT64) = t.processed_priority AND s.ingested_at > t.ingested_at THEN
                UPDATE SET 
                    user_id = s.user_id,
                    place_id = s.place_id,
                    date_id = s.date_id,
                    currency_id = s.currency_id,
                    amount_in_eur = s.amount_in_eur,
                    amount_in_pln = s.amount_in_pln,
                    amount_in_usd = s.amount_in_usd,
                    original_currency_amount = s.original_currency_amount,
                    promo_currency_amount = s.promo_currency_amount,
                    purchase_price = s.purchase_price,
                    items_count = s.items_count,
                    start_scan = s.start_scan,
                    finish_scan = s.finish_scan,
                    enter_scan_mode = s.enter_scan_mode,
                    scan_duration_seconds = s.scan_duration_seconds,
                    shopnetwork = s.shopnetwork,
                    error_code = s.error_code,
                    error_substatus = s.error_substatus,
                    is_promoted = s.is_promoted,
                    gain_currency = s.gain_currency,
                    report_id = s.report_id,
                    is_success = CAST(s.is_success AS INT64),
                    country = s.country,
                    gamification_id = s.gamification_id,
                    processed_priority = s.processed_priority,
                    ingested_at = s.ingested_at,
                    sum = s.sum,
                    products_string = s.products_string,
                    total_items_count = s.total_items_count
                    
            WHEN NOT MATCHED BY TARGET THEN
                INSERT (
                    scan_id, user_id, place_id, date_id, currency_id, amount_in_eur,
                    amount_in_pln, amount_in_usd, original_currency_amount,
                    promo_currency_amount, purchase_price, items_count, start_scan,
                    finish_scan, enter_scan_mode, scan_duration_seconds, shopnetwork,
                    error_code, error_substatus, is_promoted, gain_currency, is_finished, report_id,
                    is_success, country, gamification_id, processed_priority, ingested_at,
                    sum, products_string, total_items_count
                )
                VALUES (
                    s.scan_id, s.user_id, s.place_id, s.date_id, s.currency_id, s.amount_in_eur,
                    s.amount_in_pln, s.amount_in_usd, s.original_currency_amount,
                    s.promo_currency_amount, s.purchase_price, s.items_count, s.start_scan,
                    s.finish_scan, s.enter_scan_mode, s.scan_duration_seconds, s.shopnetwork,
                    s.error_code, s.error_substatus, s.is_promoted, s.gain_currency, s.is_finished, s.report_id,
                    s.is_success, s.country, s.gamification_id, s.processed_priority, s.ingested_at,
                    s.sum, s.products_string, s.total_items_count
                )
            """
            
            merge_job = bq_client.query(merge_sql, location=LOCATION)
            merge_result = merge_job.result()
            
            country_merged = merge_job.num_dml_affected_rows or 0
            logger.info(f"Merged {country_merged} rows into fact_scan for {country}")
            
            total_rows += country_merged
            results_by_country[country] = {
                "staging_rows": country_rows,
                "merged_rows": country_merged
            }
            
            # Cleanup staging table
            try:
                cleanup_sql = f"DROP TABLE IF EXISTS `{staging_table}`"
                bq_client.query(cleanup_sql, location=LOCATION).result()
                logger.info(f"Cleaned up staging table {staging_table}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup staging table {staging_table}: {cleanup_error}")
        
        logger.info(f"‚úÖ Completed loading ALL promotions data: {total_rows} total rows across {len(countries)} countries")
        
        return {
            "status": "success",
            "table": "fact_scan",
            "total_rows_inserted": total_rows,
            "countries_processed": len(countries),
            "countries": countries,
            "results_by_country": results_by_country,
            "date_range": f"{start_date} to {end_date}"
        }
        
    except Exception as e:
        logger.exception(f"Error loading all promotions to fact_scan")
        return {
            "status": "error",
            "table": "fact_scan",
            "message": str(e),
            "total_rows_inserted": 0
        }

def load_successful_receipts_to_all_data(country: str, 
                                       target_date: Optional[str],
                                       report_id: str,
                                       report_name: str,
                                       since_timestamp: Optional[str] = None) -> Dict[str, Any]:
    """
    Load only SUCCESSFUL receipts from gamification_bills_flat to all_data
    
    Only includes receipts with is_success IN (2, 4):
    - 2: accepted by moderator  
    - 4: synchronized with CCA and points given
    
    Additional filters:
    - is_finished = true
    - points > 0
    - country IN SUPPORTED_COUNTRIES  -- –í–°–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞–Ω—ã (24 —Å—Ç—Ä–∞–Ω—ã)
    
    Excludes:
    - Automation accepted (is_success = 1) - requires moderator approval
    - Failed receipts (is_success = -5, -4, -3, -2, -1, 0)
    - Temporary receipts (is_success = 3)
    
    These receipts will be processed further (AI correction, product matching, aggregates).
    """
    staging_table = None  # Initialize for cleanup in finally block
    try:
        # Date/time filter - –ó–ê–©–ò–¢–ê –û–¢ –°–õ–£–ß–ê–ô–ù–û–ô –ó–ê–ì–†–£–ó–ö–ò –í–°–ï–• –î–ê–ù–ù–´–•
        if since_timestamp:
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
            date_filter = f"AND time_added_ts >= TIMESTAMP '{since_timestamp}'"
        elif target_date:
            # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ (–¥–ª—è –¥–Ω–µ–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
            date_filter = f"AND time_added_date = DATE '{target_date}'"
        else:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–©–ò–¢–ê: –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞
            logger.error("–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –í–°–ï –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ/–≤—Ä–µ–º–µ–Ω–∏!")
            return {
                "status": "error",
                "table": "all_data",
                "message": "–ù–µ–ª—å–∑—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–∏. –£–∫–∞–∂–∏—Ç–µ target_date –∏–ª–∏ since_timestamp.",
                "country": country,
                "rows_inserted": 0
            }
        
        # –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê: staging ‚Üí MERGE –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        staging_table = f"{ALL_DATA_TABLE}_staging_{report_id.replace('-', '_')}"
        logger.info(f"Creating staging table {staging_table} for all supported countries (not just {country})")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω –¥–ª—è SQL
        countries_list = ', '.join([f"'{c}'" for c in SUPPORTED_COUNTRIES])
        logger.info(f"Loading data for all supported countries: {', '.join(SUPPORTED_COUNTRIES)}")
        
        # 1. –°–æ–∑–¥–∞–µ–º staging —Ç–∞–±–ª–∏—Ü—É —Å –¥–∞–Ω–Ω—ã–º–∏
        staging_sql = f"""
        CREATE OR REPLACE TABLE `{staging_table}` (
            api_id STRING,
            user_id STRING,
            event_ts TIMESTAMP,
            event_date DATE,
            source_collection STRING,
            gamification_id STRING,
            bill_image STRING,
            status STRING,
            substatus STRING,
            raw_doc JSON,
            country STRING,
            shopnetwork STRING,
            shop_name STRING,
            processed_priority INT64,
            ingested_at TIMESTAMP
        ) AS
        SELECT
            api_id,
            user_id,
            time_added_ts AS event_ts,
            time_added_date AS event_date,
            'gamification_bills' AS source_collection,
            gamification_id,
            bill_image,
            status,
            substatus,
            data AS raw_doc,
            s.country AS country,  -- –ò—Å–ø–æ–ª—å–∑—É–µ–º country –∏–∑ —Ç–∞–±–ª–∏—Ü—ã, –∞ –Ω–µ –ø–∞—Ä–∞–º–µ—Ç—Ä
            JSON_VALUE(data, '$.shop_name') AS shopnetwork,
            JSON_VALUE(data, '$.shop_name') AS shop_name,
            
            -- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ (1,2,4) –≤–∞–∂–Ω–µ–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö (0,3)
            CAST(1 AS INT64) AS processed_priority,
            CURRENT_TIMESTAMP() AS ingested_at
            
        FROM `{GAMIFICATION_BILLS_FLAT}` s
        WHERE s.time_added_date IS NOT NULL
          AND s.is_success IN (2, 4)  -- –¢–û–õ–¨–ö–û: 2=moderator, 4=synchronized (–ë–ï–ó 1=automation!)
          AND s.is_finished = true  -- Only finished receipts
          AND CAST(JSON_VALUE(s.data, '$.points') AS INT64) > 0  -- –ï—Å—Ç—å –ø–æ–∏–Ω—Ç—ã (gained currency)
          AND s.country IN ({countries_list})  -- –í–°–ï –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞–Ω—ã (24 —Å—Ç—Ä–∞–Ω—ã)
          {date_filter}
          -- –£–±—Ä–∞–ª–∏ —Ñ–∏–ª—å—Ç—Ä –ø–æ products_string - –±–µ—Ä–µ–º –í–°–ï —á–µ–∫–∏ —Å points > 0
        """
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è staging (–ë–ï–ó country - —Ñ–∏–ª—å—Ç—Ä—É–µ–º –≤ WHERE)
        staging_params = []
        
        if since_timestamp:
            staging_params.append(bigquery.ScalarQueryParameter("since_timestamp", "TIMESTAMP", since_timestamp))
        elif target_date:
            staging_params.append(bigquery.ScalarQueryParameter("target_date", "DATE", target_date))
        
        staging_job_config = bigquery.QueryJobConfig(query_parameters=staging_params)
        
        staging_job = bq_client.query(staging_sql, job_config=staging_job_config, location=LOCATION)
        staging_result = staging_job.result()
        
        logger.info(f"Created staging table {staging_table} with {staging_job.num_dml_affected_rows or 0} rows")
        
        # 2. MERGE –∏–∑ staging –≤ —Ü–µ–ª–µ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
        merge_sql = f"""
        MERGE `{ALL_DATA_TABLE}` t
        USING `{staging_table}` s
        ON t.api_id = s.api_id AND t.country = s.country
        
        WHEN MATCHED AND CAST(s.processed_priority AS INT64) > t.processed_priority THEN
            UPDATE SET 
                user_id = s.user_id,
                event_ts = s.event_ts,
                event_date = s.event_date,
                source_collection = s.source_collection,
                gamification_id = s.gamification_id,
                bill_image = s.bill_image,
                status = s.status,
                substatus = s.substatus,
                raw_doc = s.raw_doc,
                shopnetwork = s.shopnetwork,
                shop_name = s.shop_name,
                processed_priority = s.processed_priority,
                ingested_at = s.ingested_at
                
        WHEN MATCHED AND CAST(s.processed_priority AS INT64) = t.processed_priority AND s.ingested_at > t.ingested_at THEN
            UPDATE SET 
                user_id = s.user_id,
                event_ts = s.event_ts,
                event_date = s.event_date,
                source_collection = s.source_collection,
                gamification_id = s.gamification_id,
                bill_image = s.bill_image,
                status = s.status,
                substatus = s.substatus,
                raw_doc = s.raw_doc,
                shopnetwork = s.shopnetwork,
                shop_name = s.shop_name,
                processed_priority = s.processed_priority,
                ingested_at = s.ingested_at
                
        WHEN NOT MATCHED BY TARGET THEN
            INSERT (
                api_id, user_id, event_ts, event_date, source_collection, gamification_id,
                bill_image, status, substatus, raw_doc, country, shopnetwork, shop_name,
                processed_priority, ingested_at
            )
            VALUES (
                s.api_id, s.user_id, s.event_ts, s.event_date, s.source_collection, s.gamification_id,
                s.bill_image, s.status, s.substatus, s.raw_doc, s.country, s.shopnetwork, s.shop_name,
                s.processed_priority, s.ingested_at
            )
        """
        
        merge_job = bq_client.query(merge_sql, location=LOCATION)
        merge_result = merge_job.result()
        
        rows_merged = merge_job.num_dml_affected_rows or 0
        
        logger.info(f"Merged {rows_merged} successful receipts into all_data for ALL supported countries (no duplicates)")
        
        return {
            "status": "success",
            "table": "all_data",
            "rows_inserted": rows_merged,
            "countries": SUPPORTED_COUNTRIES,  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω
            "target_date": target_date
        }
        
    except Exception as e:
        logger.exception(f"Error loading successful receipts to all_data for all supported countries")
        return {
            "status": "error",
            "table": "all_data",
            "message": str(e),
            "countries": SUPPORTED_COUNTRIES,
            "rows_inserted": 0
        }
        
    finally:
        # –í—Å–µ–≥–¥–∞ —É–¥–∞–ª—è–µ–º staging —Ç–∞–±–ª–∏—Ü—É, –¥–∞–∂–µ –µ—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞
        if staging_table:
            try:
                cleanup_sql = f"DROP TABLE IF EXISTS `{staging_table}`"
                bq_client.query(cleanup_sql, location=LOCATION).result()
                logger.info(f"Cleaned up staging table {staging_table}")
            except Exception as cleanup_error:
                logger.warning(f"Failed to cleanup staging table {staging_table}: {cleanup_error}")

class BatchReceiptProcessor:
    """Batch processing of receipts using Vertex AI Batch Inference with complete workflow."""
    
    def __init__(self) -> None:
        self.project_id = PROJECT_ID
        self.dataset = DATASET
        self.gamification_bills_table = ALL_DATA_TABLE
        self.bq = bq_client
        self.bucket_name = BUCKET_NAME
        logger.info("BatchReceiptProcessor initialised: dataset=%s", DATASET)

    def create_batch_job_async(self,
                                       report_id: str,
                                       report_name: str,
                                       countries: List[str] = None,
                                       target_date: str = None,
                              date_from: str = None,
                              date_to: str = None,
                              no_date_filter: bool = False,
                              limit: int = None,
                                       test_mode: bool = False) -> Dict[str, Any]:
        """
        –°–æ–∑–¥–∞—Ç—å batch job –∏ –≤–µ—Ä–Ω—É—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ–º –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.
        –ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ —Å—Ç–∞—Ä–æ–º—É –∫–æ–¥—É –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞.
        """
        logger.info("=== ASYNC BATCH JOB CREATION START ===")
        logger.info(f"Parameters: countries={countries}, target_date={target_date}, limit={limit}")
        
        if countries:
            for c in countries:
                if c not in SUPPORTED_COUNTRIES:
                    raise ValueError(f"Unsupported country code in batch: {c}")
        
        # Query receipts from all_data (same logic as complete method)
        where_conditions = ["raw_doc IS NOT NULL"]
        query_params = []

        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò –ü–û –î–ê–¢–ï
        if no_date_filter:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —á–µ–∫–∏ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ
            logger.info("Processing ALL receipts without date filter")
        elif target_date:
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞
            where_conditions.append("event_date = @target_date")
            query_params.append(bigquery.ScalarQueryParameter("target_date", "DATE", target_date))
            logger.info(f"Processing receipts for date: {target_date}")
        elif date_from or date_to:
            # –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
            if date_from:
                where_conditions.append("event_date >= @date_from")
                query_params.append(bigquery.ScalarQueryParameter("date_from", "DATE", date_from))
            if date_to:
                where_conditions.append("event_date <= @date_to")
                query_params.append(bigquery.ScalarQueryParameter("date_to", "DATE", date_to))
            logger.info(f"Processing receipts from {date_from} to {date_to}")
        else:
            # –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π (–∞ –Ω–µ 24 —á–∞—Å–∞!)
            date_from_default = (datetime.utcnow() - timedelta(days=30)).date().isoformat()
            where_conditions.append("event_date >= @date_from")
            query_params.append(bigquery.ScalarQueryParameter("date_from", "DATE", date_from_default))
            logger.info(f"Processing receipts from last 30 days: {date_from_default}")

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if countries:
            country_list = "', '".join(countries)
            where_conditions.append(f"country IN ('{country_list}')")

        where_clause = " AND ".join(where_conditions)

        # –î–æ–±–∞–≤–ª—è–µ–º LIMIT –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        limit_clause = ""
        if limit and limit > 0:
            limit_clause = f"LIMIT {limit}"
            logger.info(f"Applying limit: {limit} receipts")

        sql = f"""
        SELECT
          ad.api_id,
          ad.user_id,
          JSON_VALUE(ad.raw_doc, '$.products_string') AS products_str,
          COALESCE(JSON_VALUE(ad.raw_doc, '$.sum'),
                   JSON_VALUE(ad.raw_doc, '$.total')) AS total_str,
          JSON_VALUE(ad.raw_doc, '$.address') AS address,
          JSON_VALUE(ad.raw_doc, '$.nip') AS nip,
          ad.shopnetwork,
          ad.shop_name,
          ad.event_ts,
          ad.event_date,
          JSON_VALUE(ad.raw_doc, '$.is_success') AS is_success,
          ad.status,
          ad.substatus,
          ad.gamification_id,
          ad.country
        FROM `{self.gamification_bills_table}` ad
        WHERE {where_clause}
          AND JSON_VALUE(ad.raw_doc, '$.is_success') IN ('1', '2', '4')  -- Success: 1=automation, 2=moderator, 4=synchronized
          AND JSON_VALUE(ad.raw_doc, '$.is_finished') = 'true'  -- Only finished receipts
          AND JSON_VALUE(ad.raw_doc, '$.products_string') IS NOT NULL
          AND JSON_VALUE(ad.raw_doc, '$.products_string') != '[]'
        ORDER BY ad.event_ts DESC
        {limit_clause}
        """
        
        job_cfg = bigquery.QueryJobConfig(query_parameters=query_params)
        df = self.bq.query(sql, job_config=job_cfg).to_dataframe()

        logger.info(f"Found {len(df)} total rows from BigQuery (limit={limit if limit else 'none'})")

        if df.empty:
            return {"status": "error", "message": "no data found for batch processing"}

        # Prepare batch data (same logic as complete method)
        shop_data_map = {}
        jsonl_lines = []
        processed_count = 0
        skipped_count = 0

        for _, row in df.iterrows():
            try:
                api_id = str(row["api_id"])
                products_str = row["products_str"]
                
                if not products_str or pd.isna(products_str):
                    skipped_count += 1
                    continue
                
                try:
                    products_json = json.loads(products_str)
                except json.JSONDecodeError as e:
                    skipped_count += 1
                    continue
                
                if not isinstance(products_json, list):
                    skipped_count += 1
                    continue
                    
                valid_products = []
                for p in products_json:
                    if isinstance(p, dict) and p.get('name'):
                        valid_products.append(p)
                
                if not valid_products:
                    skipped_count += 1
                    continue
                
                total_price = None
                if row.get("total_str"):
                    try:
                        total_price = float(row["total_str"])
                    except (ValueError, TypeError):
                        pass
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω—É —á–µ–∫–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                receipt_country = row.get("country", "PL")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∞ —á–µ–∫–∞ –≤—Ö–æ–¥–∏—Ç –≤ —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω
                if countries and receipt_country not in countries:
                    skipped_count += 1
                    continue
                
                # Store shop data for later
                shop_data_map[api_id] = {
                    'country': receipt_country,
                    'shopnetwork': row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    'shop_name': row["shop_name"] if pd.notna(row["shop_name"]) else None,
                    'address': row["address"] if pd.notna(row["address"]) else None,
                    'nip': row["nip"] if pd.notna(row["nip"]) else None,
                    'gamification_id': row["gamification_id"] if pd.notna(row["gamification_id"]) else None,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª—è–µ–º gamification_id
                    'products_json': valid_products
                }
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ò –≥–æ—Ä–æ–¥–∞
                logger.info(f"Creating combined prompt for {api_id} in {receipt_country}")
                prompt = create_combined_correction_prompt(
                    api_id=api_id,
                    products_json=valid_products,
                    total_price=total_price,
                    country_code=receipt_country,
                    nip=row["nip"] if pd.notna(row["nip"]) else None,
                    shopnetwork=row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    raw_address=row["address"] if pd.notna(row["address"]) else None
                )

                jsonl_line = {
                        "request": {
                            "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                            "generationConfig": {
                                "temperature": 0, 
                                "maxOutputTokens": 8192, 
                                "candidateCount": 1,
                                "responseMimeType": "application/json"
                            }
                        }
                }
                jsonl_lines.append(json.dumps(jsonl_line))
                processed_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.error(f"Failed to prepare batch data for api_id={row.get('api_id')}: {e}")
                continue

        logger.info(f"Batch preparation summary: {processed_count} processed, {skipped_count} skipped from {len(df)} total rows")

        if not jsonl_lines:
            return {"status": "error", "message": "no valid receipts found for batch processing"}
        
        # Upload JSONL to GCS
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        input_filename = f"batch_async_input_{report_id}_{timestamp}.jsonl"
        output_prefix = f"batch_async_output_{report_id}_{timestamp}"
        
        input_uri = f"gs://{self.bucket_name}/{input_filename}"
        output_uri = f"gs://{self.bucket_name}/{output_prefix}/"
        
        # Upload input file
        bucket = storage_client.bucket(self.bucket_name)
        blob = bucket.blob(input_filename)
        blob.upload_from_string("\n".join(jsonl_lines), content_type="application/jsonl")
        logger.info("Uploaded %s lines to %s", len(jsonl_lines), input_uri)
        
        # Create batch prediction job
        job_display_name = f"receipt-batch-async-{report_id}-{timestamp}"
        
        if test_mode:
            test_lines = jsonl_lines[:5]  # Limit for testing
            test_filename = f"test_{input_filename}"
            test_uri = f"gs://{self.bucket_name}/{test_filename}"
            test_blob = bucket.blob(test_filename)
            test_blob.upload_from_string("\n".join(test_lines), content_type="application/jsonl")
            input_uri = test_uri
            logger.info("Test mode: processing only %s lines", len(test_lines))
        
        batch_job = aiplatform.BatchPredictionJob.create(
            job_display_name=job_display_name,
            model_name="publishers/google/models/gemini-2.0-flash-001",
            instances_format="jsonl",
            predictions_format="jsonl",
            gcs_source=[input_uri],
            gcs_destination_prefix=output_uri
        )
        
        logger.info("Created async batch job: %s", batch_job.resource_name)
        
        return {
            "status": "created",
            "message": "Batch job created and running",
            "job_name": batch_job.resource_name,
            "report_id": report_id,
            "report_name": report_name,
            "job_display_name": job_display_name,
            "input_uri": input_uri,
            "output_uri": output_uri,
            "processing_summary": {
                "total_rows": len(df),
                "processed": processed_count,
                "skipped": skipped_count
            }
        }

    def process_batch_receipts_complete(self,
                                       report_id: str,
                                       report_name: str,
                                       countries: List[str] = None,
                                       target_date: str = None,
                                       date_from: str = None,
                                       date_to: str = None,
                                       since_timestamp: str = None,  # –î–û–ë–ê–í–õ–Ø–ï–ú since_timestamp!
                                       no_date_filter: bool = False,
                                       limit: int = None,
                                       test_mode: bool = False) -> Dict[str, Any]:
        """
        Complete batch processing: create job, wait for completion, process results.
        """
        logger.info("=== COMPLETE BATCH PROCESSING START ===")
        logger.info(f"Parameters: countries={countries}, target_date={target_date}, limit={limit}")
        
        start_time = time.time()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞–Ω—ã
        if countries:
            unsupported = [c for c in countries if c not in SUPPORTED_COUNTRIES]
            if unsupported:
                logger.warning(f"‚ö†Ô∏è Ignoring unsupported countries: {unsupported}")
            countries = [c for c in countries if c in SUPPORTED_COUNTRIES]
            if not countries:
                logger.warning("‚ùå No supported countries in request, using all supported countries")
                countries = None  # –ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤—Å–µ
        
        # Query receipts from all_data
        where_conditions = ["raw_doc IS NOT NULL"]
        query_params = []

        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –§–ò–õ–¨–¢–†–ê–¶–ò–ò –ü–û –î–ê–¢–ï –î–õ–Ø ALL_DATA
        if no_date_filter:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —á–µ–∫–∏ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –¥–∞—Ç–µ
            logger.info("Processing ALL receipts without date filter")
        elif since_timestamp:
            # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
            where_conditions.append("event_ts >= @since_timestamp")
            query_params.append(bigquery.ScalarQueryParameter("since_timestamp", "TIMESTAMP", since_timestamp))
            logger.info(f"Processing receipts since: {since_timestamp}")
        elif target_date:
            # –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞
            where_conditions.append("event_date = @target_date")
            query_params.append(bigquery.ScalarQueryParameter("target_date", "DATE", target_date))
            logger.info(f"Processing receipts for date: {target_date}")
        elif date_from or date_to:
            # –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
            if date_from:
                where_conditions.append("event_date >= @date_from")
                query_params.append(bigquery.ScalarQueryParameter("date_from", "DATE", date_from))
            if date_to:
                where_conditions.append("event_date <= @date_to")
                query_params.append(bigquery.ScalarQueryParameter("date_to", "DATE", date_to))
            logger.info(f"Processing receipts from {date_from} to {date_to}")
        else:
            # –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ: –í–°–ï –¥–∞–Ω–Ω—ã–µ (–∞ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞!)
            logger.info("Processing ALL receipts (default behavior)")

        # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if countries:
            country_list = "', '".join(countries)
            where_conditions.append(f"country IN ('{country_list}')")

        where_clause = " AND ".join(where_conditions)

        # –î–æ–±–∞–≤–ª—è–µ–º LIMIT –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
        limit_clause = ""
        if limit and limit > 0:
            limit_clause = f"LIMIT {limit}"
            logger.info(f"Applying limit: {limit} receipts")

        sql = f"""
        SELECT
          ad.api_id,
          ad.user_id,
          JSON_VALUE(ad.raw_doc, '$.products_string') AS products_str,
          COALESCE(JSON_VALUE(ad.raw_doc, '$.sum'),
                   JSON_VALUE(ad.raw_doc, '$.total')) AS total_str,
          JSON_VALUE(ad.raw_doc, '$.address') AS address,
          JSON_VALUE(ad.raw_doc, '$.nip') AS nip,
          ad.shopnetwork,
          ad.shop_name,
          ad.event_ts,
          ad.event_date,
          JSON_VALUE(ad.raw_doc, '$.is_success') AS is_success,
          ad.status,
          ad.substatus,
          ad.gamification_id,
          ad.country
        FROM `{self.gamification_bills_table}` ad
        WHERE {where_clause}
          AND JSON_VALUE(ad.raw_doc, '$.is_success') IN ('1', '2', '4')  -- Success: 1=automation, 2=moderator, 4=synchronized
          AND JSON_VALUE(ad.raw_doc, '$.is_finished') = 'true'  -- Only finished receipts
          AND JSON_VALUE(ad.raw_doc, '$.products_string') IS NOT NULL
          AND JSON_VALUE(ad.raw_doc, '$.products_string') != '[]'
        ORDER BY ad.event_ts DESC
        {limit_clause}
        """
        
        job_cfg = bigquery.QueryJobConfig(query_parameters=query_params)
        df = self.bq.query(sql, job_config=job_cfg).to_dataframe()

        logger.info(f"Found {len(df)} total rows from BigQuery (limit={limit if limit else 'none'})")

        if df.empty:
            return {"status": "error", "message": "no data found for batch processing"}

        # Store metadata for later use
        shop_data_map = {}
        jsonl_lines = []
        processed_count = 0
        skipped_count = 0

        for _, row in df.iterrows():
            try:
                api_id = str(row["api_id"])
                products_str = row["products_str"]
                
                if not products_str or pd.isna(products_str):
                    skipped_count += 1
                    continue
                
                try:
                    products_json = json.loads(products_str)
                except json.JSONDecodeError as e:
                    skipped_count += 1
                    continue
                
                if not isinstance(products_json, list):
                    skipped_count += 1
                    continue
                    
                valid_products = []
                for p in products_json:
                    if isinstance(p, dict) and p.get('name'):
                        valid_products.append(p)
                
                if not valid_products:
                    skipped_count += 1
                    continue
                
                total_price = None
                if row.get("total_str"):
                    try:
                        total_price = float(row["total_str"])
                    except (ValueError, TypeError):
                        pass
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω—É —á–µ–∫–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                receipt_country = row.get("country", "PL")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–∞–Ω–∞ —á–µ–∫–∞ –≤—Ö–æ–¥–∏—Ç –≤ —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º—ã—Ö —Å—Ç—Ä–∞–Ω
                if countries and receipt_country not in countries:
                    skipped_count += 1
                    continue
                
                # Store shop data for later
                shop_data_map[api_id] = {
                    'country': receipt_country,
                    'shopnetwork': row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    'shop_name': row["shop_name"] if pd.notna(row["shop_name"]) else None,
                    'address': row["address"] if pd.notna(row["address"]) else None,
                    'nip': row["nip"] if pd.notna(row["nip"]) else None,
                    'gamification_id': row["gamification_id"] if pd.notna(row["gamification_id"]) else None,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª—è–µ–º gamification_id
                    'products_json': valid_products
                }
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ò –≥–æ—Ä–æ–¥–∞
                logger.info(f"Creating combined prompt for {api_id} in {receipt_country}")
                prompt = create_combined_correction_prompt(
                    api_id=api_id,
                    products_json=valid_products,
                    total_price=total_price,
                    country_code=receipt_country,
                    nip=row["nip"] if pd.notna(row["nip"]) else None,
                    shopnetwork=row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    raw_address=row["address"] if pd.notna(row["address"]) else None
                )

                jsonl_line = {
                    "request": {
                        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                        "generationConfig": {
                            "temperature": 0, 
                            "maxOutputTokens": 8192, 
                            "candidateCount": 1,
                            "responseMimeType": "application/json"
                        }
                    }
                }
                jsonl_lines.append(json.dumps(jsonl_line))
                processed_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.error(f"Failed to prepare batch data for api_id={row.get('api_id')}: {e}")
                continue

        logger.info(f"Batch processing summary: {processed_count} processed, {skipped_count} skipped from {len(df)} total rows")

        if not jsonl_lines:
            return {"status": "error", "message": "no valid receipts found for batch processing"}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —á–µ–∫–∏ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        logger.info(f"Processing {len(jsonl_lines)} receipts in batch job")
        
        # Upload JSONL to GCS
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        input_filename = f"batch_input_{report_id}_{timestamp}.jsonl"
        output_prefix = f"batch_output_{report_id}_{timestamp}"
        
        input_uri = f"gs://{self.bucket_name}/{input_filename}"
        output_uri = f"gs://{self.bucket_name}/{output_prefix}/"
        
        # Upload input file
        bucket = storage_client.bucket(self.bucket_name)
        blob = bucket.blob(input_filename)
        blob.upload_from_string("\n".join(jsonl_lines), content_type="application/jsonl")
        logger.info("Uploaded %s lines to %s", len(jsonl_lines), input_uri)
        
        # Create batch prediction job
        job_display_name = f"receipt-batch-{report_id}-{timestamp}"
        
        if test_mode:
            test_lines = jsonl_lines[:5]  # Limit for testing
            test_filename = f"test_{input_filename}"
            test_uri = f"gs://{self.bucket_name}/{test_filename}"
            test_blob = bucket.blob(test_filename)
            test_blob.upload_from_string("\n".join(test_lines), content_type="application/jsonl")
            input_uri = test_uri
            logger.info("Test mode: processing only %s lines", len(test_lines))
        
        batch_job = aiplatform.BatchPredictionJob.create(
            job_display_name=job_display_name,
            model_name="publishers/google/models/gemini-2.0-flash-001",
            instances_format="jsonl",
            predictions_format="jsonl",
            gcs_source=[input_uri],
            gcs_destination_prefix=output_uri
        )
        
        logger.info("Created batch job: %s", batch_job.resource_name)
        logger.info("Waiting for batch job completion...")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π wait() –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ
            batch_job.wait()
            logger.info("Batch job completed successfully!")
        except Exception as e:
            return {"status": "error", "message": f"Batch job failed: {str(e)}"}
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        state = batch_job.state
        if JobState and state == JobState.JOB_STATE_SUCCEEDED:
            logger.info("Job succeeded")
        elif "SUCCEEDED" in str(state):
            logger.info("Job succeeded (string check)")
        else:
            return {"status": "error", "message": f"Job failed with state: {state}"}
        
        # PROCESS RESULTS IMMEDIATELY - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –ö–ê–ö –í –°–¢–ê–†–û–ú –ö–û–î–ï
        logger.info("Processing batch job results...")
        
        output_location = batch_job.output_info.gcs_output_directory
        
        # –ù–ê–ö–û–ü–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –î–õ–Ø –ë–ê–¢–ß–ï–í–û–ô –ó–ê–ì–†–£–ó–ö–ò
        accumulated_products = []
        accumulated_shops = []
        accumulated_vector_products = []
        bucket_name = output_location.replace("gs://", "").split("/")[0]
        prefix = "/".join(output_location.replace("gs://", "").split("/")[1:])
        
        bucket = storage_client.bucket(bucket_name)
        
        logger.info(f"Processing results from: gs://{bucket_name}/{prefix}")
        
        total_processed = 0
        total_products = 0
        stats = {
            "successful_products": 0,
            "successful_shops": 0,
            "failed_products": 0,
            "failed_shops": 0,
            "price_corrections": 0,
            "name_corrections": 0
        }
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö .jsonl —Ñ–∞–π–ª–æ–≤ –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ
        result_files_processed = 0
        for blob in bucket.list_blobs(prefix=prefix):
            if blob.name.endswith('.jsonl'):
                logger.info(f"Processing result file: {blob.name}")
                try:
                    content = blob.download_as_text()
                    result_files_processed += 1
                    
                    for line_num, line in enumerate(content.strip().split('\n'), 1):
                        if not line.strip():
                            continue
                        
                        try:
                            result = json.loads(line)
                            response_text = _extract_text_from_batch_line(result)
                            
                            if not response_text:
                                logger.warning(f"No response text in line {line_num} of {blob.name}")
                                continue
                            
                            corrected_data = clean_and_parse_json(response_text)
                            
                            # Save products
                            if corrected_data.get("products"):
                                try:
                                    # –ù–ê–ö–û–ü–õ–Ø–ï–ú –î–ê–ù–ù–´–ï –í–ú–ï–°–¢–û –ù–ï–ú–ï–î–õ–ï–ù–ù–û–ì–û –°–û–•–†–ê–ù–ï–ù–ò–Ø
                                    now = _now().isoformat()
                                    for p in corrected_data.get("products", []):
                                        try:
                                            product_row = {
                                                "report_id": report_id,
                                                "report_name": report_name,
                                                "api_id": corrected_data.get("api_id", ""),
                                                "product_name_original": str(p.get("name_original", ""))[:500],
                                                "product_name_corrected": str(p.get("name_corrected", ""))[:500],
                                                "quantity": int(p.get("quantity") or 1),
                                                "price_single_original": float(p.get("price_single_original") or 0.0),
                                                "price_single_corrected": float(p.get("price_single_corrected") or 0.0),
                                                "price_total": float(p.get("price_total") or 0.0),
                                                "name_correction_made": p.get("name_original") != p.get("name_corrected"),
                                                "price_correction_made": bool(p.get("price_correction_reason")),
                                                "price_correction_reason": p.get("price_correction_reason"),
                                                "created_at": now,
                                            }
                                            accumulated_products.append(product_row)
                                            
                                            # Vector products
                                            vector_row = {
                                                "report_id": report_id,
                                                "report_name": report_name,
                                                "api_id": corrected_data.get("api_id", ""),
                                                "clean_product_name": str(p.get("name_corrected", ""))[:500],
                                                "quantity": int(p.get("quantity") or 1),
                                                "price_single": float(p.get("price_single_corrected") or 0.0),
                                                "price_total": float(p.get("price_total") or 0.0),
                                                "created_at": now,
                                            }
                                            accumulated_vector_products.append(vector_row)
                                            
                                        except Exception as e:
                                            logger.warning("Skipping product due to error: %s", e)
                                            continue
                                    
                                    stats['successful_products'] += 1
                                    
                                    for product in corrected_data.get('products', []):
                                        if product.get('name_original') != product.get('name_corrected'):
                                            stats['name_corrections'] += 1
                                        if product.get('price_single_original') != product.get('price_single_corrected'):
                                            stats['price_corrections'] += 1
                                    
                                    total_products += len(corrected_data.get('products', []))
                                except Exception as e:
                                    stats['failed_products'] += 1
                                    logger.error(f"Failed to process products for line {line_num}: {e}")
                            
                            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≥–æ—Ä–æ–¥–µ –∏–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            api_id = corrected_data.get('api_id')
                            if api_id:
                                # –ù–∞–π—Ç–∏ shop_info –ø–æ api_id
                                shop_info = shop_data_map.get(api_id)
                        
                                if shop_info:
                                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≥–æ—Ä–æ–¥–µ –∏–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                                    city_analysis = corrected_data.get('city_analysis', {})
                                    ai_city = city_analysis.get('city')
                                    ai_region = city_analysis.get('region')
                                    city_population = city_analysis.get('city_population')
                                    confidence = city_analysis.get('confidence', 'LOW')
                                    evidence_text = city_analysis.get('evidence', '')
                                    
                                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º match_method, zip_code, province_code –∏–∑ city_analysis
                                    match_method = city_analysis.get('match_method')
                                    ai_zip_code = city_analysis.get('zip_code')
                                    ai_province_code = city_analysis.get('province_code')
                            
                                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≥–æ—Ä–æ–¥ —á–µ—Ä–µ–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
                                    if ai_city and ai_city != 'UNKNOWN':
                                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–µ—Ä–µ–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏
                                        normalized_city, normalized_region, region_code = lookup_pl_location(ai_city, shop_info.get('country', 'PL'))
                                
                                        if normalized_city:
                                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                                            final_city = normalized_city
                                            final_region = normalized_region
                                            final_region_code = region_code
                                            logger.info(f"Normalized city for {api_id}: {ai_city} -> {final_city}, region: {final_region}")
                                        else:
                                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç AI
                                            final_city = ai_city
                                            final_region = ai_region
                                            final_region_code = None
                                            logger.warning(f"Could not normalize city {ai_city} for {api_id}, using AI result")
                                
                                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º JSON –æ–±—ä–µ–∫—Ç –¥–ª—è evidence —Å match_method, zip_code, province_code
                                        evidence_dict = {
                                            'match_method': match_method,
                                            'zip_code': ai_zip_code,
                                            'province_code': ai_province_code,
                                            'confidence': confidence,
                                            'evidence': evidence_text
                                        }
                                        evidence_json = json.dumps(evidence_dict, ensure_ascii=False)
                                
                                        shop_info.update({
                                            'city': final_city,
                                            'region': final_region,
                                            'region_code': final_region_code,
                                            'city_population': city_population,
                                            'confidence': confidence,
                                            'evidence': evidence_json,
                                            'match_method': match_method,
                                            'zip_code': ai_zip_code,
                                            'province_code': ai_province_code
                                        })
                                        logger.info(f"Final city for {api_id}: {final_city}, {final_region}, population={city_population}, confidence={confidence}")
                                    else:
                                        logger.warning(f"AI could not determine city for {api_id}")
                            
                                    try:
                                        # –ù–ê–ö–û–ü–õ–Ø–ï–ú SHOP –î–ê–ù–ù–´–ï (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π)
                                        shop_chain = shop_info.get('shopnetwork') or shop_info.get('shop_name') or "UNKNOWN"
                                        shop_row = {
                                            "report_id": report_id,
                                            "report_name": report_name,
                                            "api_id": api_id,
                                            "nip": shop_info.get('nip') or "",
                                            "shop_chain": normalize_network_name_by_country(shop_chain, shop_info.get('country', 'PL')),
                                            "city": shop_info.get('city') or "UNKNOWN",
                                            "region": shop_info.get('region') or "UNKNOWN",
                                            "region_code": shop_info.get('region_code'),
                                            "shop_address": shop_info.get('address')[:255] if shop_info.get('address') else None,
                                            "city_population": parse_city_population(shop_info.get('city_population')),
                                            "country": shop_info.get('country', 'PL'),
                                            "gamification_id": shop_info.get('gamification_id'),
                                            "confidence": shop_info.get('confidence') or "AUTO",
                                            "evidence": shop_info.get('evidence'),
                                            "match_method": shop_info.get('match_method'),
                                            "zip_code": shop_info.get('zip_code'),
                                            "province_code": shop_info.get('province_code'),
                                            "created_at": _now().isoformat(),
                                        }
                                        accumulated_shops.append(shop_row)
                                        stats['successful_shops'] += 1
                                    except Exception as e:
                                        stats['failed_shops'] += 1
                                        logger.error(f"Failed to process shop for line {line_num}: {e}")
                                else:
                                    logger.warning(f"No shop data found for api_id {api_id} in line {line_num}")
                            
                            total_processed += 1
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON decode error in line {line_num} of {blob.name}: {e}")
                            continue
                        except Exception as e:
                            logger.error(f"Error processing line {line_num}: {e}")
                            continue
                except Exception as e:
                    logger.error(f"Error processing result file {blob.name}: {e}")
                    continue
        
        logger.info(f"Processed {result_files_processed} result files, {total_processed} total receipts")
        
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ë–ê–¢–ß–ï–í–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•
        logger.info("üöÄ STARTING FINAL BATCH LOAD OF ACCUMULATED DATA")
        logger.info("üìä Products: %s, Shops: %s, Vector Products: %s", 
                   len(accumulated_products), len(accumulated_shops), len(accumulated_vector_products))
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
            if accumulated_products:
                logger.info("üîÑ Loading %s products to corrected_products", len(accumulated_products))
                _storage_write_api_load(PRODUCTS_TABLE, accumulated_products, report_id, report_name)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
            if accumulated_vector_products:
                logger.info("üîÑ Loading %s vector products to products_vector_ready", len(accumulated_vector_products))
                _storage_write_api_load(VECTOR_READY_TABLE, accumulated_vector_products, report_id, report_name)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–≥–∞–∑–∏–Ω—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
            if accumulated_shops:
                logger.info("üîÑ Loading %s shops to shop_directory", len(accumulated_shops))
                _storage_write_api_load(SHOP_TABLE, accumulated_shops, report_id, report_name)
            
            logger.info("‚úÖ ALL ACCUMULATED DATA LOADED SUCCESSFULLY")
            
        except Exception as e:
            logger.error("‚ùå FAILED TO LOAD ACCUMULATED DATA: %s", e)
            raise
        
        processing_time = int(time.time() - start_time)
        
        return {
            "status": "success",
            "message": f"Complete batch processing finished! Processed {total_processed} receipts",
            "report_id": report_id,
            "report_name": report_name,
            "processing_time_seconds": processing_time,
            "batch_job_name": batch_job.resource_name,
            "receipts_processed": total_processed,
            "products_saved": total_products,
            "stats": stats,
            "processing_summary": {
                "total_rows": len(df),
                "processed": processed_count,
                "skipped": skipped_count
            }
        }

    def process_batch_by_ids(self,
                            report_id: str,
                            report_name: str,
                            api_ids: List[str],
                            country: str = "PL",
                            test_mode: bool = False) -> Dict[str, Any]:
        """Process specific receipts by their api_ids with complete workflow."""
        logger.info("=== BATCH BY IDS COMPLETE PROCESSING START ===")
        logger.info(f"Processing {len(api_ids)} specific api_ids")
        
        start_time = time.time()
        
        if not api_ids:
            return {"status": "error", "message": "No api_ids provided"}
        
        # Query receipts for specific api_ids
        placeholders = ','.join([f'@api_id_{i}' for i in range(len(api_ids))])
        
        sql = f"""
        SELECT
          ad.api_id,
          ad.user_id,
          JSON_VALUE(ad.raw_doc, '$.products_string') AS products_str,
          COALESCE(JSON_VALUE(ad.raw_doc, '$.sum'),
                   JSON_VALUE(ad.raw_doc, '$.total')) AS total_str,
          JSON_VALUE(ad.raw_doc, '$.address') AS address,
          JSON_VALUE(ad.raw_doc, '$.nip') AS nip,
          ad.shopnetwork,
          ad.shop_name,
          ad.event_ts,
          ad.event_date,
          JSON_VALUE(ad.raw_doc, '$.is_success') AS is_success,
          ad.status,
          ad.substatus,
          ad.gamification_id,
          ad.country
        FROM `{self.gamification_bills_table}` ad
        WHERE ad.api_id IN ({placeholders})
        ORDER BY ad.api_id
        """
        
        query_params = [
            bigquery.ScalarQueryParameter(f"api_id_{i}", "STRING", api_id)
            for i, api_id in enumerate(api_ids)
        ]
        query_params.append(bigquery.ScalarQueryParameter("country", "STRING", country))
        
        job_cfg = bigquery.QueryJobConfig(query_parameters=query_params)
        df = self.bq.query(sql, job_config=job_cfg).to_dataframe()

        logger.info(f"Found {len(df)} records for {len(api_ids)} requested IDs")
        
        if df.empty:
            return {"status": "error", "message": f"No data found for provided api_ids"}

        # Check which IDs were not found
        found_ids = set(df['api_id'].tolist())
        missing_ids = set(api_ids) - found_ids
        if missing_ids:
            logger.warning(f"Missing api_ids: {list(missing_ids)}")

        # Process same as batch_receipts_complete but for specific IDs
        shop_data_map = {}
        jsonl_lines = []
        processed_count = 0
        skipped_count = 0

        for _, row in df.iterrows():
            try:
                api_id = str(row["api_id"])
                products_str = row["products_str"]
                
                if not products_str or pd.isna(products_str):
                    skipped_count += 1
                    continue
                
                try:
                    products_json = json.loads(products_str)
                except json.JSONDecodeError:
                    skipped_count += 1
                    continue
                
                if not isinstance(products_json, list):
                    skipped_count += 1
                    continue
                    
                valid_products = [p for p in products_json if isinstance(p, dict) and p.get('name')]
                
                if not valid_products:
                    skipped_count += 1
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞–Ω—É —á–µ–∫–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö
                receipt_country = row.get("country", country)
                
                # Store shop data using real country from data
                shop_data_map[api_id] = {
                    'country': receipt_country,
                    'shopnetwork': row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    'shop_name': row["shop_name"] if pd.notna(row["shop_name"]) else None,
                    'address': row["address"] if pd.notna(row["address"]) else None,
                    'nip': row["nip"] if pd.notna(row["nip"]) else None,
                    'gamification_id': row["gamification_id"] if pd.notna(row["gamification_id"]) else None,  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –¥–æ–±–∞–≤–ª—è–µ–º gamification_id
                    'products_json': valid_products
                }
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ò –≥–æ—Ä–æ–¥–∞
                logger.info(f"Creating combined prompt for {api_id} in {receipt_country}")
                prompt = create_combined_correction_prompt(
                    api_id=api_id,
                    products_json=valid_products,
                    total_price=total_price,
                    country_code=receipt_country,
                    nip=row["nip"] if pd.notna(row["nip"]) else None,
                    shopnetwork=row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    raw_address=row["address"] if pd.notna(row["address"]) else None
                )
                
                # Create JSONL line for batch processing
                jsonl_lines.append(json.dumps({
                    "request": {
                        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                        "generationConfig": {
                            "temperature": 0,
                            "maxOutputTokens": 8192,
                            "candidateCount": 1,
                            "responseMimeType": "application/json"
                        }
                    }
                }))
                
                total_price = None
                if row.get("total_str"):
                    try:
                        total_price = float(row["total_str"])
                    except (ValueError, TypeError):
                        pass
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ò –≥–æ—Ä–æ–¥–∞
                prompt = create_combined_correction_prompt(
                    api_id=api_id,
                    products_json=valid_products,
                    total_price=total_price,
                    country_code=receipt_country,
                    nip=row["nip"] if pd.notna(row["nip"]) else None,
                    shopnetwork=row["shopnetwork"] if pd.notna(row["shopnetwork"]) else None,
                    raw_address=row["address"] if pd.notna(row["address"]) else None
                )

                jsonl_line = {
                    "request": {
                        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
                        "generationConfig": {
                            "temperature": 0, 
                            "maxOutputTokens": 8192, 
                            "candidateCount": 1,
                            "responseMimeType": "application/json"
                        }
                    }
                }
                jsonl_lines.append(json.dumps(jsonl_line))
                processed_count += 1
                
            except Exception as e:
                skipped_count += 1
                logger.error(f"Failed to prepare batch data for api_id={row.get('api_id')}: {e}")
                continue

        if not jsonl_lines:
            return {"status": "error", "message": "no valid receipts found for batch processing from requested IDs"}
        
        # Create and wait for batch job (same as above)
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        input_filename = f"batch_ids_input_{report_id}_{timestamp}.jsonl"
        output_prefix = f"batch_ids_output_{report_id}_{timestamp}"
        
        input_uri = f"gs://{self.bucket_name}/{input_filename}"
        output_uri = f"gs://{self.bucket_name}/{output_prefix}/"
        
        bucket = storage_client.bucket(self.bucket_name)
        blob = bucket.blob(input_filename)
        blob.upload_from_string("\n".join(jsonl_lines), content_type="application/jsonl")
        
        job_display_name = f"receipt-batch-ids-{report_id}-{timestamp}"
        
        if test_mode:
            test_lines = jsonl_lines[:3]
            test_filename = f"test_{input_filename}"
            test_uri = f"gs://{self.bucket_name}/{test_filename}"
            test_blob = bucket.blob(test_filename)
            test_blob.upload_from_string("\n".join(test_lines), content_type="application/jsonl")
            input_uri = test_uri
        
        batch_job = aiplatform.BatchPredictionJob.create(
            job_display_name=job_display_name,
            model_name="publishers/google/models/gemini-2.0-flash-001",
            instances_format="jsonl",
            predictions_format="jsonl",
            gcs_source=[input_uri],
            gcs_destination_prefix=output_uri
        )
        
        logger.info("Created batch job, waiting for completion...")
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç–æ–π wait() –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–º –∫–æ–¥–µ
            batch_job.wait()
            logger.info("Batch job completed successfully!")
        except Exception as e:
            return {"status": "error", "message": f"Batch job failed: {str(e)}"}
        
        # –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        state = batch_job.state
        if JobState and state == JobState.JOB_STATE_SUCCEEDED:
            logger.info("Job succeeded")
        elif "SUCCEEDED" in str(state):
            logger.info("Job succeeded (string check)")
        else:
            return {"status": "error", "message": f"Job failed with state: {state}"}
        
        # Process results (same logic as above)
        output_location = batch_job.output_info.gcs_output_directory
        bucket_name = output_location.replace("gs://", "").split("/")[0]
        prefix = "/".join(output_location.replace("gs://", "").split("/")[1:])
        
        bucket = storage_client.bucket(bucket_name)
        
        total_processed = 0
        total_products = 0
        
        for blob in bucket.list_blobs(prefix=prefix):
            if blob.name.endswith('.jsonl'):
                content = blob.download_as_text()
                for line in content.strip().split('\n'):
                    if not line.strip():
                        continue
                    
                    try:
                        result = json.loads(line)
                        response_text = _extract_text_from_batch_line(result)
                        
                        if response_text:
                            corrected_data = clean_and_parse_json(response_text)
                            
                            # Products —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –≤—ã—à–µ –≤ —Ü–∏–∫–ª–µ
                            
                            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≥–æ—Ä–æ–¥–µ –∏–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            api_id = corrected_data.get('api_id')
                            if api_id and api_id in shop_data_map:
                                shop_info = shop_data_map[api_id]
                                
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≥–æ—Ä–æ–¥–µ –∏–∑ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                                city_analysis = corrected_data.get('city_analysis', {})
                                ai_city = city_analysis.get('city')
                                ai_region = city_analysis.get('region')
                                city_population = city_analysis.get('city_population')
                                confidence = city_analysis.get('confidence', 'LOW')
                                evidence_text = city_analysis.get('evidence', '')
                                
                                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º match_method, zip_code, province_code –∏–∑ city_analysis
                                match_method = city_analysis.get('match_method')
                                ai_zip_code = city_analysis.get('zip_code')
                                ai_province_code = city_analysis.get('province_code')
                                
                                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≥–æ—Ä–æ–¥ —á–µ—Ä–µ–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –∫–∞–∫ –≤ —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
                                if ai_city and ai_city != 'UNKNOWN':
                                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —á–µ—Ä–µ–∑ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏
                                    normalized_city, normalized_region, region_code = lookup_pl_location(ai_city, shop_info.get('country', 'PL'))
                                    
                                    if normalized_city:
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                                        final_city = normalized_city
                                        final_region = normalized_region
                                        final_region_code = region_code
                                        logger.info(f"Normalized city for {api_id}: {ai_city} -> {final_city}, region: {final_region}")
                                    else:
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç AI
                                        final_city = ai_city
                                        final_region = ai_region
                                        final_region_code = None
                                        logger.warning(f"Could not normalize city {ai_city} for {api_id}, using AI result")
                                    
                                    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º JSON –æ–±—ä–µ–∫—Ç –¥–ª—è evidence —Å match_method, zip_code, province_code
                                    evidence_dict = {
                                        'match_method': match_method,
                                        'zip_code': ai_zip_code,
                                        'province_code': ai_province_code,
                                        'confidence': confidence,
                                        'evidence': evidence_text
                                    }
                                    evidence_json = json.dumps(evidence_dict, ensure_ascii=False)
                                    
                                    shop_info.update({
                                        'city': final_city,
                                        'region': final_region,
                                        'region_code': final_region_code,
                                        'city_population': city_population,
                                        'confidence': confidence,
                                        'evidence': evidence_json,
                                        'match_method': match_method,
                                        'zip_code': ai_zip_code,
                                        'province_code': ai_province_code
                                    })
                                    logger.info(f"Final city for {api_id}: {final_city}, {final_region}, population={city_population}, confidence={confidence}")
                                else:
                                    logger.warning(f"AI could not determine city for {api_id}")
                                
                                save_shop_to_bq(
                                    report_id=report_id,
                                    report_name=report_name,
                                    api_id=api_id,
                                    country=shop_info.get('country', country),
                                    shopnetwork=shop_info.get('shopnetwork'),
                                    shop_name=shop_info.get('shop_name'),
                                    raw_address=shop_info.get('address'),
                                    nip=shop_info.get('nip'),
                                    products_json=shop_info.get('products_json', []),
                                    gamification_id=shop_info.get('gamification_id')  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø–µ—Ä–µ–¥–∞–µ–º gamification_id
                                )
                            
                            total_processed += 1
                        
                    except Exception as e:
                        logger.error(f"Error processing result: {e}")
                        continue
        
        processing_time = int(time.time() - start_time)
        
        return {
            "status": "success",
            "message": f"Complete batch IDs processing finished! Processed {total_processed} receipts",
            "report_id": report_id,
            "report_name": report_name,
            "processing_time_seconds": processing_time,
            "batch_job_name": batch_job.resource_name,
            "requested_ids": len(api_ids),
            "found_ids": len(df),
            "missing_ids": list(missing_ids) if missing_ids else [],
            "receipts_processed": total_processed,
            "products_saved": total_products
        }

    def process_batch_results_from_gcs(self, report_id: str, report_name: str, input_uri: str) -> Dict[str, Any]:
        """Process batch results from GCS predictions.jsonl file."""
        logger.info(f"Processing batch results from GCS: {input_uri}")
        start_time = time.time()
        
        try:
            # Download and process the predictions.jsonl file
            import google.cloud.storage as storage
            
            # Parse GCS URI
            if not input_uri.startswith('gs://'):
                return {"status": "error", "message": "input_uri must be a GCS URI (gs://...)"}
            
            bucket_name = input_uri.split('/')[2]
            blob_name = '/'.join(input_uri.split('/')[3:])
            
            # Download file from GCS
            storage_client = storage.Client()
            bucket = storage_client.bucket(bucket_name)
            blob = bucket.blob(blob_name)
            
            if not blob.exists():
                return {"status": "error", "message": f"File not found: {input_uri}"}
            
            # Read the file content
            content = blob.download_as_text()
            lines = content.strip().split('\n')
            
            logger.info(f"Processing {len(lines)} prediction results from GCS")
            logger.info(f"First few lines preview:")
            for i, line in enumerate(lines[:3]):
                logger.info(f"Line {i+1}: {line[:200]}...")
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            total_lines = len(lines)
            processed_successfully = 0
            failed_parsing = 0
            failed_processing = 0
            
            # –ù–ê–ö–û–ü–õ–ï–ù–ò–ï –î–ê–ù–ù–´–• –î–õ–Ø –ë–ê–¢–ß–ï–í–û–ô –ó–ê–ì–†–£–ó–ö–ò
            accumulated_products = []
            accumulated_shops = []
            accumulated_vector_products = []
            
            # Process each prediction result
            total_processed = 0
            total_products = 0
            total_shops = 0
            
            for line_num, line in enumerate(lines, 1):
                try:
                    if not line.strip():
                        continue
                    
                    # Parse JSONL line
                    prediction_data = json.loads(line)
                    
                    if line_num <= 3:
                        logger.info(f"Line {line_num} keys: {list(prediction_data.keys())}")
                    
                    # Extract response text - handle both structures
                    response_text = None
                    
                    # Try new structure first (from our batch results)
                    if 'response' in prediction_data and 'candidates' in prediction_data['response']:
                        candidates = prediction_data['response']['candidates']
                        if candidates and len(candidates) > 0 and 'content' in candidates[0] and 'parts' in candidates[0]['content'] and len(candidates[0]['content']['parts']) > 0:
                            response_text = candidates[0]['content']['parts'][0]['text']
                    
                    # Try old structure (direct text field)
                    elif 'text' in prediction_data:
                        response_text = prediction_data['text']
                    
                    # Try alternative structure
                    elif 'content' in prediction_data and 'parts' in prediction_data['content']:
                        response_text = prediction_data['content']['parts'][0]['text']
                    
                    # Process if we found response text
                    if response_text:
                        logger.info(f"Found response text for line {line_num}, length: {len(response_text)}")
                        # Clean and parse the response
                        corrected_data = clean_and_parse_json(response_text)
                        
                        if corrected_data:
                            logger.info(f"Successfully parsed JSON for line {line_num}")
                            processed_successfully += 1
                            api_id = corrected_data.get('api_id')
                            if api_id:
                                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ shop –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                                logger.debug(f"Processing shop for api_id: {api_id}")
                                
                                # –ù–ê–ö–û–ü–õ–Ø–ï–ú –ü–†–û–î–£–ö–¢–´
                                if corrected_data.get('products'):
                                    now = _now().isoformat()
                                    for p in corrected_data.get("products", []):
                                        try:
                                            product_row = {
                                                "report_id": report_id,
                                                "report_name": report_name,
                                                "api_id": corrected_data.get("api_id", ""),
                                                "product_name_original": str(p.get("name_original", ""))[:500],
                                                "product_name_corrected": str(p.get("name_corrected", ""))[:500],
                                                "quantity": int(p.get("quantity") or 1),
                                                "price_single_original": float(p.get("price_single_original") or 0.0),
                                                "price_single_corrected": float(p.get("price_single_corrected") or 0.0),
                                                "price_total": float(p.get("price_total") or 0.0),
                                                "name_correction_made": p.get("name_original") != p.get("name_corrected"),
                                                "price_correction_made": bool(p.get("price_correction_reason")),
                                                "price_correction_reason": p.get("price_correction_reason"),
                                                "created_at": now,
                                            }
                                            accumulated_products.append(product_row)
                                            
                                            # Vector products
                                            vector_row = {
                                                "report_id": report_id,
                                                "report_name": report_name,
                                                "api_id": corrected_data.get("api_id", ""),
                                                "clean_product_name": str(p.get("name_corrected", ""))[:500],
                                                "quantity": int(p.get("quantity") or 1),
                                                "price_single": float(p.get("price_single_corrected") or 0.0),
                                                "price_total": float(p.get("price_total") or 0.0),
                                                "created_at": now,
                                            }
                                            accumulated_vector_products.append(vector_row)
                                            
                                        except Exception as e:
                                            logger.warning("Skipping product due to error: %s", e)
                                            continue
                                    
                                    total_products += len(corrected_data['products'])
                                
                                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ shop –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                                try:
                                    # Save shop data with city analysis and normalization
                                    city_analysis = corrected_data.get('city_analysis')
                                    
                                    if city_analysis:
                                        # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–≥–¥–∞ city_analysis –µ—Å—Ç—å
                                        ai_city = city_analysis.get('city', 'UNKNOWN')
                                        ai_region = city_analysis.get('region', 'UNKNOWN')
                                        city_population = city_analysis.get('city_population', None)
                                        confidence = city_analysis.get('confidence', 'LOW')
                                        evidence = city_analysis.get('evidence', 'AI city analysis')
                                    else:
                                        # Fallback –∫–æ–≥–¥–∞ city_analysis –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
                                        logger.warning(f"Missing city_analysis for api_id: {corrected_data.get('api_id')}")
                                        ai_city = 'UNKNOWN'
                                        ai_region = 'UNKNOWN'
                                        city_population = None
                                        confidence = 'LOW'
                                        evidence = 'No city analysis available'
                                    
                                    # Try to determine country from original request data
                                    country = "PL"  # Default fallback
                                    
                                    # Extract country from original request if available
                                    if 'request' in prediction_data and 'contents' in prediction_data['request']:
                                        request_contents = prediction_data['request']['contents']
                                        if request_contents and len(request_contents) > 0:
                                            parts = request_contents[0].get('parts', [])
                                            if parts and len(parts) > 0:
                                                request_text = parts[0].get('text', '')
                                                # Look for country in prompt text
                                                if 'Country: IT' in request_text:
                                                    country = "IT"
                                                elif 'Country: FR' in request_text:
                                                    country = "FR"
                                                elif 'Country: PL' in request_text:
                                                    country = "PL"
                                                logger.info(f"Detected country from request: {country}")
                                    
                                    # Normalize city through dictionaries if AI found a city
                                    final_city = ai_city
                                    final_region = ai_region
                                    final_region_code = None
                                    
                                    if ai_city and ai_city != 'UNKNOWN':
                                        try:
                                            normalized_city, normalized_region, region_code = lookup_pl_location(ai_city, country)
                                            if normalized_city:
                                                final_city = normalized_city
                                                final_region = normalized_region
                                                final_region_code = region_code
                                                logger.info(f"Normalized city for {api_id}: {ai_city} -> {final_city}, region: {final_region}")
                                            else:
                                                logger.warning(f"Could not normalize city {ai_city} for {api_id}, using AI result")
                                        except Exception as e:
                                            logger.warning(f"Error normalizing city {ai_city} for {api_id}: {e}. Using AI result.")
                                            final_city = ai_city
                                            final_region = ai_region
                                            final_region_code = None
                                    
                                    # –ù–ê–ö–û–ü–õ–Ø–ï–ú SHOP –î–ê–ù–ù–´–ï (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π)
                                    # –ü–æ–ª—É—á–∞–µ–º gamification_id –∏ –¥—Ä—É–≥–∏–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞–ø—Ä—è–º—É—é –∏–∑ all_data
                                    gamification_id_value = None
                                    nip_value = None
                                    shopnetwork_value = "UNKNOWN"
                                    address_value = None
                                    
                                    try:
                                        shop_info_query = f"""
                                        SELECT 
                                            gamification_id,
                                            shopnetwork,
                                            shop_name,
                                            JSON_VALUE(raw_doc, '$.nip') as nip,
                                            JSON_VALUE(raw_doc, '$.address') as address
                                        FROM `{PROJECT_ID}.{DATASET}.all_data`
                                        WHERE api_id = '{api_id}'
                                        LIMIT 1
                                        """
                                        shop_info_rows = list(bq_client.query(shop_info_query))
                                        
                                        if shop_info_rows:
                                            shop_info_row = shop_info_rows[0]
                                            gamification_id_value = shop_info_row.gamification_id
                                            nip_value = shop_info_row.nip
                                            shopnetwork_value = shop_info_row.shopnetwork or shop_info_row.shop_name
                                            address_value = shop_info_row.address
                                    except Exception as e:
                                        logger.warning(f"‚ö†Ô∏è Error querying shop_info for {api_id}: {e}. Using fallback values.")
                                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –∑–Ω–∞—á–µ–Ω–∏—è - shop –≤—Å–µ —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è
                                    
                                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ shop_row
                                    try:
                                        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤
                                        shopnetwork_safe = str(shopnetwork_value) if shopnetwork_value else "UNKNOWN"
                                        address_safe = str(address_value)[:255] if address_value else None
                                        city_population_safe = parse_city_population(str(city_population)) if city_population else None
                                        
                                        shop_row = {
                                            "report_id": report_id,
                                            "report_name": report_name,
                                            "api_id": api_id,
                                            "nip": str(nip_value) if nip_value else None,
                                            "shop_chain": normalize_network_name_by_country(shopnetwork_safe, country),
                                            "city": str(final_city) if final_city else "UNKNOWN",
                                            "region": str(final_region) if final_region else "UNKNOWN",
                                            "region_code": str(final_region_code) if final_region_code else None,
                                            "shop_address": address_safe,
                                            "city_population": city_population_safe,
                                            "country": str(country) if country else "PL",
                                            "gamification_id": str(gamification_id_value) if gamification_id_value else None,
                                            "confidence": str(confidence) if confidence else "LOW",
                                            "evidence": str(evidence)[:500] if evidence else "No evidence",
                                            "created_at": _now().isoformat(),
                                        }
                                        accumulated_shops.append(shop_row)
                                        total_shops += 1
                                        logger.info(f"Shop saved for {api_id}: country={country}, city={final_city}, region={final_region}")
                                        total_processed += 1
                                    except Exception as e:
                                        logger.error(f"‚ö†Ô∏è Error creating shop_row for {api_id}: {e}")
                                        import traceback
                                        logger.error(f"‚ö†Ô∏è Traceback: {traceback.format_exc()}")
                                        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å shop —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                                        try:
                                            minimal_shop_row = {
                                                "report_id": report_id,
                                                "report_name": report_name,
                                                "api_id": str(api_id),
                                                "nip": None,
                                                "shop_chain": "UNKNOWN",
                                                "city": "UNKNOWN",
                                                "region": "UNKNOWN",
                                                "region_code": None,
                                                "shop_address": None,
                                                "city_population": None,
                                                "country": str(country) if country else "PL",
                                                "gamification_id": str(gamification_id_value) if gamification_id_value else None,
                                                "confidence": "LOW",
                                                "evidence": f"Error creating shop: {str(e)[:200]}",
                                                "created_at": _now().isoformat(),
                                            }
                                            accumulated_shops.append(minimal_shop_row)
                                            total_shops += 1
                                            logger.warning(f"‚ö†Ô∏è Saved minimal shop_row for {api_id} due to error")
                                            total_processed += 1
                                        except Exception as e2:
                                            logger.error(f"‚ùå Failed to save even minimal shop_row for {api_id}: {e2}")
                                            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
                                            # –ù–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º total_processed, —Ç–∞–∫ –∫–∞–∫ shop –Ω–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
                                except Exception as e:
                                    logger.error(f"‚ö†Ô∏è Error processing shop data for {api_id}: {e}")
                                    import traceback
                                    logger.error(f"‚ö†Ô∏è Traceback: {traceback.format_exc()}")
                                    # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å shop —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ shop –¥–∞–Ω–Ω—ã—Ö
                                    try:
                                        minimal_shop_row = {
                                            "report_id": report_id,
                                            "report_name": report_name,
                                            "api_id": str(api_id),
                                            "nip": None,
                                            "shop_chain": "UNKNOWN",
                                            "city": "UNKNOWN",
                                            "region": "UNKNOWN",
                                            "region_code": None,
                                            "shop_address": None,
                                            "city_population": None,
                                            "country": "PL",  # Fallback country
                                            "gamification_id": None,
                                            "confidence": "LOW",
                                            "evidence": f"Error processing shop data: {str(e)[:200]}",
                                            "created_at": _now().isoformat(),
                                        }
                                        accumulated_shops.append(minimal_shop_row)
                                        total_shops += 1
                                        logger.warning(f"‚ö†Ô∏è Saved minimal shop_row for {api_id} due to error processing shop data")
                                        total_processed += 1
                                    except Exception as e2:
                                        logger.error(f"‚ùå Failed to save even minimal shop_row for {api_id}: {e2}")
                                        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
                                        # –ù–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º total_processed, —Ç–∞–∫ –∫–∞–∫ shop –Ω–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª—Å—è
                        else:
                            logger.warning(f"Could not parse JSON for line {line_num}")
                            failed_parsing += 1
                    else:
                        if line_num <= 3:
                            logger.warning(f"No response text found for line {line_num}")
                        failed_processing += 1
                    
                except Exception as e:
                    logger.error(f"Error processing line {line_num}: {e}")
                    continue
            
            # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ë–ê–¢–ß–ï–í–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –ù–ê–ö–û–ü–õ–ï–ù–ù–´–• –î–ê–ù–ù–´–•
            logger.info("üöÄ STARTING FINAL BATCH LOAD OF ACCUMULATED DATA (GCS)")
            logger.info("üìä Products: %s, Shops: %s, Vector Products: %s", 
                       len(accumulated_products), len(accumulated_shops), len(accumulated_vector_products))
            
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–¥—É–∫—Ç—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
                if accumulated_products:
                    logger.info("üîÑ Loading %s products to corrected_products", len(accumulated_products))
                    _storage_write_api_load(PRODUCTS_TABLE, accumulated_products, report_id, report_name)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
                if accumulated_vector_products:
                    logger.info("üîÑ Loading %s vector products to products_vector_ready", len(accumulated_vector_products))
                    _storage_write_api_load(VECTOR_READY_TABLE, accumulated_vector_products, report_id, report_name)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–≥–∞–∑–∏–Ω—ã –±–æ–ª—å—à–∏–º –±–∞—Ç—á–µ–º
                if accumulated_shops:
                    logger.info("üîÑ Loading %s shops to shop_directory", len(accumulated_shops))
                    _storage_write_api_load(SHOP_TABLE, accumulated_shops, report_id, report_name)
                
                logger.info("‚úÖ ALL ACCUMULATED DATA LOADED SUCCESSFULLY (GCS)")
                
            except Exception as e:
                logger.error("‚ùå FAILED TO LOAD ACCUMULATED DATA (GCS): %s", e)
                raise
            
            processing_time = int(time.time() - start_time)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            logger.info(f"üìä PROCESSING SUMMARY:")
            logger.info(f"  Total lines in file: {total_lines}")
            logger.info(f"  Successfully processed: {processed_successfully}")
            logger.info(f"  Failed parsing: {failed_parsing}")
            logger.info(f"  Failed processing: {failed_processing}")
            logger.info(f"  Final receipts processed: {total_processed}")
            
            if failed_parsing > 0 or failed_processing > 0:
                logger.warning(f"‚ö†Ô∏è LOST {failed_parsing + failed_processing} receipts during processing!")
            
            return {
                "status": "success",
                "message": f"Processed {total_processed} receipts from GCS results",
                "report_id": report_id,
                "report_name": report_name,
                "processing_time_seconds": processing_time,
                "input_uri": input_uri,
                "receipts_processed": total_processed,
                "products_saved": len(accumulated_products),
                "vector_products_saved": len(accumulated_vector_products),
                "diagnostics": {
                    "total_lines": total_lines,
                    "processed_successfully": processed_successfully,
                    "failed_parsing": failed_parsing,
                    "failed_processing": failed_processing
                },
                "shops_saved": len(accumulated_shops)
            }
            
        except Exception as e:
            logger.error(f"Error processing batch results from GCS: {e}")
            return {
                "status": "error",
                "message": f"Failed to process batch results: {str(e)}",
                "report_id": report_id,
                "report_name": report_name
        }

# Initialize batch processor
batch_processor = BatchReceiptProcessor()

# -----------------------------------------------------------------------------
# Batch Status Checker
# -----------------------------------------------------------------------------
@functions_framework.http
def batch_status_checker(request: Request):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö batch jobs"""
    try:
        # Handle CORS preflight
        if request.method == "OPTIONS":
            headers = {
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            }
            return ("", 204, headers)
        
        if request.method != "GET":
            return jsonify({"status": "error", "message": "Only GET method allowed"}), 405
        
        # Extract report_id from path
        path_parts = request.path.strip('/').split('/')
        if len(path_parts) < 2 or path_parts[-2] != 'batch_status':
            return jsonify({"status": "error", "message": "Invalid path format. Use /batch_status/{report_id}"}), 400
        
        report_id = path_parts[-1]
        if not report_id:
            return jsonify({"status": "error", "message": "Report ID is required"}), 400
        
        logger.info(f"Checking batch status for report_id: {report_id}")
        
        # Search for batch jobs with this report_id in the name
        # This is a simplified approach - in production you might want to store job metadata in a database
        try:
            # List recent batch prediction jobs
            jobs = aiplatform.BatchPredictionJob.list(
                filter=f'display_name~"receipt-batch-async-{report_id}"',
                order_by="create_time desc",
                limit=1
            )
            
            if not jobs:
                return jsonify({
                    "status": "not_found",
                    "message": f"No batch job found for report_id: {report_id}",
                    "report_id": report_id
                }), 404
            
            job = jobs[0]
            state = job.state
            
            # Determine status and progress
            status_info = {
                "report_id": report_id,
                "job_name": job.resource_name,
                "display_name": job.display_name,
                "create_time": job.create_time.isoformat() if job.create_time else None,
                "update_time": job.update_time.isoformat() if job.update_time else None,
            }
            
            if JobState:
                if state == JobState.JOB_STATE_QUEUED:
                    status_info.update({
                        "status": "queued",
                        "progress": "0%",
                        "message": "Job is queued for execution"
                    })
                elif state == JobState.JOB_STATE_RUNNING:
                    status_info.update({
                        "status": "running", 
                        "progress": "50%",
                        "message": "Job is currently running"
                    })
                elif state == JobState.JOB_STATE_SUCCEEDED:
                    status_info.update({
                        "status": "completed",
                        "progress": "100%",
                        "message": "Job completed successfully",
                        "output_location": job.output_info.gcs_output_directory if job.output_info else None
                    })
                elif state == JobState.JOB_STATE_FAILED:
                    status_info.update({
                        "status": "failed",
                        "progress": "0%",
                        "message": f"Job failed: {job.error.message if job.error else 'Unknown error'}",
                        "error": str(job.error) if job.error else None
                    })
                elif state == JobState.JOB_STATE_CANCELLED:
                    status_info.update({
                        "status": "cancelled",
                        "progress": "0%",
                        "message": "Job was cancelled"
                    })
                else:
                    status_info.update({
                        "status": "unknown",
                        "progress": "unknown",
                        "message": f"Unknown job state: {state}",
                        "raw_state": str(state)
                    })
            else:
                # Fallback to string comparison
                state_str = str(state)
                if "QUEUED" in state_str:
                    status_info.update({
                        "status": "queued",
                        "progress": "0%",
                        "message": "Job is queued for execution"
                    })
                elif "RUNNING" in state_str:
                    status_info.update({
                        "status": "running",
                        "progress": "50%", 
                        "message": "Job is currently running"
                    })
                elif "SUCCEEDED" in state_str:
                    status_info.update({
                        "status": "completed",
                        "progress": "100%",
                        "message": "Job completed successfully",
                        "output_location": job.output_info.gcs_output_directory if job.output_info else None
                    })
                elif "FAILED" in state_str:
                    status_info.update({
                        "status": "failed",
                        "progress": "0%",
                        "message": f"Job failed: {job.error.message if job.error else 'Unknown error'}",
                        "error": str(job.error) if job.error else None
                    })
                else:
                    status_info.update({
                        "status": "unknown",
                        "progress": "unknown",
                        "message": f"Unknown job state: {state_str}",
                        "raw_state": state_str
                    })
            
            headers = {
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "GET, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            }
            return (jsonify(status_info), 200, headers)
            
        except Exception as e:
            logger.error(f"Error checking batch status for {report_id}: {e}")
            return jsonify({
                "status": "error",
                "message": f"Error checking job status: {str(e)}",
                "report_id": report_id
            }), 500
            
    except Exception as e:
        logger.exception("Error in batch_status_checker")
        return jsonify({"status": "error", "message": str(e)}), 500

# -----------------------------------------------------------------------------
# Active Promotions Processor
# -----------------------------------------------------------------------------
@functions_framework.http
def process_active_promotions(request: Request):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    """
    try:
        logger.info("üöÄ Starting active promotions processing")
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        data = request.get_json(silent=True) or {}
        limit_per_promo = data.get("limit_per_promo", None)  # –ë–µ–∑ –ª–∏–º–∏—Ç–∞ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –¥–ª—è –æ—Ç—á–µ—Ç–∞
        report_id = generate_report_id("active_promotions")
        report_name = f"Active Promotions Processing - {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}"
        
        logger.info(f"Report ID: {report_id}")
        logger.info(f"Report Name: {report_name}")
        logger.info(f"Limit per promo: {limit_per_promo}")
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏–∏
        result = process_active_promotions_parallel(
            report_id=report_id,
            report_name=report_name,
            limit_per_promo=limit_per_promo
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        result.update({
            "report_id": report_id,
            "report_name": report_name,
            "timestamp": datetime.utcnow().isoformat(),
            "service": "receipt-data-processor-active-promotions",
            "dataset": DATASET,
            "supported_countries": SUPPORTED_COUNTRIES
        })
        
        logger.info(f"‚úÖ Active promotions processing completed: {result.get('status', 'unknown')}")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"‚ùå Error in active promotions processing: {e}", exc_info=True)
        return jsonify({
            "status": "error",
            "message": f"Active promotions processing failed: {str(e)}",
            "timestamp": datetime.utcnow().isoformat(),
            "service": "receipt-data-processor-active-promotions"
        }), 500

# -----------------------------------------------------------------------------
# HTTP handler
# -----------------------------------------------------------------------------


@functions_framework.http
def receipt_data_processor(request: Request):
    """
    Cloud Function entry point —Å –ø–æ–ª–Ω—ã–º —Ü–∏–∫–ª–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–µ–∂–∏–º—ã:
    * mode="single" - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–µ–∫–∞ (–ø–µ—Ä–µ–¥–∞–µ—Ç–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON)
    * mode="single_by_id" - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–µ–∫–∞ –ø–æ ID (–¥–∞–Ω–Ω—ã–µ –±–µ—Ä—É—Ç—Å—è –∏–∑ BigQuery)
    * mode="batch" - –ø–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    * mode="batch_ids" - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö ID —Å –æ–∂–∏–¥–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    * mode="load_to_fact_scan" - –∑–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —á–µ–∫–æ–≤ –≤ fact_scan
    * mode="load_to_all_data" - –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω—ã—Ö —á–µ–∫–æ–≤ –≤ all_data
    * mode="load_both" - –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –≤ fact_scan, –∏ –≤ all_data
    """
    try:
        # Handle CORS preflight
        if request.method == "OPTIONS":
            headers = {
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type",
            }
            return ("", 204, headers)
        
        # Health endpoint
        if request.path == '/health' or request.args.get('health') == 'true':
            return jsonify({
                "status": "ok", 
                "service": "receipt-data-processor-complete",
                "timestamp": datetime.utcnow().isoformat(),
                "dataset": DATASET,
                "supported_countries": SUPPORTED_COUNTRIES
            })
            
        data = request.get_json(silent=True) or {}
        mode = data.get("mode", "single")
        
        # Auto-detect country from gamification_id if not provided
        country = data.get("country")
        gamification_id = data.get("gamification_id")
        
        if not country and gamification_id:
            country = get_country_from_gamification(gamification_id)
            if country:
                logger.info(f"üåç Auto-detected country: {country} from gamification_id: {gamification_id}")
            else:
                country = "PL"  # Default fallback
                logger.warning(f"‚ö†Ô∏è Could not detect country for gamification_id: {gamification_id}, using default: PL")
        
        # Validate country if provided
        if country and country not in SUPPORTED_COUNTRIES:
            return jsonify({"status": "error", "message": f"Unsupported country: {country}"}), 400
            
        # Determine report identifiers
        report_name = data.get("report_name") or f"Receipt_{mode.title()}"
        report_id = data.get("report_id") or generate_report_id(report_name)
        
        logger.info(f"Processing request: mode={mode}, report_id={report_id}")
        
        result = {}  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º result –∑–∞—Ä–∞–Ω–µ–µ
        
        if mode == "single":
            api_id = data.get("api_id")
            products_json = data.get("products")
            if not api_id or not products_json:
                return jsonify({"status": "error", "message": "api_id and products are required"}), 400
                
            total_price = data.get("total_price")
            if total_price is not None:
                try:
                    total_price = float(total_price)
                except (TypeError, ValueError):
                    total_price = None
                    
            result = process_single_receipt(
                api_id=api_id,
                products_json=products_json,
                total_price=total_price,
                country=country,
                report_id=report_id,
                report_name=report_name,
                shopnetwork=data.get("shopnetwork"),
                shop_name=data.get("shop_name"),
                raw_address=data.get("address"),
                nip=data.get("nip"),
            )
            
        elif mode == "single_by_id":
            api_id = data.get("api_id")
            if not api_id:
                return jsonify({"status": "error", "message": "api_id is required"}), 400
                
            result = process_single_receipt_by_id(
                api_id=api_id,
                country=country,
                report_id=report_id,
                report_name=report_name
            )
            
        elif mode == "batch":
            target_date = data.get('target_date')
            date_from = data.get('date_from')
            date_to = data.get('date_to')
            since_timestamp = data.get('since_timestamp')
            no_date_filter = data.get('no_date_filter', False)
            countries = data.get('countries')
            if not countries and data.get('country'):
                countries = [data.get('country')]
            if not countries:
                countries = get_countries_from_data(target_date, None)
                if countries:
                    logger.info(f"üåç Auto-detected countries: {countries}")
                else:
                    logger.error(f"‚ùå No countries found in data for date: {target_date}")
                    return jsonify({
                        'status': 'error',
                        'message': f'No countries found in data for date: {target_date}. Please specify countries parameter.'
                    }), 400
            limit = data.get('limit', None)
            test_mode = data.get('test_mode', False)
            async_mode = data.get('async', False)

            if async_mode:
                logger.info(f"Starting async batch job creation: countries={countries}, date={target_date}, limit={limit}")
                try:
                    result = batch_processor.create_batch_job_async(
                        report_id=report_id,
                        report_name=report_name,
                        countries=countries,
                        target_date=target_date,
                        date_from=date_from,
                        date_to=date_to,
                        no_date_filter=no_date_filter,
                        limit=limit,
                        test_mode=test_mode
                    )
                    result["check_status_endpoint"] = f"/batch_status/{report_id}"
                except Exception as e:
                    return jsonify({"status": "error", "message": str(e)}), 503
            else:
                logger.info(f"Starting complete batch processing: countries={countries}, date={target_date}, limit={limit}")
                result = batch_processor.process_batch_receipts_complete(
                    report_id=report_id,
                    report_name=report_name,
                    countries=countries,
                    target_date=target_date,
                    date_from=date_from,
                    date_to=date_to,
                    since_timestamp=since_timestamp,
                    no_date_filter=no_date_filter,
                    limit=limit,
                    test_mode=test_mode
                )
            
        elif mode == "load_to_fact_scan":
            target_date = data.get('target_date')
            gamification_id = data.get('gamification_id')
            overwrite_mode = data.get('overwrite_mode', False)
            result = load_receipts_to_fact_scan(
                country=country,
                target_date=target_date,
                report_id=report_id,
                report_name=report_name,
                gamification_id=gamification_id,
                overwrite_mode=overwrite_mode
            )
            
        elif mode == "load_all_promos_to_fact_scan":
            start_date = data.get('start_date')
            end_date = data.get('end_date')
            overwrite_mode = data.get('overwrite_mode', False)
            result = load_all_promos_to_fact_scan(
                start_date=start_date,
                end_date=end_date,
                report_id=report_id,
                report_name=report_name,
                overwrite_mode=overwrite_mode
            )
            
        elif mode == "load_to_all_data":
            target_date = data.get('target_date')
            since_timestamp = data.get('since_timestamp')
            result = load_successful_receipts_to_all_data(
                country=country,
                target_date=target_date,
                report_id=report_id,
                report_name=report_name,
                since_timestamp=since_timestamp
            )
            
        elif mode == "load_both":
            target_date = data.get('target_date')
            since_timestamp = data.get('since_timestamp')
            incremental = data.get('incremental', False)
            countries = data.get('countries', [])
            gamification_id = data.get('gamification_id')
            overwrite_mode = data.get('overwrite_mode', False)
            
            # –ï—Å–ª–∏ incremental=True –∏ –Ω–µ—Ç since_timestamp, –±–µ—Ä–µ–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
            if incremental and not since_timestamp:
                since_timestamp = (datetime.now(timezone.utc) - timedelta(hours=1)).isoformat()
                logger.info(f"üîÑ Incremental mode: using since_timestamp={since_timestamp}")
            
            if not countries and country:
                countries = [country]

            if not countries:
                return jsonify({"status": "error", "message": "A list of 'countries' is required for mode 'load_both'"}), 400

            results_by_country = {}
            overall_status = "success"

            for c in countries:
                logger.info(f"Processing 'load_both' for country: {c}")
            fact_scan_result = load_receipts_to_fact_scan(
                    country=c,
                target_date=target_date,
                report_id=report_id,
                    report_name=report_name,
                    since_timestamp=since_timestamp,
                    gamification_id=gamification_id,
                    overwrite_mode=overwrite_mode
            )
            
            all_data_result = load_successful_receipts_to_all_data(
                    country=c,
                target_date=target_date,
                report_id=report_id,
                report_name=report_name,
                since_timestamp=since_timestamp
            )

            if fact_scan_result['status'] == 'error' or all_data_result['status'] == 'error':
                overall_status = "partial_error"

            results_by_country[c] = {
                "fact_scan_result": fact_scan_result,
                "all_data_result": all_data_result
            }
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ result –≤—ã–Ω–µ—Å–µ–Ω–æ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã —Ü–∏–∫–ª–∞ –∏ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∞ 'load_both'
            result = {
                "status": overall_status,
                "message": f"Finished loading data for countries: {', '.join(countries)}",
                "results_by_country": results_by_country,
                "report_id": report_id,
                "report_name": report_name
            }
            
        elif mode == "batch_ids":
            api_ids = data.get('api_ids', [])
            input_uri = data.get('input_uri')
            
            if not api_ids and not input_uri:
                return jsonify({"status": "error", "message": "Either api_ids list or input_uri is required for batch_ids mode"}), 400
            
            if api_ids and input_uri:
                return jsonify({"status": "error", "message": "Provide either api_ids or input_uri, not both"}), 400
            
            test_mode = data.get('test_mode', False)

            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ if/else
            if api_ids:
                if not isinstance(api_ids, list):
                    return jsonify({"status": "error", "message": "api_ids must be a list"}), 400
                    
                if len(api_ids) > 5000:
                    return jsonify({"status": "error", "message": "api_ids list too large (max 5000)"}), 400
                
                result = batch_processor.process_batch_by_ids(
                    report_id=report_id,
                    report_name=report_name,
                    api_ids=api_ids,
                    country=country,
                    test_mode=test_mode
                )
            else:
                result = batch_processor.process_batch_results_from_gcs(
                    report_id=report_id,
                    report_name=report_name,
                    input_uri=input_uri
                )
        else:
            return jsonify({"status": "error", "message": f"Unknown mode: {mode}. Supported: single, single_by_id, batch, batch_ids, load_to_fact_scan, load_to_all_data, load_both"}), 400
            
        headers = {
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type",
        }
        return (jsonify(result), 200, headers)
        
    except Exception as e:
        logger.exception("Error handling request")
        return (jsonify({"status": "error", "message": str(e)}), 500, {
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "POST, GET, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type",
        })

def main():
    from flask import Flask, request as flask_request
    app = Flask(__name__)

    @app.route("/", methods=["POST", "GET", "OPTIONS"])
    def local_endpoint():
        return receipt_data_processor(flask_request)

    @app.route("/health", methods=["GET"])
    def health_check():
        return jsonify({
            "status": "ok", 
            "service": "receipt-data-processor-local",
            "timestamp": datetime.utcnow().isoformat(),
            "dataset": DATASET,
            "supported_countries": SUPPORTED_COUNTRIES
        })

    app.run(host="0.0.0.0", port=8080, debug=True)

if __name__ == "__main__":
    main()